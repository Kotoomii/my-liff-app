"""
NASA-TLXãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å€¤äºˆæ¸¬æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«
webhooktest.pyã®æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ãã‚·ãƒ³ãƒ—ãƒ«ãªå®Ÿè£…
"""

import pandas as pd
import numpy as np
import logging
from datetime import datetime, timedelta
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import joblib
import os
from typing import Dict, List, Optional

from config import Config

logger = logging.getLogger(__name__)

# document_for_ai.mdã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹æ´»å‹•ã‚«ãƒ†ã‚´ãƒªãƒªã‚¹ãƒˆ
# Google Sheetsã®å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã«å®Œå…¨ä¸€è‡´ã•ã›ã‚‹ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ç¢ºèªæ¸ˆã¿ï¼‰
KNOWN_ACTIVITIES = [
    'ç¡çœ ', 'é£Ÿäº‹', 'èº«ã®ã¾ã‚ã‚Šã®ç”¨äº‹', 'ç™‚é¤Šãƒ»é™é¤Š', 'ä»•äº‹', 'ä»•äº‹ã®ã¤ãã‚ã„',
    'æˆæ¥­ãƒ»å­¦å†…ã®æ´»å‹•', 'å­¦æ ¡å¤–ã®å­¦ç¿’', 'ç‚Šäº‹ãƒ»æƒé™¤ãƒ»æ´—æ¿¯', 'è²·ã„ç‰©', 'å­ã©ã‚‚ã®ä¸–è©±',
    'å®¶åº­é›‘äº‹', 'é€šå‹¤', 'é€šå­¦', 'ç¤¾ä¼šå‚åŠ ', 'ä¼šè©±ãƒ»äº¤éš›', 'ã‚¹ãƒãƒ¼ãƒ„', 'è¡Œæ¥½ãƒ»æ•£ç­–',
    'è¶£å‘³ãƒ»å¨¯æ¥½ãƒ»æ•™é¤Š(ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆé™¤ã)', 'è¶£å‘³ãƒ»å¨¯æ¥½ãƒ»æ•™é¤Šã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆ(å‹•ç”»é™¤ã)',
    'ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆå‹•ç”»', 'ãƒ†ãƒ¬ãƒ“', 'éŒ²ç”»ç•ªçµ„ãƒ»DVD', 'ãƒ©ã‚¸ã‚ª', 'æ–°è',
    'é›‘èªŒãƒ»æ¼«ç”»ãƒ»æœ¬', 'éŸ³æ¥½', 'ä¼‘æ†©', 'ãã®ä»–', 'ä¸æ˜'
]

# æ´»å‹•åã®è¡¨è¨˜ã‚†ã‚Œã‚’æ­£è¦åŒ–ã™ã‚‹ãƒãƒƒãƒ”ãƒ³ã‚°
# ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«è¨˜éŒ²ã•ã‚Œã¦ã„ã‚‹æ§˜ã€…ãªè¡¨è¨˜ã‚’æ¨™æº–å½¢å¼ï¼ˆKNOWN_ACTIVITIESï¼‰ã«çµ±ä¸€
ACTIVITY_NORMALIZATION_MAP = {
    'å­ä¾›ã®ä¸–è©±': 'å­ã©ã‚‚ã®ä¸–è©±',
    'å­ä¾›': 'å­ã©ã‚‚ã®ä¸–è©±',
    'ä¼‘æ¯': 'ä¼‘æ†©',  # å¤ã„ãƒ‡ãƒ¼ã‚¿ã«'ä¼‘æ¯'ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã®å¯¾å¿œ
    # å¿…è¦ã«å¿œã˜ã¦ä»–ã®è¡¨è¨˜ã‚†ã‚Œã‚’è¿½åŠ 
    # ä¾‹: 'ã‚¤ãƒ³ã‚¿ä¸€ãƒãƒƒãƒˆå‹•ç”»': 'ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆå‹•ç”»',
}

class FrustrationPredictor:
    def __init__(self):
        self.model = None
        self.feature_columns = []
        self.target_variable = 'NASA_F_scaled'
        self.config = Config()
        self.activity_columns = []  # æ´»å‹•ã‚«ãƒ†ã‚´ãƒªã®One-Hotåˆ—å

    def _create_model(self):
        """
        config.MODEL_TYPEã«åŸºã¥ã„ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
        """
        if self.config.MODEL_TYPE == 'Linear':
            logger.info("LinearRegressionãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™")
            return LinearRegression(n_jobs=-1)
        elif self.config.MODEL_TYPE == 'SVR':
            logger.info("SVRï¼ˆã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼å›å¸°ï¼‰ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™ - GridSearchCVã§æœ€é©åŒ–")
            from sklearn.model_selection import GridSearchCV

            # æ¢ç´¢ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“
            param_grid = {
                'C': [0.1, 1, 10, 100],
                'epsilon': [0.01, 0.1, 0.5],
                'gamma': ['scale', 'auto']
            }

            # GridSearchCVã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢
            base_svr = SVR(kernel='rbf')
            grid_search = GridSearchCV(
                base_svr,
                param_grid,
                cv=3,  # 3-fold cross validation
                scoring='neg_mean_squared_error',  # RMSEã‚’æœ€å°åŒ–
                n_jobs=-1,  # ä¸¦åˆ—å‡¦ç†
                verbose=1
            )

            return grid_search
        else:  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯RandomForest
            logger.info("RandomForestRegressorãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™")
            return RandomForestRegressor(
                n_estimators=self.config.N_ESTIMATORS,
                max_depth=self.config.MAX_DEPTH,
                random_state=self.config.RANDOM_STATE,
                n_jobs=-1
            )

    def preprocess_activity_data(self, activity_data: pd.DataFrame) -> pd.DataFrame:
        """
        æ´»å‹•ãƒ‡ãƒ¼ã‚¿ã‚’å‰å‡¦ç† (webhooktest.pyå½¢å¼)
        """
        try:
            if activity_data.empty:
                return pd.DataFrame()

            df = activity_data.copy()
            df['Timestamp'] = pd.to_datetime(df['Timestamp'])

            if 'NASA_F' not in df.columns:
                logger.error("NASA_Fåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return pd.DataFrame()

            # CatSubã®è¡¨è¨˜ã‚†ã‚Œã‚’æ­£è¦åŒ–ï¼ˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ç›´å¾Œã«å®Ÿè¡Œï¼‰
            if 'CatSub' in df.columns:
                original_unique = df['CatSub'].unique()
                df['CatSub'] = df['CatSub'].replace(ACTIVITY_NORMALIZATION_MAP)
                normalized_unique = df['CatSub'].unique()

                # æ­£è¦åŒ–ãŒè¡Œã‚ã‚ŒãŸå ´åˆã¯ãƒ­ã‚°å‡ºåŠ›
                if len(original_unique) != len(normalized_unique) or not all(o in normalized_unique for o in original_unique):
                    logger.info(f"CatSubã®è¡¨è¨˜ã‚†ã‚Œã‚’æ­£è¦åŒ–ã—ã¾ã—ãŸ: {len(original_unique)}ç¨®é¡ â†’ {len(normalized_unique)}ç¨®é¡")
                    # æ­£è¦åŒ–ã•ã‚ŒãŸãŒã€ã¾ã KNOWN_ACTIVITIESã«ãªã„å€¤ã‚’è­¦å‘Š
                    unknown_activities = set(normalized_unique) - set(KNOWN_ACTIVITIES)
                    if unknown_activities:
                        logger.warning(f"æœªçŸ¥ã®æ´»å‹•ã‚«ãƒ†ã‚´ãƒªãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ: {unknown_activities}")

            # æ•°å€¤å¤‰æ›
            df['NASA_F'] = pd.to_numeric(df['NASA_F'], errors='coerce')
            df['Duration'] = pd.to_numeric(df['Duration'], errors='coerce')

            # 15åˆ†æœªæº€ã®æ´»å‹•ã‚’é™¤å¤–
            df = df[df['Duration'] >= 15]

            # Durationã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚° (15-720åˆ† â†’ 0-1)
            # 15åˆ†=æœ€å°å€¤ã€720åˆ†(12æ™‚é–“)=å¦¥å½“ãªæœ€å¤§å€¤
            duration_min = 15
            duration_max = 720
            df['Duration_scaled'] = (df['Duration'] - duration_min) / (duration_max - duration_min)
            # ç¯„å›²å¤–ã®å€¤ã‚’ã‚¯ãƒªãƒƒãƒ—ï¼ˆ0-1ã«åã‚ã‚‹ï¼‰
            df['Duration_scaled'] = df['Duration_scaled'].clip(0, 1)

            # Duration_scaledã®ç•°å¸¸å€¤ãƒã‚§ãƒƒã‚¯
            if df['Duration_scaled'].isna().any():
                logger.warning(f"âš ï¸ Duration_scaledã«NaNå€¤ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ")
            if np.isinf(df['Duration_scaled']).any():
                logger.warning(f"âš ï¸ Duration_scaledã«ç„¡é™å¤§ã®å€¤ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ")
                df['Duration_scaled'] = df['Duration_scaled'].replace([np.inf, -np.inf], np.nan)

            # æ™‚é–“ç‰¹å¾´é‡ (webhooktest.pyå½¢å¼)
            df['hour'] = df['Timestamp'].dt.hour
            df['hour_rad'] = 2 * np.pi * df['hour'] / 24
            df['hour_sin'] = np.sin(df['hour_rad'])
            df['hour_cos'] = np.cos(df['hour_rad'])

            # æ›œæ—¥ç‰¹å¾´é‡
            df['weekday_str'] = df['Timestamp'].dt.strftime('%a')
            df = pd.get_dummies(df, columns=['weekday_str'], prefix='weekday', dtype=int)  # intå‹ã«æŒ‡å®š

            # ã€é‡è¦ã€‘å…¨æ›œæ—¥ã®åˆ—ã‚’ç¢ºå®Ÿã«ä½œæˆï¼ˆãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œãªã„æ›œæ—¥ã¯0ã§åŸ‹ã‚ã‚‹ï¼‰
            # ã“ã‚Œã«ã‚ˆã‚Šã€DiCEå®Ÿè¡Œæ™‚ã«ãƒ¢ãƒ‡ãƒ«ã®feature_columnsã¨ä¸€è‡´ã•ã›ã‚‹
            for day in ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']:
                col_name = f'weekday_{day}'
                if col_name not in df.columns:
                    df[col_name] = 0

            # NASA_Fã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚° (0-20 â†’ 0-1)
            df['NASA_F_scaled'] = df['NASA_F'] / 20.0

            # æ´»å‹•ã‚«ãƒ†ã‚´ãƒªã®One-HotåŒ– (webhooktest.pyå½¢å¼)
            # CatSubã®å€¤ã‚’å–å¾—ã—ã€æ—¢çŸ¥ã®æ´»å‹•ãƒªã‚¹ãƒˆã§One-HotåŒ–
            for activity in KNOWN_ACTIVITIES:
                df[f'activity_{activity}'] = (df['CatSub'] == activity).astype(int)

            if self.config.LOG_DATA_OPERATIONS:
                logger.info(f"æ´»å‹•ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†å®Œäº†: {len(df)} è¡Œ")

            return df

        except Exception as e:
            logger.error(f"æ´»å‹•ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return pd.DataFrame()

    def aggregate_fitbit_by_activity(self, activity_data: pd.DataFrame, fitbit_data: pd.DataFrame) -> pd.DataFrame:
        """
        Fitbitãƒ‡ãƒ¼ã‚¿ã‚’æ´»å‹•æœŸé–“ã”ã¨ã«é›†è¨ˆ (webhooktest.pyå½¢å¼ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°)
        """
        try:
            if activity_data.empty:
                return activity_data

            if fitbit_data.empty:
                logger.warning("Fitbitãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚ç”Ÿä½“æƒ…å ±ãªã—ã§ç¶™ç¶šã—ã¾ã™")
                # Fitbitãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã€SDNN_scaled, Lorenz_Area_scaledã‚’NaNã§è¿½åŠ 
                activity_data['SDNN_scaled'] = np.nan
                activity_data['Lorenz_Area_scaled'] = np.nan
                return activity_data

            fitbit_data = fitbit_data.copy()
            fitbit_data['Timestamp'] = pd.to_datetime(fitbit_data['Timestamp'])

            # SDNNã¨Lorenz_Areaã‚’æ•°å€¤åŒ–
            if 'SDNN' in fitbit_data.columns:
                fitbit_data['SDNN'] = pd.to_numeric(fitbit_data['SDNN'], errors='coerce')
            else:
                fitbit_data['SDNN'] = np.nan

            if 'Lorenz_Area' in fitbit_data.columns:
                fitbit_data['Lorenz_Area'] = pd.to_numeric(fitbit_data['Lorenz_Area'], errors='coerce')
            else:
                fitbit_data['Lorenz_Area'] = np.nan

            # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚° (webhooktest.pyå½¢å¼)
            sdnn_max = fitbit_data['SDNN'].max()
            lorenz_max = fitbit_data['Lorenz_Area'].max()

            logger.warning(f"ğŸ” ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°: Fitbitãƒ‡ãƒ¼ã‚¿ä»¶æ•°={len(fitbit_data)}, SDNN_max={sdnn_max}, Lorenz_max={lorenz_max}")

            if sdnn_max > 0:
                fitbit_data['SDNN_scaled'] = fitbit_data['SDNN'] / sdnn_max
                # ç„¡é™å¤§ã®å€¤ã‚’ãƒã‚§ãƒƒã‚¯
                if np.isinf(fitbit_data['SDNN_scaled']).any():
                    logger.warning(f"âš ï¸ SDNN_scaledã«ç„¡é™å¤§ã®å€¤ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚NaNã«ç½®æ›ã—ã¾ã™ã€‚")
                    fitbit_data['SDNN_scaled'] = fitbit_data['SDNN_scaled'].replace([np.inf, -np.inf], np.nan)
                logger.warning(f"ğŸ” SDNN_scaled è¨ˆç®—å®Œäº†: min={fitbit_data['SDNN_scaled'].min():.3f}, max={fitbit_data['SDNN_scaled'].max():.3f}")
            else:
                fitbit_data['SDNN_scaled'] = np.nan
                logger.warning(f"âš ï¸ SDNN_max ãŒ 0 ä»¥ä¸‹ã®ãŸã‚ã€SDNN_scaled = NaN ã«è¨­å®š")

            if lorenz_max > 0:
                fitbit_data['Lorenz_Area_scaled'] = fitbit_data['Lorenz_Area'] / lorenz_max
                # ç„¡é™å¤§ã®å€¤ã‚’ãƒã‚§ãƒƒã‚¯
                if np.isinf(fitbit_data['Lorenz_Area_scaled']).any():
                    logger.warning(f"âš ï¸ Lorenz_Area_scaledã«ç„¡é™å¤§ã®å€¤ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚NaNã«ç½®æ›ã—ã¾ã™ã€‚")
                    fitbit_data['Lorenz_Area_scaled'] = fitbit_data['Lorenz_Area_scaled'].replace([np.inf, -np.inf], np.nan)
                logger.warning(f"ğŸ” Lorenz_Area_scaled è¨ˆç®—å®Œäº†: min={fitbit_data['Lorenz_Area_scaled'].min():.3f}, max={fitbit_data['Lorenz_Area_scaled'].max():.3f}")
            else:
                fitbit_data['Lorenz_Area_scaled'] = np.nan
                logger.warning(f"âš ï¸ Lorenz_max ãŒ 0 ä»¥ä¸‹ã®ãŸã‚ã€Lorenz_Area_scaled = NaN ã«è¨­å®š")

            # å„æ´»å‹•æœŸé–“ã®Fitbitçµ±è¨ˆé‡ã‚’è¨ˆç®—
            activity_with_fitbit = []

            logger.warning(f"ğŸ” æ´»å‹•ã¨Fitbitãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒãƒãƒ³ã‚°é–‹å§‹: æ´»å‹•æ•°={len(activity_data)}, Fitbitãƒ‡ãƒ¼ã‚¿æ™‚é–“ç¯„å›²=[{fitbit_data['Timestamp'].min()} - {fitbit_data['Timestamp'].max()}]")

            for idx, activity in activity_data.iterrows():
                start_time = activity['Timestamp']
                duration_minutes = activity['Duration']
                end_time = start_time + timedelta(minutes=duration_minutes)

                # è©²å½“æœŸé–“ã®Fitbitãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                fitbit_period = fitbit_data[
                    (fitbit_data['Timestamp'] >= start_time) &
                    (fitbit_data['Timestamp'] <= end_time)
                ]

                # çµ±è¨ˆé‡ã‚’è¨ˆç®— (webhooktest.pyã¯SDNN_scaled, Lorenz_Area_scaledã®å¹³å‡ã‚’ä½¿ç”¨)
                activity_dict = activity.to_dict()
                if not fitbit_period.empty:
                    sdnn_mean = fitbit_period['SDNN_scaled'].mean()
                    lorenz_mean = fitbit_period['Lorenz_Area_scaled'].mean()
                    activity_dict['SDNN_scaled'] = sdnn_mean
                    activity_dict['Lorenz_Area_scaled'] = lorenz_mean
                else:
                    activity_dict['SDNN_scaled'] = np.nan
                    activity_dict['Lorenz_Area_scaled'] = np.nan

                activity_with_fitbit.append(activity_dict)

            result_df = pd.DataFrame(activity_with_fitbit)
            if self.config.LOG_DATA_OPERATIONS:
                logger.info(f"Fitbitçµ±è¨ˆé‡åŒ–å®Œäº†: {len(result_df)} è¡Œ")
            return result_df

        except Exception as e:
            logger.error(f"Fitbitçµ±è¨ˆé‡åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚SSDN_scaled, Lorenz_Area_scaledã‚’NaNã§è¿½åŠ 
            activity_data['SDNN_scaled'] = np.nan
            activity_data['Lorenz_Area_scaled'] = np.nan
            return activity_data

    def check_data_quality(self, df_enhanced: pd.DataFrame) -> Dict:
        """
        å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®å“è³ªã‚’ãƒã‚§ãƒƒã‚¯
        """
        try:
            data_quality = {
                'total_samples': len(df_enhanced),
                'is_sufficient': False,
                'quality_level': 'insufficient',
                'warnings': [],
                'recommendations': []
            }

            if len(df_enhanced) < 10:
                data_quality['warnings'].append(f"ãƒ‡ãƒ¼ã‚¿æ•°ãŒä¸è¶³ã—ã¦ã„ã¾ã™ï¼ˆ{len(df_enhanced)}ä»¶/æœ€ä½10ä»¶å¿…è¦ï¼‰")
                data_quality['recommendations'].append("ã‚ˆã‚Šå¤šãã®æ´»å‹•ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²ã—ã¦ãã ã•ã„ã€‚")
            elif len(df_enhanced) < 30:
                data_quality['quality_level'] = 'minimal'
                data_quality['is_sufficient'] = True
                data_quality['warnings'].append(f"ãƒ‡ãƒ¼ã‚¿æ•°ãŒå°‘ãªã„ãŸã‚ã€äºˆæ¸¬ç²¾åº¦ãŒä½ã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼ˆ{len(df_enhanced)}ä»¶ï¼‰")
                data_quality['recommendations'].append("30ä»¶ä»¥ä¸Šã®ãƒ‡ãƒ¼ã‚¿ã§ç²¾åº¦ãŒå‘ä¸Šã—ã¾ã™ã€‚")
            elif len(df_enhanced) < 100:
                data_quality['quality_level'] = 'moderate'
                data_quality['is_sufficient'] = True
            else:
                data_quality['quality_level'] = 'good'
                data_quality['is_sufficient'] = True

            # ãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å€¤ã®åˆ†æ•£ãƒã‚§ãƒƒã‚¯
            if 'NASA_F' in df_enhanced.columns:
                frustration_std = df_enhanced['NASA_F'].std()
                if frustration_std < 1.0:
                    data_quality['warnings'].append(f"ãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å€¤ã®ãƒãƒ©ã¤ããŒå°ã•ã„ã§ã™ï¼ˆæ¨™æº–åå·®: {frustration_std:.2f}ï¼‰")
                    data_quality['recommendations'].append("æ§˜ã€…ãªçŠ¶æ³ã§ã®æ´»å‹•ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²ã—ã¦ãã ã•ã„ã€‚")

            return data_quality

        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'total_samples': 0,
                'is_sufficient': False,
                'quality_level': 'error',
                'warnings': [str(e)],
                'recommendations': []
            }

    def train_model(self, df_enhanced: pd.DataFrame) -> Dict:
        """
        ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ (webhooktest.pyå½¢å¼ã®ã‚·ãƒ³ãƒ—ãƒ«ãªå®Ÿè£…)
        """
        try:
            data_quality = self.check_data_quality(df_enhanced)

            if len(df_enhanced) < 10:
                raise ValueError(f"è¨“ç·´ã«ã¯æœ€ä½10å€‹ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™ï¼ˆç¾åœ¨: {len(df_enhanced)}ä»¶ï¼‰")

            # NaNå€¤ã‚’å«ã‚€è¡Œã‚’é™¤å¤–
            required_cols = ['SDNN_scaled', 'Lorenz_Area_scaled', 'NASA_F_scaled', 'Duration_scaled']
            df_clean = df_enhanced.dropna(subset=required_cols)

            if df_clean.empty:
                raise ValueError("æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚SDNN, Lorenz_Area, NASA_F, DurationãŒã™ã¹ã¦å¿…è¦ã§ã™ã€‚")

            # æ´»å‹•ã‚«ãƒ†ã‚´ãƒªåˆ—ã‚’å–å¾—
            activity_cols = [col for col in df_clean.columns if col.startswith('activity_')]
            self.activity_columns = activity_cols

            # æ›œæ—¥åˆ—ã‚’å–å¾—
            weekday_cols = [col for col in df_clean.columns if col.startswith('weekday_')]

            # æ™‚é–“ç‰¹å¾´é‡
            time_features = ['hour_sin', 'hour_cos']

            # ç‰¹å¾´é‡: webhooktest.pyå½¢å¼ + Duration
            feature_list = ['SDNN_scaled', 'Lorenz_Area_scaled', 'Duration_scaled'] + activity_cols + time_features + weekday_cols
            self.feature_columns = feature_list

            # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’æŠ½å‡º
            X = df_clean[self.feature_columns]
            y = df_clean['NASA_F_scaled']

            # ğŸ” ç‰¹å¾´é‡ã®çµ±è¨ˆæƒ…å ±ã‚’è©³ç´°ã«ãƒ­ã‚°å‡ºåŠ›
            logger.warning(f"ğŸ” ç‰¹å¾´é‡ã®çµ±è¨ˆæƒ…å ±ï¼ˆè¨“ç·´å‰ï¼‰:")
            logger.warning(f"   ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(X)}")
            logger.warning(f"   ç‰¹å¾´é‡æ•°: {len(self.feature_columns)}")
            for col in self.feature_columns[:10]:  # æœ€åˆã®10åˆ—ã®ã¿
                if col in X.columns:
                    col_min = X[col].min()
                    col_max = X[col].max()
                    col_mean = X[col].mean()
                    col_std = X[col].std()
                    logger.warning(f"   {col}: min={col_min:.6f}, max={col_max:.6f}, mean={col_mean:.6f}, std={col_std:.6f}")
            logger.warning(f"   y (NASA_F_scaled): min={y.min():.6f}, max={y.max():.6f}, mean={y.mean():.6f}, std={y.std():.6f}")

            # NaN/infå€¤ã®ãƒã‚§ãƒƒã‚¯ã¨é™¤å»
            # X, yã®ä¸¡æ–¹ã‹ã‚‰ç„¡åŠ¹ãªå€¤ã‚’å«ã‚€è¡Œã‚’é™¤å»
            invalid_mask = X.isna().any(axis=1) | y.isna() | np.isinf(X).any(axis=1) | np.isinf(y)
            if invalid_mask.sum() > 0:
                logger.warning(f"âš ï¸ ç„¡åŠ¹ãªå€¤ã‚’å«ã‚€è¡Œã‚’é™¤å»: {invalid_mask.sum()}ä»¶")
                X = X[~invalid_mask]
                y = y[~invalid_mask]

            if len(X) < 10:
                raise ValueError(f"æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ï¼ˆ{len(X)}ä»¶ï¼‰ã€‚æœ€ä½10ä»¶å¿…è¦ã§ã™ã€‚")

            logger.warning(f"âœ… ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼å®Œäº†: ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œ={len(X)}ä»¶, é™¤å»={invalid_mask.sum()}ä»¶")

            # å…¨ãƒ‡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«è¨“ç·´ï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²ãªã—ï¼‰
            # é‹ç”¨ãƒ•ãƒ­ãƒ¼: æ¯æ—¥ã€å‰æ—¥ã¾ã§ã®å…¨ãƒ‡ãƒ¼ã‚¿ã§å†è¨“ç·´ â†’ å½“æ—¥ã‚’äºˆæ¸¬
            self.model = self._create_model()
            self.model.fit(X, y)

            # GridSearchCVã®å ´åˆã€æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ­ã‚°å‡ºåŠ›
            if hasattr(self.model, 'best_params_'):
                logger.warning(f"ğŸ¯ GridSearchCVæœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {self.model.best_params_}")
                logger.warning(f"ğŸ¯ GridSearchCVæœ€è‰¯ã‚¹ã‚³ã‚¢: {-self.model.best_score_:.4f} (RMSE)")

            # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹è©•ä¾¡
            train_pred = self.model.predict(X)
            train_rmse = np.sqrt(mean_squared_error(y, train_pred))
            train_mae = mean_absolute_error(y, train_pred)
            train_r2 = r2_score(y, train_pred)

            results = {
                'train_rmse': float(train_rmse),
                'train_mae': float(train_mae),
                'train_r2': float(train_r2),
                'training_samples': len(X),
                'feature_count': len(self.feature_columns),
                'data_quality': data_quality,
                'model_type': self.config.MODEL_TYPE
            }

            # feature_importanceã¯RandomForestã®ã¿
            if hasattr(self.model, 'feature_importances_'):
                results['feature_importance'] = dict(zip(self.feature_columns, self.model.feature_importances_))
            elif hasattr(self.model, 'coef_'):
                # LinearRegressionã®å ´åˆã¯ä¿‚æ•°ã‚’è¨˜éŒ²
                results['feature_coefficients'] = dict(zip(self.feature_columns, self.model.coef_))

            if self.config.LOG_MODEL_TRAINING:
                logger.info(f"ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº† - RMSE: {train_rmse:.4f}, MAE: {train_mae:.4f}, RÂ²: {train_r2:.3f}, ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(X)}")

            return results

        except Exception as e:
            logger.error(f"ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã‚¨ãƒ©ãƒ¼: {e}")
            return {'error': str(e)}

    def walk_forward_validation_train(self, df_enhanced: pd.DataFrame) -> Dict:
        """
        Walk Forward Validationã«ã‚ˆã‚‹è¨“ç·´ (webhooktest.pyå½¢å¼ã®ç‰¹å¾´é‡)
        éå»ã®ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´ã—ã€ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡
        """
        try:
            data_quality = self.check_data_quality(df_enhanced)

            if len(df_enhanced) < 10:
                raise ValueError(f"Walk Forward Validationã«ã¯æœ€ä½10å€‹ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™ï¼ˆç¾åœ¨: {len(df_enhanced)}ä»¶ï¼‰")

            # NaNå€¤ã‚’å«ã‚€è¡Œã‚’é™¤å¤–
            required_cols = ['SDNN_scaled', 'Lorenz_Area_scaled', 'NASA_F_scaled', 'Duration_scaled']
            df_clean = df_enhanced.dropna(subset=required_cols).copy()

            if df_clean.empty:
                raise ValueError("æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚SDNN, Lorenz_Area, NASA_F, DurationãŒã™ã¹ã¦å¿…è¦ã§ã™ã€‚")

            # æ´»å‹•ã‚«ãƒ†ã‚´ãƒªåˆ—ã‚’å–å¾—
            activity_cols = [col for col in df_clean.columns if col.startswith('activity_')]
            self.activity_columns = activity_cols

            # æ›œæ—¥åˆ—ã‚’å–å¾—
            weekday_cols = [col for col in df_clean.columns if col.startswith('weekday_')]

            # æ™‚é–“ç‰¹å¾´é‡
            time_features = ['hour_sin', 'hour_cos']

            # ç‰¹å¾´é‡ãƒªã‚¹ãƒˆ (webhooktest.pyå½¢å¼ + Duration)
            feature_list = ['SDNN_scaled', 'Lorenz_Area_scaled', 'Duration_scaled'] + activity_cols + time_features + weekday_cols
            self.feature_columns = feature_list

            # NaN/infå€¤ã®ãƒã‚§ãƒƒã‚¯ã¨é™¤å»ï¼ˆWalk Forward Validationå®Ÿè¡Œå‰ï¼‰
            X_check = df_clean[self.feature_columns]
            y_check = df_clean['NASA_F_scaled']

            # ğŸ” ç‰¹å¾´é‡ã®çµ±è¨ˆæƒ…å ±ã‚’è©³ç´°ã«ãƒ­ã‚°å‡ºåŠ›ï¼ˆWalk Forward Validationï¼‰
            logger.warning(f"ğŸ” ç‰¹å¾´é‡ã®çµ±è¨ˆæƒ…å ±ï¼ˆWalk Forwardè¨“ç·´å‰ï¼‰:")
            logger.warning(f"   ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(X_check)}")
            logger.warning(f"   ç‰¹å¾´é‡æ•°: {len(self.feature_columns)}")
            for col in self.feature_columns[:10]:  # æœ€åˆã®10åˆ—ã®ã¿
                if col in X_check.columns:
                    col_min = X_check[col].min()
                    col_max = X_check[col].max()
                    col_mean = X_check[col].mean()
                    col_std = X_check[col].std()
                    logger.warning(f"   {col}: min={col_min:.6f}, max={col_max:.6f}, mean={col_mean:.6f}, std={col_std:.6f}")
            logger.warning(f"   y (NASA_F_scaled): min={y_check.min():.6f}, max={y_check.max():.6f}, mean={y_check.mean():.6f}, std={y_check.std():.6f}")

            invalid_mask = X_check.isna().any(axis=1) | y_check.isna() | np.isinf(X_check).any(axis=1) | np.isinf(y_check)
            if invalid_mask.sum() > 0:
                logger.warning(f"âš ï¸ ç„¡åŠ¹ãªå€¤ã‚’å«ã‚€è¡Œã‚’é™¤å»: {invalid_mask.sum()}ä»¶")
                df_clean = df_clean[~invalid_mask].copy()

            if len(df_clean) < 10:
                raise ValueError(f"æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ï¼ˆ{len(df_clean)}ä»¶ï¼‰ã€‚æœ€ä½10ä»¶å¿…è¦ã§ã™ã€‚")

            logger.warning(f"âœ… ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼å®Œäº†: ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œ={len(df_clean)}ä»¶, é™¤å»={invalid_mask.sum()}ä»¶")

            # Walk Forward Validation: éå»ã®ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´ã€ç¾åœ¨ã‚’äºˆæ¸¬
            predictions = []
            actuals = []

            # æœ€åˆã®30%ã¯ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—æœŸé–“ã¨ã—ã¦ä½¿ç”¨
            start_idx = max(10, int(len(df_clean) * 0.3))

            for i in range(start_idx, len(df_clean)):
                # éå»ã®ãƒ‡ãƒ¼ã‚¿(iä»¥å‰)ã§è¨“ç·´
                train_data = df_clean.iloc[:i]
                X_train = train_data[self.feature_columns]
                y_train = train_data['NASA_F_scaled']

                # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
                model = self._create_model()
                model.fit(X_train, y_train)

                # ç¾åœ¨(iç•ªç›®)ã®ãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬
                current_data = df_clean.iloc[i:i+1]
                X_current = current_data[self.feature_columns]
                y_current = current_data['NASA_F_scaled'].values[0]

                prediction = model.predict(X_current)[0]

                predictions.append(prediction)
                actuals.append(y_current)

            if len(predictions) == 0:
                raise ValueError("æœ‰åŠ¹ãªäºˆæ¸¬ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")

            # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«: å…¨ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´
            X_all = df_clean[self.feature_columns]
            y_all = df_clean['NASA_F_scaled']

            self.model = self._create_model()
            self.model.fit(X_all, y_all)

            # GridSearchCVã®å ´åˆã€æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ­ã‚°å‡ºåŠ›
            if hasattr(self.model, 'best_params_'):
                logger.warning(f"ğŸ¯ GridSearchCVæœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆæœ€çµ‚ãƒ¢ãƒ‡ãƒ«ï¼‰: {self.model.best_params_}")
                logger.warning(f"ğŸ¯ GridSearchCVæœ€è‰¯ã‚¹ã‚³ã‚¢ï¼ˆæœ€çµ‚ãƒ¢ãƒ‡ãƒ«ï¼‰: {-self.model.best_score_:.4f} (RMSE)")

            # è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            predictions_array = np.array(predictions)
            actuals_array = np.array(actuals)

            rmse = np.sqrt(mean_squared_error(actuals_array, predictions_array))
            mae = mean_absolute_error(actuals_array, predictions_array)
            r2 = r2_score(actuals_array, predictions_array)

            # äºˆæ¸¬å€¤ã®å¤šæ§˜æ€§ãƒã‚§ãƒƒã‚¯
            prediction_std = np.std(predictions_array)
            prediction_unique = len(np.unique(np.round(predictions_array, 2)))

            results = {
                'walk_forward_rmse': float(rmse),
                'walk_forward_mae': float(mae),
                'walk_forward_r2': float(r2),
                'total_predictions': len(predictions),
                'training_samples': len(df_clean),
                'feature_count': len(self.feature_columns),
                'data_quality': data_quality,
                'model_type': self.config.MODEL_TYPE,
                'prediction_diversity': {
                    'std': float(prediction_std),
                    'unique_values': int(prediction_unique),
                    'is_diverse': prediction_std > 0.05 and prediction_unique > 3
                }
            }

            # feature_importanceã¯RandomForestã®ã¿
            if hasattr(self.model, 'feature_importances_'):
                results['feature_importance'] = dict(zip(self.feature_columns, self.model.feature_importances_))
            elif hasattr(self.model, 'coef_'):
                # LinearRegressionã®å ´åˆã¯ä¿‚æ•°ã‚’è¨˜éŒ²
                results['feature_coefficients'] = dict(zip(self.feature_columns, self.model.coef_))

            if self.config.LOG_MODEL_TRAINING:
                logger.info(f"Walk Forward Validationå®Œäº† - RMSE: {rmse:.4f}, MAE: {mae:.4f}, RÂ²: {r2:.3f}, äºˆæ¸¬æ•°: {len(predictions)}")

            return results

        except Exception as e:
            logger.error(f"Walk Forward Validationè¨“ç·´ã‚¨ãƒ©ãƒ¼: {e}")
            return {'error': str(e)}

    def predict_single_activity(self, activity_category: str, duration: int = 60, current_time: datetime = None) -> dict:
        """
        å˜ä¸€ã®æ´»å‹•ã«å¯¾ã—ã¦ãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å€¤ã‚’äºˆæ¸¬ (webhooktest.pyå½¢å¼)

        æ³¨æ„: å±¥æ­´ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ã€SDNN_scaled, Lorenz_Area_scaledã«å›ºå®šå€¤ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
        predict_with_historyã®ä½¿ç”¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚
        """
        try:
            if self.model is None:
                return {
                    'predicted_frustration': np.nan,
                    'confidence': 0.0,
                    'error': 'ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´ã•ã‚Œã¦ã„ã¾ã›ã‚“'
                }

            if current_time is None:
                current_time = datetime.now()

            # æ´»å‹•ã‚«ãƒ†ã‚´ãƒªã®è¡¨è¨˜ã‚†ã‚Œã‚’æ­£è¦åŒ–ï¼ˆå¤–éƒ¨ã‹ã‚‰ç›´æ¥å‘¼ã°ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ï¼‰
            activity_category = ACTIVITY_NORMALIZATION_MAP.get(activity_category, activity_category)

            # ç‰¹å¾´é‡ã‚’æ§‹ç¯‰
            features = {}

            # Durationã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
            duration_min = 15
            duration_max = 720
            duration_scaled = (duration - duration_min) / (duration_max - duration_min)
            duration_scaled = np.clip(duration_scaled, 0, 1)
            features['Duration_scaled'] = duration_scaled

            # æ™‚é–“ç‰¹å¾´é‡
            hour_rad = 2 * np.pi * current_time.hour / 24
            features['hour_sin'] = np.sin(hour_rad)
            features['hour_cos'] = np.cos(hour_rad)

            # æ›œæ—¥ã®One-Hot (ç¾åœ¨ã®æ›œæ—¥ã®ã¿1ã€ä»–ã¯0)
            weekday_str = current_time.strftime('%a')
            for day in ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']:
                features[f'weekday_{day}'] = 1 if weekday_str == day else 0

            # æ´»å‹•ã‚«ãƒ†ã‚´ãƒªã®One-Hot (æŒ‡å®šã•ã‚ŒãŸæ´»å‹•ã®ã¿1ã€ä»–ã¯0)
            for activity in KNOWN_ACTIVITIES:
                features[f'activity_{activity}'] = 1 if activity_category == activity else 0

            # ç”Ÿä½“æƒ…å ± (å±¥æ­´ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®å¹³å‡å€¤ã‚’ä½¿ç”¨)
            # ã“ã‚Œã¯æ¨å¥¨ã•ã‚Œãªã„æ–¹æ³•ã§ã™ã€‚predict_with_historyã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
            features['SDNN_scaled'] = 0.5
            features['Lorenz_Area_scaled'] = 0.5
            logger.warning(f"ç”Ÿä½“æƒ…å ±ã«å›ºå®šå€¤ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚predict_with_historyã®ä½¿ç”¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚")

            # DataFrameã«å¤‰æ›ã—ã€ãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´é‡é †ã«ä¸¦ã¹ã‚‹
            feature_df = pd.DataFrame([features])
            for col in self.feature_columns:
                if col not in feature_df.columns:
                    feature_df[col] = 0.0
            feature_df = feature_df[self.feature_columns]

            # äºˆæ¸¬å®Ÿè¡Œ (0-1ã‚¹ã‚±ãƒ¼ãƒ«)
            prediction_scaled = self.model.predict(feature_df)[0]

            # å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ« (0-20) ã«æˆ»ã™
            prediction = prediction_scaled * 20.0

            return {
                'predicted_frustration': float(prediction),
                'confidence': 0.3,  # å›ºå®šå€¤ä½¿ç”¨ã®ãŸã‚ä½ä¿¡é ¼åº¦
                'activity_category': activity_category,
                'duration': duration,
                'timestamp': current_time,
                'used_historical_data': False
            }

        except Exception as e:
            logger.error(f"å˜ä¸€æ´»å‹•äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'predicted_frustration': np.nan,
                'confidence': 0.0,
                'error': str(e)
            }

    def predict_from_row(self, row_data: pd.Series) -> dict:
        """
        ãƒ‡ãƒ¼ã‚¿è¡Œã‹ã‚‰ç›´æ¥äºˆæ¸¬ï¼ˆå®Ÿæ¸¬å€¤ã®ç”Ÿä½“æƒ…å ±ã‚’ä½¿ç”¨ï¼‰

        Args:
            row_data: preprocess_activity_dataã¨aggregate_fitbit_by_activityã‚’çµŒãŸãƒ‡ãƒ¼ã‚¿è¡Œ

        Returns:
            äºˆæ¸¬çµæœ
        """
        try:
            if self.model is None:
                return {
                    'predicted_frustration': np.nan,
                    'confidence': 0.0,
                    'error': 'ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´ã•ã‚Œã¦ã„ã¾ã›ã‚“'
                }

            # ç‰¹å¾´é‡ã‚’æ§‹ç¯‰ï¼ˆè¡Œãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç›´æ¥å–å¾—ï¼‰
            features = {}

            # Durationï¼ˆæ—¢ã«ã‚¹ã‚±ãƒ¼ãƒ«æ¸ˆã¿ã®ã¯ãšï¼‰
            if 'Duration_scaled' in row_data.index:
                features['Duration_scaled'] = row_data['Duration_scaled']
            else:
                # ã‚¹ã‚±ãƒ¼ãƒ«æ¸ˆã¿ã§ãªã„å ´åˆã¯è¨ˆç®—
                duration = row_data.get('Duration', 60)
                duration_min = 15
                duration_max = 720
                duration_scaled = (duration - duration_min) / (duration_max - duration_min)
                duration_scaled = np.clip(duration_scaled, 0, 1)
                features['Duration_scaled'] = duration_scaled

            # æ™‚é–“ç‰¹å¾´é‡ï¼ˆæ—¢ã«è¨ˆç®—æ¸ˆã¿ï¼‰
            if 'hour_sin' in row_data.index and 'hour_cos' in row_data.index:
                features['hour_sin'] = row_data['hour_sin']
                features['hour_cos'] = row_data['hour_cos']
            else:
                # è¨ˆç®—ã•ã‚Œã¦ã„ãªã„å ´åˆã¯å†è¨ˆç®—
                timestamp = pd.to_datetime(row_data.get('Timestamp'))
                hour_rad = 2 * np.pi * timestamp.hour / 24
                features['hour_sin'] = np.sin(hour_rad)
                features['hour_cos'] = np.cos(hour_rad)

            # æ›œæ—¥ã®One-Hotï¼ˆæ—¢ã«è¨ˆç®—æ¸ˆã¿ï¼‰
            for day in ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']:
                weekday_col = f'weekday_{day}'
                features[weekday_col] = row_data.get(weekday_col, 0)

            # æ´»å‹•ã‚«ãƒ†ã‚´ãƒªã®One-Hotï¼ˆæ—¢ã«è¨ˆç®—æ¸ˆã¿ï¼‰
            for activity in KNOWN_ACTIVITIES:
                activity_col = f'activity_{activity}'
                features[activity_col] = row_data.get(activity_col, 0)

            # ç”Ÿä½“æƒ…å ±ï¼ˆå®Ÿæ¸¬å€¤ã‚’ç›´æ¥ä½¿ç”¨ï¼‰
            if 'SDNN_scaled' in row_data.index:
                features['SDNN_scaled'] = row_data['SDNN_scaled']
            else:
                logger.error("SDNN_scaledåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return {
                    'predicted_frustration': np.nan,
                    'confidence': 0.0,
                    'error': 'SDNN_scaledåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'
                }

            if 'Lorenz_Area_scaled' in row_data.index:
                features['Lorenz_Area_scaled'] = row_data['Lorenz_Area_scaled']
            else:
                logger.error("Lorenz_Area_scaledåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return {
                    'predicted_frustration': np.nan,
                    'confidence': 0.0,
                    'error': 'Lorenz_Area_scaledåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'
                }

            # NaNãƒã‚§ãƒƒã‚¯
            if pd.isna(features['SDNN_scaled']) or pd.isna(features['Lorenz_Area_scaled']):
                return {
                    'predicted_frustration': np.nan,
                    'confidence': 0.0,
                    'error': 'ç”Ÿä½“æƒ…å ±ãŒNaNã§ã™'
                }

            # DataFrameã«å¤‰æ›
            feature_df = pd.DataFrame([features])
            for col in self.feature_columns:
                if col not in feature_df.columns:
                    feature_df[col] = 0
            feature_df = feature_df[self.feature_columns]

            # ãƒ‡ãƒãƒƒã‚°: å…¥åŠ›ç‰¹å¾´é‡ã‚’ç¢ºèª
            logger.warning(f"ğŸ” predict_from_row: æ´»å‹•={row_data.get('CatSub')}, SDNN={features['SDNN_scaled']:.3f}, Lorenz={features['Lorenz_Area_scaled']:.3f}")

            # äºˆæ¸¬å®Ÿè¡Œ (0-1ã‚¹ã‚±ãƒ¼ãƒ«)
            prediction_scaled = self.model.predict(feature_df)[0]

            logger.warning(f"ğŸ” predict_from_row: äºˆæ¸¬çµæœ scaled={prediction_scaled:.3f}")

            # å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ« (0-20) ã«æˆ»ã™
            prediction = prediction_scaled * 20.0

            logger.warning(f"ğŸ” predict_from_row: äºˆæ¸¬çµæœ Få€¤={prediction:.2f}")

            # NaN/Infã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
            if np.isnan(prediction) or np.isinf(prediction):
                logger.error(f"äºˆæ¸¬å€¤ãŒä¸æ­£ã§ã™ (NaN/Inf): {prediction}")
                return {
                    'predicted_frustration': np.nan,
                    'confidence': 0.0,
                    'error': 'äºˆæ¸¬å€¤ã®è¨ˆç®—ã«å¤±æ•—ã—ã¾ã—ãŸ'
                }

            return {
                'predicted_frustration': float(prediction),
                'confidence': 0.8,  # å®Ÿæ¸¬å€¤ä½¿ç”¨ã®ãŸã‚é«˜ä¿¡é ¼åº¦
                'activity_category': row_data.get('CatSub', 'unknown'),
                'timestamp': row_data.get('Timestamp'),
                'used_actual_biodata': True
            }

        except Exception as e:
            logger.error(f"è¡Œãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'predicted_frustration': np.nan,
                'confidence': 0.0,
                'error': str(e)
            }

    def predict_with_history(self, activity_category: str, duration: int, current_time: datetime, historical_data: pd.DataFrame) -> dict:
        """
        éå»ã®å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦äºˆæ¸¬ (webhooktest.pyå½¢å¼)

        Args:
            activity_category: æ´»å‹•ã‚«ãƒ†ã‚´ãƒª
            duration: æ´»å‹•æ™‚é–“ï¼ˆåˆ†ï¼‰
            current_time: ç¾åœ¨æ™‚åˆ»
            historical_data: éå»ã®ãƒ‡ãƒ¼ã‚¿ (aggregate_fitbit_by_activityã®å‡ºåŠ›)

        Returns:
            äºˆæ¸¬çµæœ
        """
        try:
            if self.model is None:
                return self.predict_single_activity(activity_category, duration, current_time)

            if historical_data.empty:
                logger.warning("å±¥æ­´ãƒ‡ãƒ¼ã‚¿ãŒç©ºã®ãŸã‚ã€å›ºå®šå€¤ã‚’ä½¿ç”¨ã—ã¾ã™")
                return self.predict_single_activity(activity_category, duration, current_time)

            # ç‰¹å¾´é‡ã‚’æ§‹ç¯‰
            features = {}

            # Durationã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
            duration_min = 15
            duration_max = 720
            duration_scaled = (duration - duration_min) / (duration_max - duration_min)
            duration_scaled = np.clip(duration_scaled, 0, 1)
            features['Duration_scaled'] = duration_scaled

            # æ™‚é–“ç‰¹å¾´é‡
            hour_rad = 2 * np.pi * current_time.hour / 24
            features['hour_sin'] = np.sin(hour_rad)
            features['hour_cos'] = np.cos(hour_rad)

            # æ›œæ—¥ã®One-Hot
            weekday_str = current_time.strftime('%a')
            for day in ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']:
                features[f'weekday_{day}'] = 1 if weekday_str == day else 0

            # æ´»å‹•ã‚«ãƒ†ã‚´ãƒªã®One-Hot
            for activity in KNOWN_ACTIVITIES:
                features[f'activity_{activity}'] = 1 if activity_category == activity else 0

            # ç”Ÿä½“æƒ…å ±: å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã®å¹³å‡å€¤ã‚’ä½¿ç”¨
            logger.info(f"Appäºˆæ¸¬: ===== predict_with_historyé–‹å§‹ =====")
            logger.info(f"Appäºˆæ¸¬: activity_category = {activity_category}")
            logger.info(f"Appäºˆæ¸¬: duration = {duration}")
            logger.info(f"Appäºˆæ¸¬: current_time = {current_time}")
            logger.info(f"Appäºˆæ¸¬: historical_data.shape = {historical_data.shape}")

            if 'SDNN_scaled' in historical_data.columns:
                sdnn_mean = historical_data['SDNN_scaled'].dropna().mean()
                logger.info(f"Appäºˆæ¸¬: SDNN_scaledå¹³å‡å€¤ = {sdnn_mean}")
                if pd.isna(sdnn_mean):
                    logger.error("SDNN_scaledã®æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                    return {
                        'predicted_frustration': np.nan,
                        'confidence': 0.0,
                        'error': 'SDNN_scaledã®æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“',
                        'activity_category': activity_category,
                        'duration': duration,
                        'timestamp': current_time,
                        'used_historical_data': False
                    }
                features['SDNN_scaled'] = sdnn_mean
            else:
                logger.error("SDNN_scaledåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return {
                    'predicted_frustration': np.nan,
                    'confidence': 0.0,
                    'error': 'SDNN_scaledåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“',
                    'activity_category': activity_category,
                    'duration': duration,
                    'timestamp': current_time,
                    'used_historical_data': False
                }

            if 'Lorenz_Area_scaled' in historical_data.columns:
                lorenz_mean = historical_data['Lorenz_Area_scaled'].dropna().mean()
                logger.info(f"Appäºˆæ¸¬: Lorenz_Area_scaledå¹³å‡å€¤ = {lorenz_mean}")
                if pd.isna(lorenz_mean):
                    logger.error("Lorenz_Area_scaledã®æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                    return {
                        'predicted_frustration': np.nan,
                        'confidence': 0.0,
                        'error': 'Lorenz_Area_scaledã®æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“',
                        'activity_category': activity_category,
                        'duration': duration,
                        'timestamp': current_time,
                        'used_historical_data': False
                    }
                features['Lorenz_Area_scaled'] = lorenz_mean
            else:
                logger.error("Lorenz_Area_scaledåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return {
                    'predicted_frustration': np.nan,
                    'confidence': 0.0,
                    'error': 'Lorenz_Area_scaledåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“',
                    'activity_category': activity_category,
                    'duration': duration,
                    'timestamp': current_time,
                    'used_historical_data': False
                }

            # DataFrameã«å¤‰æ›
            feature_df = pd.DataFrame([features])
            for col in self.feature_columns:
                if col not in feature_df.columns:
                    feature_df[col] = 0
            feature_df = feature_df[self.feature_columns]

            logger.info(f"Appäºˆæ¸¬: feature_df shape = {feature_df.shape}")
            logger.info(f"Appäºˆæ¸¬: feature_df ã®ä¸»è¦ãªå€¤:")
            for col in ['SDNN_scaled', 'Lorenz_Area_scaled', 'hour_sin', 'hour_cos']:
                if col in feature_df.columns:
                    logger.info(f"  - {col} = {feature_df[col].iloc[0]}")

            # äºˆæ¸¬å®Ÿè¡Œ (0-1ã‚¹ã‚±ãƒ¼ãƒ«)
            logger.info("Appäºˆæ¸¬: ===== ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ã‚’å®Ÿè¡Œã—ã¾ã™ =====")
            logger.info(f"Appäºˆæ¸¬: self.model ã®ã‚¿ã‚¤ãƒ— = {type(self.model)}")

            prediction_scaled = self.model.predict(feature_df)[0]

            logger.info(f"Appäºˆæ¸¬: ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬çµæœï¼ˆã‚¹ã‚±ãƒ¼ãƒ«æ¸ˆã¿ 0-1ï¼‰= {prediction_scaled}")

            # å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ« (0-20) ã«æˆ»ã™
            prediction = prediction_scaled * 20.0

            logger.info(f"Appäºˆæ¸¬: ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›ï¼ˆÃ—20ï¼‰å¾Œã®Få€¤ = {prediction}")
            logger.info(f"Appäºˆæ¸¬: ===== ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬å®Œäº† =====")

            # NaN/Infã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
            if np.isnan(prediction) or np.isinf(prediction):
                logger.error(f"äºˆæ¸¬å€¤ãŒä¸æ­£ã§ã™ (NaN/Inf): {prediction}")
                return {
                    'predicted_frustration': np.nan,
                    'confidence': 0.0,
                    'error': 'äºˆæ¸¬å€¤ã®è¨ˆç®—ã«å¤±æ•—ã—ã¾ã—ãŸ',
                    'activity_category': activity_category,
                    'duration': duration,
                    'timestamp': current_time,
                    'used_historical_data': True
                }

            return {
                'predicted_frustration': float(prediction),
                'confidence': 0.7,
                'activity_category': activity_category,
                'duration': duration,
                'timestamp': current_time,
                'used_historical_data': True,
                'historical_records': len(historical_data)
            }

        except Exception as e:
            logger.error(f"å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ãŸäºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
            return self.predict_single_activity(activity_category, duration, current_time)

    def get_prediction_confidence(self, prediction: float, features: dict) -> float:
        """
        äºˆæ¸¬ã®ä¿¡é ¼åº¦ã‚’è¨ˆç®—
        """
        try:
            # åŸºæœ¬ä¿¡é ¼åº¦
            base_confidence = 0.7 if 1 <= prediction <= 20 else 0.3

            # ç‰¹å¾´é‡ã®å®Œå…¨æ€§
            feature_completeness = len([v for v in features.values() if v != 0]) / len(features)
            completeness_bonus = feature_completeness * 0.2

            confidence = min(0.95, base_confidence + completeness_bonus)
            return confidence

        except Exception as e:
            logger.error(f"ä¿¡é ¼åº¦è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return 0.0

    def save_model(self, filepath: str):
        """ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜"""
        try:
            model_data = {
                'model': self.model,
                'feature_columns': self.feature_columns,
                'activity_columns': self.activity_columns
            }
            joblib.dump(model_data, filepath)
            if self.config.LOG_MODEL_TRAINING:
                logger.info(f"ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filepath}")
        except Exception as e:
            logger.error(f"ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def load_model(self, filepath: str) -> bool:
        """ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        try:
            if os.path.exists(filepath):
                model_data = joblib.load(filepath)
                self.model = model_data['model']
                self.feature_columns = model_data['feature_columns']
                self.activity_columns = model_data.get('activity_columns', [])
                if self.config.LOG_MODEL_TRAINING:
                    logger.info(f"ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {filepath}")
                return True
            else:
                logger.warning(f"ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {filepath}")
                return False
        except Exception as e:
            logger.error(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return False
