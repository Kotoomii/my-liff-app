"""
ãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å€¤äºˆæ¸¬ãƒ»åå®Ÿä»®æƒ³èª¬æ˜ã‚·ã‚¹ãƒ†ãƒ 
è¡Œå‹•å¤‰æ›´ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§ã®äºˆæ¸¬ã¨DiCEã«ã‚ˆã‚‹æ”¹å–„ææ¡ˆã‚’æä¾›ã™ã‚‹Flaskã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
"""

from flask import Flask, render_template, jsonify, request
import os
import logging
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo
from typing import Dict
import threading
import time
import json

# æ—¥æœ¬æ¨™æº–æ™‚ï¼ˆJSTï¼‰ã®ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³
JST = ZoneInfo('Asia/Tokyo')

from ml_model import FrustrationPredictor
from sheets_connector import SheetsConnector
from counterfactual_explainer import ActivityCounterfactualExplainer
from llm_feedback_generator import LLMFeedbackGenerator
from scheduler import FeedbackScheduler
from config import Config

app = Flask(__name__)

# Cloud Runç”¨ãƒ­ã‚°è¨­å®š
config = Config()

# æ§‹é€ åŒ–ãƒ­ã‚°ç”¨ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼
class StructuredFormatter(logging.Formatter):
    def format(self, record):
        log_obj = {
            'severity': record.levelname,
            'message': record.getMessage(),
            'timestamp': datetime.utcnow().isoformat() + 'Z'
        }
        if hasattr(record, 'user_id'):
            log_obj['user_id'] = record.user_id
        if hasattr(record, 'endpoint'):
            log_obj['endpoint'] = record.endpoint
        if record.exc_info:
            log_obj['exception'] = self.formatException(record.exc_info)
        return json.dumps(log_obj)

# ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«è¨­å®š
# ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šä¸€æ™‚çš„ã«INFOãƒ¬ãƒ™ãƒ«ã«è¨­å®šï¼ˆDiCEã¨Appäºˆæ¸¬ã®èª¿æŸ»ã®ãŸã‚ï¼‰
log_level_str = os.environ.get('LOG_LEVEL', config.LOG_LEVEL)
if log_level_str == 'WARNING' and not config.IS_CLOUD_RUN:
    # ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ã¯è‡ªå‹•çš„ã«INFOãƒ¬ãƒ™ãƒ«ã«ã—ã¦ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ã‚’è¡¨ç¤º
    log_level = logging.INFO
    print("âš ï¸ ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰: ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã‚’INFOã«è¨­å®šã—ã¾ã—ãŸ")
else:
    log_level = getattr(logging, log_level_str.upper(), logging.WARNING)

# Cloud Runç’°å¢ƒã§ã¯æ§‹é€ åŒ–ãƒ­ã‚°ã‚’ä½¿ç”¨
if config.IS_CLOUD_RUN:
    handler = logging.StreamHandler()
    handler.setFormatter(StructuredFormatter())
    logging.basicConfig(level=log_level, handlers=[handler])
else:
    # ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ã¯æ¨™æº–ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

logger = logging.getLogger(__name__)

# Flaskæ¨™æº–ãƒ­ã‚°ã®æŠ‘åˆ¶ï¼ˆHTTPã‚¢ã‚¯ã‚»ã‚¹ãƒ­ã‚°ã‚’ç„¡åŠ¹åŒ–ï¼‰
log = logging.getLogger('werkzeug')
log.setLevel(logging.ERROR)
log.disabled = True  # å®Œå…¨ã«ç„¡åŠ¹åŒ–

# GUnicornã‚¢ã‚¯ã‚»ã‚¹ãƒ­ã‚°ã‚‚æŠ‘åˆ¶
gunicorn_logger = logging.getLogger('gunicorn.access')
gunicorn_logger.disabled = True
gunicorn_error_logger = logging.getLogger('gunicorn.error')
gunicorn_error_logger.setLevel(logging.ERROR)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
sheets_connector = SheetsConnector()
explainer = ActivityCounterfactualExplainer()
feedback_generator = LLMFeedbackGenerator()

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã®ãƒ¢ãƒ‡ãƒ«ç®¡ç†
user_predictors = {}  # {user_id: FrustrationPredictor}

# schedulerã«user_predictorsã‚’æ¸¡ã™ï¼ˆdata_monitor_loopã§å­¦ç¿’æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ã‚’å…±æœ‰ï¼‰
scheduler = FeedbackScheduler(user_predictors=user_predictors)
logger.warning(f"ğŸ”§ main.py: user_predictorsè¾æ›¸ã®ID={id(user_predictors)}")

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åˆæœŸåŒ–ãƒ•ãƒ©ã‚°
_app_initialized = False

def get_predictor(user_id: str) -> FrustrationPredictor:
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã®predictorã‚’å–å¾—ï¼ˆå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆï¼‰
    """
    if user_id not in user_predictors:
        logger.warning(f"ğŸ†• æ–°ã—ã„predictorã‚’ä½œæˆ: user_id={user_id}, è¾æ›¸ID={id(user_predictors)}")
        user_predictors[user_id] = FrustrationPredictor()
        logger.warning(f"âœ… predictorä½œæˆå®Œäº†: keys={list(user_predictors.keys())}")
    return user_predictors[user_id]

def ensure_model_trained(user_id: str, force_retrain: bool = False) -> dict:
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
    å¿…è¦ã«å¿œã˜ã¦è‡ªå‹•è¨“ç·´ã‚’å®Ÿè¡Œ

    Args:
        user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
        force_retrain: å¼·åˆ¶å†è¨“ç·´

    Returns:
        è¨“ç·´çµæœã¾ãŸã¯çŠ¶æ…‹æƒ…å ±
    """
    predictor = get_predictor(user_id)

    # ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´æ¸ˆã¿ã§å¼·åˆ¶å†è¨“ç·´ã§ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
    logger.warning(f"ğŸ” ensure_model_trained: user_id={user_id}, predictor.model is None={predictor.model is None}, force_retrain={force_retrain}")
    if predictor.model is not None and not force_retrain:
        logger.warning(f"âœ… ãƒ¢ãƒ‡ãƒ«ã¯æ—¢ã«è¨“ç·´æ¸ˆã¿ã§ã™: user_id={user_id}")
        return {
            'status': 'already_trained',
            'message': 'ãƒ¢ãƒ‡ãƒ«ã¯æ—¢ã«è¨“ç·´æ¸ˆã¿ã§ã™'
        }

    logger.warning(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã‚’é–‹å§‹ã—ã¾ã™: user_id={user_id}")

    # ãƒ‡ãƒ¼ã‚¿å–å¾—
    activity_data = sheets_connector.get_activity_data(user_id)
    fitbit_data = sheets_connector.get_fitbit_data(user_id)

    if activity_data.empty:
        return {
            'status': 'no_data',
            'message': 'ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“',
            'user_id': user_id
        }

    # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
    activity_processed = predictor.preprocess_activity_data(activity_data)
    df_enhanced = predictor.aggregate_fitbit_by_activity(activity_processed, fitbit_data)

    # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
    data_quality = predictor.check_data_quality(df_enhanced)

    if len(df_enhanced) < 10:
        return {
            'status': 'insufficient_data',
            'message': f'ãƒ‡ãƒ¼ã‚¿ä¸è¶³: {len(df_enhanced)}ä»¶ < 10ä»¶',
            'user_id': user_id,
            'data_count': len(df_enhanced),
            'data_quality': data_quality
        }

    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
    try:
        training_results = predictor.walk_forward_validation_train(df_enhanced)

        # è¨“ç·´çµæœã«ã‚¨ãƒ©ãƒ¼ãŒå«ã¾ã‚Œã¦ã„ãªã„ã‹ãƒã‚§ãƒƒã‚¯
        if 'error' in training_results:
            logger.error(f"ãƒ¢ãƒ‡ãƒ«è¨“ç·´å¤±æ•—: user_id={user_id}, error={training_results['error']}")
            return {
                'status': 'error',
                'message': f"è¨“ç·´å¤±æ•—: {training_results['error']}",
                'user_id': user_id,
                'data_quality': data_quality
            }

        # ãƒ¢ãƒ‡ãƒ«ãŒæ­£å¸¸ã«åˆæœŸåŒ–ã•ã‚ŒãŸã‹æœ€çµ‚ç¢ºèª
        if predictor.model is None:
            logger.error(f"âŒ ãƒ¢ãƒ‡ãƒ«è¨“ç·´å¾Œã‚‚modelãŒNoneã§ã™: user_id={user_id}")
            logger.error(f"   training_results: {training_results}")
            return {
                'status': 'error',
                'message': 'ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ',
                'user_id': user_id,
                'data_quality': data_quality
            }

        logger.warning(f"âœ…âœ…âœ… ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†: user_id={user_id}, "
                   f"RMSE={training_results.get('walk_forward_rmse', 0):.4f}, "
                   f"RÂ²={training_results.get('walk_forward_r2', 0):.3f}, "
                   f"model_type={type(predictor.model).__name__}")
        return {
            'status': 'success',
            'message': 'ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†',
            'user_id': user_id,
            'data_count': len(df_enhanced),
            'training_results': training_results,
            'data_quality': data_quality
        }
    except Exception as e:
        logger.error(f"ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã‚¨ãƒ©ãƒ¼: user_id={user_id}, error={e}", exc_info=True)
        return {
            'status': 'error',
            'message': f'è¨“ç·´ã‚¨ãƒ©ãƒ¼: {str(e)}',
            'user_id': user_id
        }

# ãƒ‡ãƒ¼ã‚¿æ›´æ–°ç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰
data_monitor_thread = None
data_monitor_running = False
last_prediction_result = {}  # å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®äºˆæ¸¬çµæœã‚’ä¿å­˜: {user_id: prediction_data}

def check_fitbit_data_availability(row):
    """
    Fitbitãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯ã™ã‚‹
    äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ãŒä½¿ç”¨ã™ã‚‹SDNN_scaledã¨Lorenz_Area_scaledãŒå­˜åœ¨ã™ã‚‹ã‹ã‚’ç¢ºèª
    """
    try:
        # äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ãŒä½¿ç”¨ã™ã‚‹å¿…é ˆã‚«ãƒ©ãƒ 
        sdnn_scaled = row.get('SDNN_scaled')
        lorenz_area_scaled = row.get('Lorenz_Area_scaled')

        # ä¸¡æ–¹ãŒæœ‰åŠ¹ãªæ•°å€¤ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
        sdnn_valid = False
        lorenz_valid = False

        if sdnn_scaled is not None and pd.notna(sdnn_scaled):
            try:
                float_val = float(sdnn_scaled)
                # NaN, Inf, -Infã§ãªã„æœ‰åŠ¹ãªæ•°å€¤ã‹ç¢ºèª
                if not (np.isnan(float_val) or np.isinf(float_val)):
                    sdnn_valid = True
            except (ValueError, TypeError):
                pass

        if lorenz_area_scaled is not None and pd.notna(lorenz_area_scaled):
            try:
                float_val = float(lorenz_area_scaled)
                # NaN, Inf, -Infã§ãªã„æœ‰åŠ¹ãªæ•°å€¤ã‹ç¢ºèª
                if not (np.isnan(float_val) or np.isinf(float_val)):
                    lorenz_valid = True
            except (ValueError, TypeError):
                pass

        is_available = sdnn_valid and lorenz_valid

        if not is_available:
            logger.debug(f"ç”Ÿä½“ãƒ‡ãƒ¼ã‚¿ä¸è¶³: SDNN_scaled={sdnn_valid}, Lorenz_Area_scaled={lorenz_valid}")

        return is_available

    except Exception as e:
        logger.warning(f"Fitbitãƒ‡ãƒ¼ã‚¿å¯ç”¨æ€§ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
        return False

@app.route('/')
def index():
    """ãƒ¡ã‚¤ãƒ³ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ - éå»24æ™‚é–“ã®DiCEçµæœå¯è¦–åŒ–"""
    import time
    timestamp = str(int(time.time()))
    return render_template('frustration_dashboard.html', timestamp=timestamp)

@app.route('/mirror')
def smart_mirror():
    """ã‚¹ãƒãƒ¼ãƒˆãƒŸãƒ©ãƒ¼å°‚ç”¨UI - å®Œå…¨è‡ªå‹•é‹è»¢ãƒ»ã‚¿ãƒƒãƒãƒ¬ã‚¹æ“ä½œ"""
    return render_template('smart_mirror.html')

@app.route('/tablet')
def tablet_mirror():
    """ã‚¿ãƒ–ãƒ¬ãƒƒãƒˆå°‚ç”¨UI - æ‰‹å‹•ãƒ¦ãƒ¼ã‚¶ãƒ¼é¸æŠãƒ»æ—¥æ¬¡å¹³å‡è¡¨ç¤º"""
    return render_template('tablet_mirror.html')

@app.route('/mobile')
def mobile_mirror():
    """ã‚¹ãƒãƒ›å°‚ç”¨UI - æ‰‹å‹•ãƒ¦ãƒ¼ã‚¶ãƒ¼é¸æŠãƒ»ç¸¦ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«è¡¨ç¤º"""
    return render_template('mobile_mirror.html')

@app.route('/trends')
def frustration_trends():
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ¥ãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å€¤æ¨ç§»ç¢ºèªã‚·ãƒ¼ãƒˆ"""
    return render_template('frustration_trends.html')

@app.route('/activity')
def activity_page():
    """æ´»å‹•ãƒ‡ãƒ¼ã‚¿å…¥åŠ›ãƒšãƒ¼ã‚¸ (my-liff-appäº’æ›) - ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆ"""
    from flask import redirect
    return redirect('https://kotoomii.github.io/my-liff-app/activity.html')

@app.route('/nasa')
def nasa_page():
    """NASA-TLXè©•ä¾¡ãƒšãƒ¼ã‚¸ (my-liff-appäº’æ›) - ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆ"""
    from flask import redirect
    return redirect('https://kotoomii.github.io/my-liff-app/nasa.html')

@app.route('/api/frustration/predict', methods=['POST'])
def predict_frustration():
    """
    è¡Œå‹•å¤‰æ›´ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§ã®ãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å€¤äºˆæ¸¬API
    """
    try:
        data = request.get_json()
        user_id = data.get('user_id', 'default')
        target_timestamp = data.get('timestamp')
        
        if target_timestamp:
            target_timestamp = datetime.fromisoformat(target_timestamp)
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã®predictorã‚’å–å¾—
        predictor = get_predictor(user_id)

        # ãƒ‡ãƒ¼ã‚¿å–å¾—
        activity_data = sheets_connector.get_activity_data(user_id)
        fitbit_data = sheets_connector.get_fitbit_data(user_id)

        if activity_data.empty:
            return jsonify({
                'status': 'error',
                'message': 'æ´»å‹•ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'
            }), 400

        # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
        activity_processed = predictor.preprocess_activity_data(activity_data)
        if activity_processed.empty:
            return jsonify({
                'status': 'error',
                'message': 'ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ'
            }), 400

        # Fitbitãƒ‡ãƒ¼ã‚¿ã¨ã®çµ±åˆ
        df_enhanced = predictor.aggregate_fitbit_by_activity(activity_processed, fitbit_data)

        # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
        data_quality = predictor.check_data_quality(df_enhanced)

        # ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´ã•ã‚Œã¦ã„ãªã„å ´åˆã¯è‡ªå‹•è¨“ç·´
        training_info = {'auto_trained': False}
        if predictor.model is None:
            logger.info(f"ãƒ¢ãƒ‡ãƒ«æœªè¨“ç·´: user_id={user_id}, è‡ªå‹•è¨“ç·´ã‚’é–‹å§‹ã—ã¾ã™")
            training_result = ensure_model_trained(user_id)
            training_info = {
                'auto_trained': True,
                'status': training_result.get('status'),
                'message': training_result.get('message')
            }

            # è¨“ç·´ãŒå¤±æ•—ã—ãŸå ´åˆã¯ã‚¨ãƒ©ãƒ¼ã‚’è¿”ã™
            if training_result.get('status') != 'success':
                return jsonify({
                    'status': 'error',
                    'message': f"ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã«å¤±æ•—ã—ã¾ã—ãŸ: {training_result.get('message')}",
                    'user_id': user_id,
                    'data_quality': data_quality,
                    'training_result': training_result
                }), 400

        # ãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å€¤äºˆæ¸¬
        prediction_result = predictor.predict_frustration_at_activity_change(
            df_enhanced, target_timestamp
        )

        # ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±
        training_results = training_info if training_info['auto_trained'] else {'status': 'already_trained'}

        # è¡Œå‹•å¤‰æ›´ã‚¿ã‚¤ãƒŸãƒ³ã‚°ä¸€è¦§
        change_timestamps = predictor.get_activity_change_timestamps(df_enhanced)

        response = {
            'status': 'success',
            'user_id': user_id,
            'prediction': prediction_result,
            'activity_change_timestamps': [ts.isoformat() for ts in change_timestamps],
            'model_performance': training_results,
            'data_quality': data_quality,
            'timestamp': datetime.now().isoformat()
        }

        # ãƒ‡ãƒ¼ã‚¿ä¸è¶³æ™‚ã®è­¦å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
        if not data_quality['is_sufficient']:
            response['warning'] = {
                'message': 'ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚ã€äºˆæ¸¬ç²¾åº¦ãŒä½ã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚',
                'details': data_quality['warnings'],
                'recommendations': data_quality['recommendations']
            }

        return jsonify(response)
        
    except Exception as e:
        logger.error(f"ãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å€¤äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/frustration/daily-dice-schedule', methods=['POST'])
def generate_daily_dice_schedule():
    """
    1æ—¥ã®çµ‚ã‚ã‚Šã«æ™‚é–“ã”ã¨ã®DiCEæ”¹å–„ææ¡ˆã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ç”Ÿæˆ
    """
    try:
        data = request.get_json()
        user_id = data.get('user_id', 'default')
        target_date = data.get('date', datetime.now().date().isoformat())

        if isinstance(target_date, str):
            target_date = datetime.fromisoformat(target_date).date()

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã®predictorã‚’å–å¾—
        predictor = get_predictor(user_id)

        # ãã®æ—¥ã®æ´»å‹•ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        activity_data = sheets_connector.get_activity_data(user_id)
        fitbit_data = sheets_connector.get_fitbit_data(user_id)

        if activity_data.empty:
            return jsonify({
                'status': 'error',
                'message': 'æ´»å‹•ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'
            }), 400

        # æŒ‡å®šæ—¥ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        activity_data['Date'] = pd.to_datetime(activity_data['Timestamp']).dt.date
        daily_activities = activity_data[activity_data['Date'] == target_date].copy()

        if daily_activities.empty:
            return jsonify({
                'status': 'success',
                'user_id': user_id,
                'date': target_date.isoformat(),
                'message': 'ãã®æ—¥ã®æ´»å‹•ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“',
                'schedule': [],
                'recommendations': []
            })

        # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã¨ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
        activity_processed = predictor.preprocess_activity_data(activity_data)
        df_enhanced = predictor.aggregate_fitbit_by_activity(activity_processed, fitbit_data)

        # ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªï¼ˆDiCEå®Ÿè¡Œå‰ã«å¿…é ˆï¼‰
        training_result = ensure_model_trained(user_id)
        if training_result.get('status') not in ['success', 'already_trained']:
            return jsonify({
                'status': 'error',
                'message': f"ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã«å¤±æ•—ã—ã¾ã—ãŸ: {training_result.get('message')}",
                'user_id': user_id,
                'training_result': training_result
            }), 400

        # 1æ™‚é–“ã”ã¨ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ææ¡ˆã‚’ç”Ÿæˆï¼ˆDiCEæ–¹å¼ï¼‰
        dice_result = explainer.generate_hourly_alternatives(
            df_enhanced,  # å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’æ¸¡ã™
            predictor,
            target_date
        )
        
        if dice_result and dice_result.get('hourly_schedule'):
            hourly_schedule = dice_result['hourly_schedule']
            message = dice_result.get('message', 'æ™‚é–“åˆ¥æ”¹å–„ææ¡ˆã‚’ç”Ÿæˆã—ã¾ã—ãŸ')
            total_improvement = dice_result.get('total_improvement', 0)
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šåŸºæœ¬çš„ãªæ™‚é–“åˆ¥ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
            hourly_schedule = []
            daily_activities = daily_activities.sort_values('Timestamp')
            
            for hour in range(24):  # 0-23æ™‚
                hour_start = datetime.combine(target_date, datetime.min.time()) + timedelta(hours=hour)
                hour_end = hour_start + timedelta(hours=1)
                
                # ãã®æ™‚é–“å¸¯ã®æ´»å‹•ã‚’å–å¾—
                hour_activities = daily_activities[
                    (daily_activities['Timestamp'] >= hour_start) & 
                    (daily_activities['Timestamp'] < hour_end)
                ]
                
                if not hour_activities.empty:
                    # å®Ÿéš›ã®æ´»å‹•ã¨ãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å€¤
                    actual_activity = hour_activities.iloc[0]
                    actual_frustration = actual_activity.get('NASA_F', 10.0)
                    
                
                # ãã®æ™‚é–“ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«æƒ…å ±
                hour_info = {
                    'hour': hour,
                    'time_range': f"{hour:02d}:00-{(hour+1):02d}:00",
                    'actual_activity': actual_activity['CatSub'],
                    'actual_frustration': round(float(actual_frustration), 2),
                    'actual_duration': int(actual_activity.get('Duration', 60)),
                    'suggested_activity': None,
                    'suggested_frustration': None,
                    'improvement': 0.0,
                    'has_suggestion': False
                }
                
                # DiCEææ¡ˆãŒã‚ã‚‹å ´åˆ
                if dice_suggestions and dice_suggestions.get('alternatives'):
                    best_alternative = dice_suggestions['alternatives'][0]
                    suggested_frustration = best_alternative.get('predicted_frustration', actual_frustration)
                    improvement = actual_frustration - suggested_frustration
                    
                    if improvement > 0.5:  # 0.5ä»¥ä¸Šã®æ”¹å–„ãŒè¦‹è¾¼ã‚ã‚‹å ´åˆã®ã¿ææ¡ˆ
                        hour_info.update({
                            'suggested_activity': best_alternative.get('activity', actual_activity['CatSub']),
                            'suggested_frustration': round(suggested_frustration, 2),
                            'improvement': round(improvement, 2),
                            'has_suggestion': True
                        })
                        
                        # æ—¥æ¬¡æ¨å¥¨äº‹é …ã«è¿½åŠ 
                        daily_recommendations.append({
                            'time': f"{hour:02d}:00",
                            'original': f"{actual_activity['CatSub']} (ãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: {actual_frustration:.1f})",
                            'suggested': f"{best_alternative.get('activity')} (äºˆæ¸¬ãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: {suggested_frustration:.1f})",
                            'improvement': round(improvement, 2),
                            'reason': f"ã“ã®æ™‚é–“ã«{best_alternative.get('activity')}ã‚’è¡Œã†ã“ã¨ã§ã€ãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’{improvement:.1f}ãƒã‚¤ãƒ³ãƒˆå‰Šæ¸›ã§ãã¾ã™"
                        })
                
                hourly_schedule.append(hour_info)
            
            else:
                # ãã®æ™‚é–“ã«æ´»å‹•ãŒãªã„å ´åˆ
                hourly_schedule.append({
                    'hour': hour,
                    'time_range': f"{hour:02d}:00-{(hour+1):02d}:00",
                    'actual_activity': None,
                    'actual_frustration': None,
                    'actual_duration': 0,
                    'suggested_activity': None,
                    'suggested_frustration': None,
                    'improvement': 0.0,
                    'has_suggestion': False
                })
        
        # å…¨ä½“çµ±è¨ˆ
        total_actual_frustration = sum([h['actual_frustration'] for h in hourly_schedule if h['actual_frustration'] is not None])
        total_suggested_frustration = sum([h['suggested_frustration'] for h in hourly_schedule if h['suggested_frustration'] is not None])
        total_improvement = sum([h['improvement'] for h in hourly_schedule])
        
        # DiCEã«ã‚ˆã‚‹æ™‚é–“åˆ¥æ”¹å–„ææ¡ˆãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼ã«çµ±ä¸€
        if dice_result and dice_result.get('hourly_schedule'):
            return jsonify({
                'status': 'success',
                'user_id': user_id,
                'date': target_date.isoformat(),
                'schedule': dice_result['hourly_schedule'],
                'message': dice_result.get('message', 'ä»Šæ—¥ã“ã®ã‚ˆã†ãªæ´»å‹•ã‚’ã—ã¦ã„ãŸã‚‰ã‚¹ãƒˆãƒ¬ã‚¹ãƒ¬ãƒ™ãƒ«ãŒä¸‹ãŒã£ã¦ã„ã¾ã—ãŸ'),
                'total_improvement': dice_result.get('total_improvement', 0),
                'summary': dice_result.get('summary', ''),
                'confidence': dice_result.get('confidence', 0.7),
                'generated_at': datetime.now().isoformat()
            })
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¿œç­”
            return jsonify({
                'status': 'success',
                'user_id': user_id,
                'date': target_date.isoformat(),
                'schedule': [],
                'message': 'ä»Šæ—¥ã“ã®ã‚ˆã†ãªæ´»å‹•ã‚’ã—ã¦ã„ãŸã‚‰ã‚¹ãƒˆãƒ¬ã‚¹ãƒ¬ãƒ™ãƒ«ãŒä¸‹ãŒã£ã¦ã„ã¾ã—ãŸ',
                'total_improvement': 0,
                'summary': 'ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚ã€è©³ç´°ãªææ¡ˆã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸ',
                'confidence': 0.3,
                'generated_at': datetime.now().isoformat()
            })
        
    except Exception as e:
        logger.error(f"æ—¥æ¬¡DiCEã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/frustration/dice-analysis', methods=['POST'])
def generate_dice_analysis():
    """
    Hourly Logã‹ã‚‰DiCEææ¡ˆã‚’å–å¾—ã™ã‚‹APIï¼ˆDiCEå®Ÿè¡Œã¯ã—ãªã„ï¼‰
    ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ãŒ23:55 JSTï¼ˆ14:55 UTCï¼‰ã«å®Ÿè¡Œã—ãŸDiCEçµæœã‚’èª­ã¿å–ã‚‹ã ã‘
    """
    try:
        data = request.get_json()
        user_id = data.get('user_id', 'default')
        date = data.get('date', datetime.now().strftime('%Y-%m-%d'))

        # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã‚’DEBUGã«å¤‰æ›´ï¼ˆé »ç¹ã«å‘¼ã°ã‚Œã‚‹ãŸã‚ã€WARNINGãƒ¬ãƒ™ãƒ«ã§ã¯å¤šã™ãã‚‹ï¼‰
        if config.ENABLE_DEBUG_LOGS:
            logger.debug(f"ğŸ“Š DiCEåˆ†æå–å¾—: user_id={user_id}, date={date}")

        # Hourly Logã‹ã‚‰DiCEææ¡ˆã‚’å–å¾—
        hourly_log = sheets_connector.get_hourly_log(user_id, date)

        if hourly_log.empty:
            return jsonify({
                'status': 'success',
                'user_id': user_id,
                'dice_analysis': {
                    'type': 'no_data',
                    'timeline': [],
                    'summary': 'ã¾ã DiCEææ¡ˆãŒç”Ÿæˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚23:55ä»¥é™ã«ç¢ºèªã—ã¦ãã ã•ã„ã€‚'
                },
                'timestamp': datetime.now().isoformat()
            })

        # Activity_Dataã‚’å–å¾—ï¼ˆDurationã‚’å–å¾—ã™ã‚‹ãŸã‚ï¼‰
        activity_data = sheets_connector.get_activity_data(user_id)

        # DiCEææ¡ˆãŒã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ã®ã¿æŠ½å‡º
        dice_suggestions = []
        for idx, row in hourly_log.iterrows():
            dice_suggestion = row.get('DiCEææ¡ˆæ´»å‹•å')
            if pd.notna(dice_suggestion) and dice_suggestion != '':
                time_str = row.get('æ™‚åˆ»')
                activity = row.get('æ´»å‹•å')
                predicted_f = row.get('äºˆæ¸¬NASA_F')
                improvement = row.get('æ”¹å–„å¹…')
                improved_f = row.get('æ”¹å–„å¾ŒFå€¤')

                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ä½œæˆï¼ˆãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ãŒDateå‹ã¨ã—ã¦æ‰±ãˆã‚‹ã‚ˆã†ã«ï¼‰
                timestamp_str = f"{date} {time_str}:00" if time_str else f"{date} 00:00:00"

                # æ”¹å–„å¹…ã‚’æ•°å€¤åŒ–
                improvement_value = float(improvement) if pd.notna(improvement) else 0

                # time_rangeã‚’è¨ˆç®—ï¼ˆActivity_Dataã‹ã‚‰Durationã‚’å–å¾—ï¼‰
                time_range = time_str  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
                duration_minutes = 60  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
                try:
                    if not activity_data.empty and 'Timestamp' in activity_data.columns:
                        # è©²å½“ã™ã‚‹æ´»å‹•ã‚’æ¢ã™
                        matching = activity_data[
                            (activity_data['Timestamp'].dt.strftime('%Y-%m-%d %H:%M') == f"{date} {time_str}") &
                            (activity_data['CatSub'] == activity)
                        ]
                        if not matching.empty:
                            duration_minutes = int(matching.iloc[0].get('Duration', 60))
                            # time_rangeã‚’è¨ˆç®—
                            from datetime import datetime as dt_class, timedelta
                            start_time = dt_class.strptime(f"{date} {time_str}", '%Y-%m-%d %H:%M')
                            end_time = start_time + timedelta(minutes=duration_minutes)
                            time_range = f"{time_str}-{end_time.strftime('%H:%M')}"
                except Exception as e:
                    logger.debug(f"time_rangeè¨ˆç®—ã‚¨ãƒ©ãƒ¼ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ä½¿ç”¨ï¼‰: {e}")

                dice_suggestions.append({
                    # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ç”¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼ˆtablet_mirror.htmlï¼‰
                    'timestamp': timestamp_str,
                    'frustration_reduction': improvement_value,
                    # ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰äº’æ›æ€§ç”¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼ˆæ—¢å­˜ã‚³ãƒ¼ãƒ‰ç”¨ï¼‰
                    'time': time_str,
                    'improvement': improvement_value,
                    # å…±é€šãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
                    'original_activity': activity,
                    'original_frustration': float(predicted_f) if pd.notna(predicted_f) else None,
                    'suggested_activity': dice_suggestion,
                    'improved_frustration': float(improved_f) if pd.notna(improved_f) else None,
                    'time_range': time_range  # è¿½åŠ 
                })

        # DiCEåˆ†æçµæœã‚’æ§‹ç¯‰
        if len(dice_suggestions) > 0:
            dice_result = {
                'type': 'dice_analysis',
                'timeline': dice_suggestions,
                'summary': f'{len(dice_suggestions)}ä»¶ã®æ”¹å–„ææ¡ˆãŒã‚ã‚Šã¾ã™ã€‚',
                'total_improvement': sum([s.get('improvement', 0) or 0 for s in dice_suggestions])
            }
        else:
            dice_result = {
                'type': 'no_suggestions',
                'timeline': [],
                'summary': 'DiCEææ¡ˆã¯ã¾ã ç”Ÿæˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚23:55ä»¥é™ã«ç¢ºèªã—ã¦ãã ã•ã„ã€‚'
            }

        # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã‚’DEBUGã«å¤‰æ›´ï¼ˆé »ç¹ã«å‘¼ã°ã‚Œã‚‹ãŸã‚ï¼‰
        if config.ENABLE_DEBUG_LOGS:
            logger.debug(f"âœ… DiCEææ¡ˆå–å¾—å®Œäº†: {len(dice_suggestions)}ä»¶")

        return jsonify({
            'status': 'success',
            'user_id': user_id,
            'dice_analysis': dice_result,
            'timestamp': datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"DiCEåˆ†æå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/frustration/timeline', methods=['POST'])
def get_frustration_timeline():
    """
    éå»24æ™‚é–“ã®ãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å€¤ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³å–å¾—API
    """
    try:
        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ç¢ºèªã¨ä¿®æ­£
        if request.is_json:
            data = request.get_json()
            if data is None:
                data = {}
        else:
            data = {}
        
        user_id = data.get('user_id', 'default')
        date = data.get('date', datetime.now().strftime('%Y-%m-%d'))
        
        if config.ENABLE_DEBUG_LOGS:
            logger.debug(f"Timeline APIå‘¼ã³å‡ºã— - user_id: {user_id}, date: {date}")
        
        # ãƒ‡ãƒ¼ã‚¿å–å¾—
        activity_data = sheets_connector.get_activity_data(user_id)
        fitbit_data = sheets_connector.get_fitbit_data(user_id)
        
        # Google Sheetsæ¥ç¶šã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯é©åˆ‡ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿”ã™
        if activity_data.empty:
            if sheets_connector.gc is None:
                return jsonify({
                    'status': 'success',
                    'date': date,
                    'timeline': [],
                    'is_new_user': True,
                    'message': 'Google Sheetsã«æ¥ç¶šã§ãã¾ã›ã‚“ã€‚èªè¨¼è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚'
                })
            else:
                return jsonify({
                    'status': 'success',
                    'date': date,
                    'timeline': [],
                    'is_new_user': True,
                    'message': 'ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“',
                    'warning': {
                        'message': 'æ–°è¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¾ãŸã¯ãƒ‡ãƒ¼ã‚¿æœªè¨˜éŒ²ã§ã™ã€‚',
                        'recommendations': [
                            'æ´»å‹•ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²ã—ã¦ãã ã•ã„ã€‚',
                            'ãƒ‡ãƒ¼ã‚¿ãŒè“„ç©ã•ã‚Œã‚‹ã¨ã€ãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³äºˆæ¸¬ã¨DiCEææ¡ˆãŒåˆ©ç”¨å¯èƒ½ã«ãªã‚Šã¾ã™ã€‚'
                        ]
                    }
                })
        
        # æŒ‡å®šæ—¥ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        target_date = datetime.strptime(date, '%Y-%m-%d').date()
        if 'Timestamp' in activity_data.columns:
            activity_data['date'] = pd.to_datetime(activity_data['Timestamp']).dt.date
            daily_data = activity_data[activity_data['date'] == target_date]
        else:
            daily_data = pd.DataFrame()
        
        if daily_data.empty:
            return jsonify({
                'status': 'success',
                'date': date,
                'timeline': [],
                'message': 'æŒ‡å®šæ—¥ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'
            })
        
        # Hourly Logã‹ã‚‰äºˆæ¸¬æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆFå€¤ã€DiCEææ¡ˆã‚’å«ã‚€ï¼‰
        hourly_log = sheets_connector.get_hourly_log(user_id, date)

        # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ä½œæˆï¼ˆæ´»å‹•ãƒ‡ãƒ¼ã‚¿ + Hourly Logãƒãƒ¼ã‚¸ï¼‰
        timeline = []
        for idx, row in daily_data.iterrows():
            timestamp = row['Timestamp']
            time_str = timestamp.strftime('%H:%M') if hasattr(timestamp, 'strftime') else str(timestamp)
            activity_name = row.get('CatSub', '')

            # æ´»å‹•åã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
            if not activity_name or pd.isna(activity_name):
                logger.warning(f"æ´»å‹•åãŒä¸æ­£ã§ã™ (CatSub='{activity_name}') @{time_str} - ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                continue

            # Hourly Logã‹ã‚‰äºˆæ¸¬å€¤ãƒ»DiCEææ¡ˆã‚’å–å¾—
            # ã€é‡è¦ã€‘Hourly Logã«ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹æ´»å‹•ã®ã¿ã‚’è¡¨ç¤º
            if hourly_log.empty:
                continue  # Hourly LogãŒç©ºãªã‚‰å…¨ã¦ã‚¹ã‚­ãƒƒãƒ—

            cached = hourly_log[
                (hourly_log['æ™‚åˆ»'] == time_str) &
                (hourly_log['æ´»å‹•å'] == activity_name)
            ]

            if cached.empty:
                # Hourly Logã«æœªç™»éŒ²ã®æ´»å‹•ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤10ã‚’è¡¨ç¤ºã—ãªã„ãŸã‚ï¼‰
                continue

            # Hourly Logã‹ã‚‰äºˆæ¸¬å€¤ã¨DiCEææ¡ˆã‚’å–å¾—
            cached_row = cached.iloc[0]
            predicted_frustration = cached_row.get('äºˆæ¸¬NASA_F')
            dice_suggestion = cached_row.get('DiCEææ¡ˆæ´»å‹•å')
            improvement = cached_row.get('æ”¹å–„å¹…')
            improved_frustration = cached_row.get('æ”¹å–„å¾ŒFå€¤')

            # NaNãƒã‚§ãƒƒã‚¯
            if pd.isna(predicted_frustration):
                predicted_frustration = None
            if pd.isna(dice_suggestion) or dice_suggestion == '':
                dice_suggestion = None
            if pd.isna(improvement):
                improvement = None
            if pd.isna(improved_frustration):
                improved_frustration = None

            # Få€¤å¤‰æ›
            frustration_for_timeline = float(predicted_frustration) if predicted_frustration is not None else None

            # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã«è¿½åŠ ï¼ˆHourly Logã«ç™»éŒ²æ¸ˆã¿ã®æ´»å‹•ã®ã¿ï¼‰
            duration_minutes = int(row.get('Duration', 60))
            timeline_entry = {
                'timestamp': timestamp.isoformat(),
                'hour': timestamp.hour if hasattr(timestamp, 'hour') else 0,
                'activity': activity_name,
                'duration': duration_minutes,
                'frustration_value': frustration_for_timeline,
                'is_predicted': predicted_frustration is not None
            }

            # DiCEææ¡ˆãŒã‚ã‚‹å ´åˆã¯è¿½åŠ 
            if dice_suggestion:
                # time_rangeã‚’è¨ˆç®—ï¼ˆä¾‹: "13:00-16:00"ï¼‰
                from datetime import timedelta
                end_time = timestamp + timedelta(minutes=duration_minutes)
                time_range = f"{time_str}-{end_time.strftime('%H:%M')}"

                timeline_entry['dice_suggestion'] = {
                    'suggested_activity': dice_suggestion,
                    'improvement': float(improvement) if improvement is not None else None,
                    'improved_frustration': float(improved_frustration) if improved_frustration is not None else None,
                    'time_range': time_range  # è¿½åŠ 
                }

            timeline.append(timeline_entry)
        
        # æ™‚é–“é †ã«ã‚½ãƒ¼ãƒˆ
        timeline.sort(key=lambda x: x['timestamp'])
        
        return jsonify({
            'status': 'success',
            'user_id': user_id,
            'date': date,
            'timeline': timeline,
            'total_entries': len(timeline)
        })
        
    except Exception as e:
        logger.error(f"ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/feedback/generate', methods=['POST'])
def generate_feedback():
    """
    Daily Summaryã‹ã‚‰ChatGPTãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’èª­ã¿å–ã‚‹APIï¼ˆç”Ÿæˆã—ãªã„ï¼‰

    ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ç”Ÿæˆã¯1æ—¥1å›ã€scheduler.py ã® _execute_evening_feedback ã§å®Ÿè¡Œã•ã‚Œã‚‹ï¼ˆ22:50 JSTï¼‰
    ã“ã®APIã¯Daily Summaryã‚·ãƒ¼ãƒˆã‹ã‚‰æ—¢ã«ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’èª­ã¿å–ã‚‹ã ã‘
    """
    try:
        data = request.get_json()
        user_id = data.get('user_id', 'default')
        feedback_type = data.get('feedback_type', data.get('type', 'daily'))

        # ä»Šæ—¥ã®æ—¥ä»˜ã‚’å–å¾—
        today = datetime.now().strftime('%Y-%m-%d')

        logger.info(f"ğŸ“– Daily Summaryã‹ã‚‰ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å–å¾—: user_id={user_id}, date={today}")

        # Daily Summaryã‚·ãƒ¼ãƒˆã‹ã‚‰ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’å–å¾—
        summary = sheets_connector.get_daily_summary(user_id, today)

        if not summary:
            return jsonify({
                'status': 'success',
                'user_id': user_id,
                'feedback_type': 'daily',
                'feedback': {
                    'main_feedback': 'ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã¯ã¾ã ç”Ÿæˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚22:50ä»¥é™ã«ç¢ºèªã—ã¦ãã ã•ã„ã€‚',
                    'action_plan': [],
                    'total_improvement_potential': 0,
                    'num_suggestions': 0,
                    'generated_at': None
                },
                'message': 'ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã¯æ¯æ—¥22:50ï¼ˆJSTï¼‰ã«ç”Ÿæˆã•ã‚Œã¾ã™ã€‚'
            })

        # Daily Summaryã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å½¢å¼ã«å¤‰æ›
        feedback_result = {
            'main_feedback': summary.get('chatgpt_feedback', ''),
            'action_plan': summary.get('action_plan', []),
            'total_improvement_potential': summary.get('dice_improvement', 0),
            'num_suggestions': summary.get('dice_count', 0),
            'generated_at': summary.get('generated_at', '')
        }

        logger.info(f"âœ… Daily Summaryã‹ã‚‰ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å–å¾—å®Œäº†: user_id={user_id}")

        return jsonify({
            'status': 'success',
            'user_id': user_id,
            'feedback_type': 'daily',
            'feedback': feedback_result,
            'daily_stats': {
                'avg_predicted': round(summary.get('avg_predicted', 0), 2) if summary.get('avg_predicted') is not None else None,
                'dice_suggestions': summary.get('dice_count', 0)
            },
            'timestamp': datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500


@app.route('/api/scheduler/status', methods=['GET'])
def scheduler_status():
    """å®šæœŸãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã®çŠ¶æ…‹å–å¾—"""
    try:
        status = scheduler.get_status()
        return jsonify({
            'status': 'success',
            'scheduler': status
        })
    except Exception as e:
        logger.error(f"ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼çŠ¶æ…‹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/scheduler/config', methods=['POST'])
def update_scheduler_config():
    """å®šæœŸãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã®è¨­å®šæ›´æ–°"""
    try:
        data = request.get_json()
        morning_time = data.get('morning_time')
        evening_time = data.get('evening_time')
        enabled = data.get('enabled')
        
        scheduler.update_schedule_config(morning_time, evening_time, enabled)
        
        return jsonify({
            'status': 'success',
            'message': 'ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®šã‚’æ›´æ–°ã—ã¾ã—ãŸ'
        })
    except Exception as e:
        logger.error(f"ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®šæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/scheduler/trigger', methods=['POST'])
def trigger_manual_feedback():
    """æ‰‹å‹•ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å®Ÿè¡Œ"""
    try:
        data = request.get_json()
        user_id = data.get('user_id', 'default')
        feedback_type = data.get('type', 'evening')
        
        feedback = scheduler.trigger_manual_feedback(user_id, feedback_type)
        
        return jsonify({
            'status': 'success',
            'feedback': feedback,
            'message': f'{feedback_type}ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’æ‰‹å‹•å®Ÿè¡Œã—ã¾ã—ãŸ'
        })
    except Exception as e:
        logger.error(f"æ‰‹å‹•ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

# ==========================================
# Cloud Schedulerç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆmin_instances=0ç”¨ï¼‰
# ==========================================

@app.route('/api/scheduler/monitor', methods=['POST'])
def trigger_cloud_scheduler_monitor():
    """
    Cloud Schedulerã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ç›£è¦–ã‚’èµ·å‹•ï¼ˆ1å›å®Ÿè¡Œï¼‰
    æ—¢å­˜ã®data_monitor_loop()ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€ã“ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã¯ä½¿ç”¨ã—ãªã„
    """
    try:
        # ç°¡æ˜“èªè¨¼: ãƒ˜ãƒƒãƒ€ãƒ¼ãƒã‚§ãƒƒã‚¯ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        # Cloud Schedulerã§ã¯ OIDC ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨
        auth_header = request.headers.get('X-Scheduler-Auth', '')
        expected_auth = os.environ.get('SCHEDULER_AUTH_TOKEN', 'default-scheduler-token')

        if auth_header != expected_auth:
            logger.warning(f"âš ï¸ Cloud Schedulerèªè¨¼å¤±æ•—: ãƒ˜ãƒƒãƒ€ãƒ¼={auth_header[:10]}...")
            return jsonify({
                'status': 'error',
                'message': 'èªè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸ'
            }), 401

        logger.warning(f"ğŸ” Cloud Schedulerã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ç›£è¦–ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å—ä¿¡: {datetime.now(JST).strftime('%Y-%m-%d %H:%M')}")

        # ãƒ‡ãƒ¼ã‚¿ç›£è¦–ã‚’1å›å®Ÿè¡Œ
        result = run_data_monitor_once()

        return jsonify({
            'status': 'success',
            'message': 'ãƒ‡ãƒ¼ã‚¿ç›£è¦–ã‚’å®Ÿè¡Œã—ã¾ã—ãŸ',
            'result': result,
            'timestamp': datetime.now(JST).isoformat()
        })

    except Exception as e:
        logger.error(f"Cloud Schedulerãƒ‡ãƒ¼ã‚¿ç›£è¦–ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/scheduler/dice', methods=['POST'])
def trigger_cloud_scheduler_dice():
    """
    Cloud Schedulerã‹ã‚‰DiCEå®Ÿè¡Œã‚’èµ·å‹•ï¼ˆ1å›å®Ÿè¡Œï¼‰
    æ—¢å­˜ã®scheduler._execute_evening_feedback()ã¨åŒç­‰ã®å‡¦ç†
    """
    try:
        # ç°¡æ˜“èªè¨¼: ãƒ˜ãƒƒãƒ€ãƒ¼ãƒã‚§ãƒƒã‚¯
        auth_header = request.headers.get('X-Scheduler-Auth', '')
        expected_auth = os.environ.get('SCHEDULER_AUTH_TOKEN', 'default-scheduler-token')

        if auth_header != expected_auth:
            logger.warning(f"âš ï¸ Cloud Schedulerèªè¨¼å¤±æ•—: ãƒ˜ãƒƒãƒ€ãƒ¼={auth_header[:10]}...")
            return jsonify({
                'status': 'error',
                'message': 'èªè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸ'
            }), 401

        logger.warning(f"ğŸ² Cloud Schedulerã‹ã‚‰DiCEå®Ÿè¡Œãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å—ä¿¡: {datetime.now(JST).strftime('%Y-%m-%d %H:%M')}")

        # scheduler._execute_evening_feedback()ã‚’ç›´æ¥å‘¼ã³å‡ºã—
        scheduler._execute_evening_feedback()

        return jsonify({
            'status': 'success',
            'message': 'DiCEå®Ÿè¡Œã‚’å®Œäº†ã—ã¾ã—ãŸ',
            'timestamp': datetime.now(JST).isoformat()
        })

    except Exception as e:
        logger.error(f"Cloud Scheduler DiCEå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

# ==========================================
# æ—¢å­˜ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# ==========================================

@app.route('/api/feedback/history', methods=['GET'])
def get_feedback_history():
    """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å±¥æ­´å–å¾—"""
    try:
        user_id = request.args.get('user_id')
        days = int(request.args.get('days', 7))
        
        history = scheduler.get_recent_feedback(user_id, days)
        
        return jsonify({
            'status': 'success',
            'history': history,
            'count': len(history)
        })
    except Exception as e:
        logger.error(f"ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å±¥æ­´å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/health', methods=['GET'])
def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    try:
        user_id = request.args.get('user_id', 'default')

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã®predictorã‚’å–å¾—
        predictor = get_predictor(user_id)

        # å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®çŠ¶æ…‹ç¢ºèª
        sheets_status = sheets_connector.test_connection()
        scheduler_status = scheduler.get_status()

        return jsonify({
            'status': 'success',
            'timestamp': datetime.now().isoformat(),
            'components': {
                'sheets_connector': sheets_status,
                'scheduler': scheduler_status,
                'predictor_ready': predictor.model is not None,
                'explainer_ready': explainer is not None,
                'feedback_generator_ready': feedback_generator is not None
            }
        })
    except Exception as e:
        logger.error(f"ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/debug/model', methods=['GET'])
def debug_model():
    """ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒãƒƒã‚°æƒ…å ±å–å¾—API"""
    try:
        user_id = request.args.get('user_id', 'default')

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã®predictorã‚’å–å¾—
        predictor = get_predictor(user_id)

        # ãƒ‡ãƒ¼ã‚¿å–å¾—
        activity_data = sheets_connector.get_activity_data(user_id)
        fitbit_data = sheets_connector.get_fitbit_data(user_id)

        # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
        activity_processed = predictor.preprocess_activity_data(activity_data)
        df_enhanced = predictor.aggregate_fitbit_by_activity(activity_processed, fitbit_data)

        # ãƒ‡ãƒ¼ã‚¿å“è³ª
        data_quality = predictor.check_data_quality(df_enhanced)

        # NASA_Fçµ±è¨ˆ
        nasa_f_stats = None
        if not df_enhanced.empty and 'NASA_F' in df_enhanced.columns:
            nasa_f = df_enhanced['NASA_F'].dropna()
            if not nasa_f.empty:
                nasa_f_stats = {
                    'count': int(len(nasa_f)),
                    'mean': float(nasa_f.mean()),
                    'std': float(nasa_f.std()),
                    'min': float(nasa_f.min()),
                    'max': float(nasa_f.max()),
                    'unique_values': int(nasa_f.nunique()),
                    'value_distribution': nasa_f.value_counts().head(10).to_dict()
                }

        # æ´»å‹•çµ±è¨ˆ
        activity_stats = None
        if not df_enhanced.empty and 'CatSub' in df_enhanced.columns:
            activities = df_enhanced['CatSub'].dropna()
            if not activities.empty:
                activity_stats = {
                    'total': int(len(activities)),
                    'unique': int(activities.nunique()),
                    'top_5': activities.value_counts().head(5).to_dict()
                }

        # ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹
        model_info = {
            'is_trained': predictor.model is not None,
            'feature_count': len(predictor.feature_columns) if predictor.feature_columns else 0,
            'feature_names': predictor.feature_columns if predictor.feature_columns else []
        }

        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼æƒ…å ±
        encoders_info = {}
        if hasattr(predictor, 'encoders'):
            for key, encoder in predictor.encoders.items():
                if hasattr(encoder, 'classes_'):
                    encoders_info[key] = {
                        'n_classes': len(encoder.classes_),
                        'classes': list(encoder.classes_)
                    }

        # Walk Forward Validationçµæœ
        wfv_results = None
        if predictor.model is not None and len(df_enhanced) >= 10:
            try:
                training_results = predictor.walk_forward_validation_train(df_enhanced)
                wfv_results = {
                    'rmse': float(training_results.get('walk_forward_rmse', 0)),
                    'mae': float(training_results.get('walk_forward_mae', 0)),
                    'r2': float(training_results.get('walk_forward_r2', 0)),
                    'prediction_diversity': training_results.get('prediction_diversity', {}),
                    'feature_importance': {
                        k: float(v) for k, v in sorted(
                            training_results.get('feature_importance', {}).items(),
                            key=lambda x: x[1],
                            reverse=True
                        )[:15]  # ä¸Šä½15ç‰¹å¾´é‡
                    }
                }
            except Exception as e:
                logger.error(f"WFVå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
                wfv_results = {'error': str(e)}

        # ãƒ†ã‚¹ãƒˆäºˆæ¸¬
        test_predictions = []
        if not df_enhanced.empty and predictor.model is not None:
            test_cases = df_enhanced.head(3)
            for idx, row in test_cases.iterrows():
                activity = row.get('CatSub')
                if activity is None:
                    activity = 'unknown'

                duration = row.get('Duration', 60)
                timestamp = pd.to_datetime(row.get('Timestamp'))
                actual = row.get('NASA_F')

                # å›ºå®šå€¤äºˆæ¸¬
                pred1 = predictor.predict_single_activity(activity, duration, timestamp)

                # å±¥æ­´ä½¿ç”¨äºˆæ¸¬
                pred2 = predictor.predict_with_history(activity, duration, timestamp, df_enhanced)

                test_predictions.append({
                    'activity': activity,
                    'timestamp': timestamp.isoformat(),
                    'actual': float(actual) if pd.notna(actual) else None,
                    'predicted_fixed': float(pred1.get('predicted_frustration', 0)),
                    'predicted_history': float(pred2.get('predicted_frustration', 0)),
                    'historical_records': int(pred2.get('historical_records', 0))
                })

        # è¨ºæ–­
        issues = []
        if data_quality['total_samples'] < 10:
            issues.append(f"ãƒ‡ãƒ¼ã‚¿æ•°ä¸è¶³: {data_quality['total_samples']}ä»¶")
        if nasa_f_stats and nasa_f_stats['std'] < 1.0:
            issues.append(f"NASA_Fã®åˆ†æ•£ãŒå°ã•ã„: {nasa_f_stats['std']:.2f}")
        if nasa_f_stats and nasa_f_stats['unique_values'] < 3:
            issues.append(f"NASA_Fã®ç¨®é¡ãŒå°‘ãªã„: {nasa_f_stats['unique_values']}ç¨®é¡")
        if not predictor.model:
            issues.append("ãƒ¢ãƒ‡ãƒ«æœªè¨“ç·´")
        if activity_stats and activity_stats['unique'] < 3:
            issues.append(f"æ´»å‹•ç¨®é¡ãŒå°‘ãªã„: {activity_stats['unique']}ç¨®é¡")

        return jsonify({
            'status': 'success',
            'user_id': user_id,
            'timestamp': datetime.now().isoformat(),
            'data_quality': data_quality,
            'nasa_f_stats': nasa_f_stats,
            'activity_stats': activity_stats,
            'model_info': model_info,
            'encoders_info': encoders_info,
            'wfv_results': wfv_results,
            'test_predictions': test_predictions,
            'issues': issues,
            'diagnosis': 'OK' if not issues else 'Issues detected'
        })

    except Exception as e:
        logger.error(f"ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒãƒƒã‚°ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/data/stats', methods=['GET'])
def get_data_stats():
    """ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆæƒ…å ±å–å¾—API"""
    try:
        user_id = request.args.get('user_id', 'default')

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã®predictorã‚’å–å¾—
        predictor = get_predictor(user_id)

        # ãƒ‡ãƒ¼ã‚¿å–å¾—
        activity_data = sheets_connector.get_activity_data(user_id)
        fitbit_data = sheets_connector.get_fitbit_data(user_id)

        # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
        activity_processed = predictor.preprocess_activity_data(activity_data)
        df_enhanced = predictor.aggregate_fitbit_by_activity(activity_processed, fitbit_data)

        # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
        data_quality = predictor.check_data_quality(df_enhanced)

        # NASA_Fçµ±è¨ˆ
        nasa_f_stats = {}
        if not activity_data.empty and 'NASA_F' in activity_data.columns:
            nasa_f_values = pd.to_numeric(activity_data['NASA_F'], errors='coerce').dropna()
            if not nasa_f_values.empty:
                nasa_f_stats = {
                    'count': int(len(nasa_f_values)),
                    'mean': float(nasa_f_values.mean()),
                    'std': float(nasa_f_values.std()),
                    'min': float(nasa_f_values.min()),
                    'max': float(nasa_f_values.max()),
                    'median': float(nasa_f_values.median()),
                    'unique_values': int(nasa_f_values.nunique())
                }

        # æ´»å‹•ã‚«ãƒ†ã‚´ãƒªçµ±è¨ˆ
        activity_stats = {}
        if not activity_data.empty and 'CatSub' in activity_data.columns:
            activity_counts = activity_data['CatSub'].value_counts()
            activity_stats = {
                'total_activities': int(len(activity_data)),
                'unique_activities': int(activity_data['CatSub'].nunique()),
                'top_activities': activity_counts.head(10).to_dict()
            }

        # Fitbitãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ
        fitbit_stats = {
            'total_records': int(len(fitbit_data)) if not fitbit_data.empty else 0,
            'has_data': not fitbit_data.empty
        }

        # ãƒ¢ãƒ‡ãƒ«çµ±è¨ˆ
        model_stats = {
            'is_trained': predictor.model is not None,
            'feature_count': len(predictor.feature_columns) if hasattr(predictor, 'feature_columns') and predictor.feature_columns else 0,
            'training_history_count': len(predictor.walk_forward_history) if hasattr(predictor, 'walk_forward_history') and predictor.walk_forward_history else 0
        }

        # æ—¥ä»˜ç¯„å›²
        date_range = {}
        if not activity_data.empty and 'Timestamp' in activity_data.columns:
            timestamps = pd.to_datetime(activity_data['Timestamp'], errors='coerce').dropna()
            if not timestamps.empty:
                date_range = {
                    'earliest': timestamps.min().isoformat(),
                    'latest': timestamps.max().isoformat(),
                    'days_span': (timestamps.max() - timestamps.min()).days
                }

        return jsonify({
            'status': 'success',
            'user_id': user_id,
            'timestamp': datetime.now().isoformat(),
            'data_quality': data_quality,
            'nasa_f_stats': nasa_f_stats,
            'activity_stats': activity_stats,
            'fitbit_stats': fitbit_stats,
            'model_stats': model_stats,
            'date_range': date_range,
            'summary': {
                'total_activity_records': int(len(activity_data)) if not activity_data.empty else 0,
                'processed_records': int(len(df_enhanced)) if not df_enhanced.empty else 0,
                'data_sufficient': data_quality.get('is_sufficient', False),
                'quality_level': data_quality.get('quality_level', 'unknown')
            }
        })

    except Exception as e:
        logger.error(f"ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/users', methods=['GET'])
def get_users():
    """åˆ©ç”¨å¯èƒ½ãƒ¦ãƒ¼ã‚¶ãƒ¼ä¸€è¦§å–å¾—"""
    try:
        # Config.pyã‹ã‚‰è¨­å®šæ¸ˆã¿ãƒ¦ãƒ¼ã‚¶ãƒ¼ä¸€è¦§ã‚’å–å¾—
        config = Config()
        users = config.get_all_users()
        
        return jsonify({
            'status': 'success',
            'users': users
        })
    except Exception as e:
        logger.error(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/frustration/trends', methods=['POST'])
def get_frustration_trends():
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ¥ãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å€¤æ¨ç§»ãƒ‡ãƒ¼ã‚¿å–å¾—"""
    try:
        data = request.get_json()
        user_id = data.get('user_id', 'default')
        days = data.get('days', 30)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ30æ—¥é–“
        
        # ãƒ‡ãƒ¼ã‚¿å–å¾—
        activity_data = sheets_connector.get_activity_data(user_id)
        
        if activity_data.empty:
            return jsonify({
                'status': 'error',
                'message': 'æ´»å‹•ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'
            }), 400
        
        # æ—¥åˆ¥å¹³å‡ãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å€¤ã‚’è¨ˆç®—
        activity_data['Date'] = pd.to_datetime(activity_data['Timestamp']).dt.date
        
        # éå»æŒ‡å®šæ—¥æ•°ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        end_date = datetime.now().date()
        start_date = end_date - timedelta(days=days)
        activity_data = activity_data[activity_data['Date'] >= start_date]
        
        # æ—¥åˆ¥çµ±è¨ˆã®è¨ˆç®—
        daily_stats = []
        date_range = [start_date + timedelta(days=x) for x in range(days)]
        
        for date in date_range:
            day_data = activity_data[activity_data['Date'] == date]
            
            if not day_data.empty:
                avg_frustration = day_data['NASA_F'].mean()
                min_frustration = day_data['NASA_F'].min()
                max_frustration = day_data['NASA_F'].max()
                activity_count = len(day_data)
                total_duration = day_data['Duration'].sum()
                
                # æ´»å‹•åˆ¥é›†è¨ˆ
                activity_summary = day_data.groupby('CatSub').agg({
                    'NASA_F': ['mean', 'count'],
                    'Duration': 'sum'
                }).round(1)
                
                activity_breakdown = []
                for activity in activity_summary.index:
                    activity_breakdown.append({
                        'activity': activity,
                        'avg_frustration': activity_summary.loc[activity, ('NASA_F', 'mean')],
                        'count': int(activity_summary.loc[activity, ('NASA_F', 'count')]),
                        'total_duration': int(activity_summary.loc[activity, ('Duration', 'sum')])
                    })
            else:
                avg_frustration = None
                min_frustration = None
                max_frustration = None
                activity_count = 0
                total_duration = 0
                activity_breakdown = []
            
            daily_stats.append({
                'date': date.isoformat(),
                'avg_frustration': avg_frustration,
                'min_frustration': min_frustration,
                'max_frustration': max_frustration,
                'activity_count': activity_count,
                'total_duration': total_duration,
                'activity_breakdown': activity_breakdown
            })
        
        # æœŸé–“å…¨ä½“ã®çµ±è¨ˆ
        if not activity_data.empty:
            period_stats = {
                'avg_frustration': activity_data['NASA_F'].mean(),
                'min_frustration': activity_data['NASA_F'].min(),
                'max_frustration': activity_data['NASA_F'].max(),
                'total_activities': len(activity_data),
                'total_duration': activity_data['Duration'].sum(),
                'improvement_trend': calculate_trend(activity_data)
            }
        else:
            period_stats = {
                'avg_frustration': 0,
                'min_frustration': 0,
                'max_frustration': 0,
                'total_activities': 0,
                'total_duration': 0,
                'improvement_trend': 0
            }
        
        return jsonify({
            'status': 'success',
            'user_id': user_id,
            'period': f'{days}æ—¥é–“',
            'daily_trends': daily_stats,
            'period_summary': period_stats,
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"æ¨ç§»ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

def calculate_trend(activity_data):
    """ãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å€¤ã®æ”¹å–„ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’è¨ˆç®—"""
    try:
        if len(activity_data) < 2:
            return 0

        # æ—¥ä»˜é †ã«ã‚½ãƒ¼ãƒˆ
        activity_data = activity_data.sort_values('Timestamp')

        # å‰åŠã¨å¾ŒåŠã«åˆ†ã‘ã¦å¹³å‡å€¤ã‚’æ¯”è¼ƒ
        mid_point = len(activity_data) // 2
        first_half_avg = activity_data.iloc[:mid_point]['NASA_F'].mean()
        second_half_avg = activity_data.iloc[mid_point:]['NASA_F'].mean()

        # æ”¹å–„åº¦ã‚’è¨ˆç®—ï¼ˆè² ã®å€¤ãŒæ”¹å–„ã‚’æ„å‘³ã™ã‚‹ï¼‰
        trend = second_half_avg - first_half_avg
        return round(trend, 2)

    except Exception:
        return 0

def calculate_and_save_daily_summary(user_id: str, target_date=None):
    """
    æŒ‡å®šãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ—¥æ¬¡ã‚µãƒãƒªãƒ¼ã‚’è¨ˆç®—ã—ã¦ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ä¿å­˜

    Args:
        user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
        target_date: å¯¾è±¡æ—¥ä»˜ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯æ˜¨æ—¥ï¼‰

    Returns:
        ä¿å­˜æˆåŠŸ: True, å¤±æ•—: False
    """
    try:
        if target_date is None:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯æ˜¨æ—¥
            target_date = (datetime.now() - timedelta(days=1)).date()
        elif isinstance(target_date, str):
            target_date = datetime.fromisoformat(target_date).date()

        # ãƒ‡ãƒ¼ã‚¿å–å¾—
        activity_data = sheets_connector.get_activity_data(user_id, use_cache=False)

        if activity_data.empty:
            logger.warning(f"æ—¥æ¬¡ã‚µãƒãƒªãƒ¼è¨ˆç®—: ãƒ¦ãƒ¼ã‚¶ãƒ¼ {user_id} ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return False

        # å¯¾è±¡æ—¥ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        activity_data['Date'] = pd.to_datetime(activity_data['Timestamp']).dt.date
        daily_data = activity_data[activity_data['Date'] == target_date]

        if daily_data.empty:
            logger.info(f"æ—¥æ¬¡ã‚µãƒãƒªãƒ¼è¨ˆç®—: {target_date} ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ (ãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_id})")
            return False

        # ãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å€¤ã®çµ±è¨ˆã‚’è¨ˆç®—
        frustration_values = daily_data['NASA_F'].dropna()

        if frustration_values.empty:
            logger.warning(f"æ—¥æ¬¡ã‚µãƒãƒªãƒ¼è¨ˆç®—: NASA_Få€¤ãŒã‚ã‚Šã¾ã›ã‚“ (ãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_id}, æ—¥ä»˜: {target_date})")
            return False

        # ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        summary_data = {
            'avg_frustration': float(frustration_values.mean()),
            'min_frustration': float(frustration_values.min()),
            'max_frustration': float(frustration_values.max()),
            'activity_count': int(len(daily_data)),
            'total_duration': int(daily_data['Duration'].sum()),
            'unique_activities': int(daily_data['CatSub'].nunique()),
            'notes': f'Auto-generated from {len(daily_data)} activities'
        }

        # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ä¿å­˜
        success = sheets_connector.save_daily_summary(
            user_id=user_id,
            date=target_date.isoformat(),
            summary_data=summary_data
        )

        if success and config.ENABLE_INFO_LOGS:
            logger.info(f"æ—¥æ¬¡ã‚µãƒãƒªãƒ¼ä¿å­˜å®Œäº†: {user_id}, {target_date}, "
                       f"å¹³å‡: {summary_data['avg_frustration']:.2f}, "
                       f"æ´»å‹•æ•°: {summary_data['activity_count']}")

        return success

    except Exception as e:
        logger.error(f"æ—¥æ¬¡ã‚µãƒãƒªãƒ¼è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
        return False

# ===== DEBUG API ENDPOINTS =====

@app.route('/api/frustration/daily-summary', methods=['POST'])
def generate_daily_summary():
    """
    æ—¥æ¬¡ã‚µãƒãƒªãƒ¼è¨ˆç®—ãƒ»ä¿å­˜API
    æŒ‡å®šæ—¥ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯æ˜¨æ—¥ï¼‰ã®ãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµ±è¨ˆã‚’è¨ˆç®—ã—ã¦ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ä¿å­˜
    """
    try:
        data = request.get_json() or {}
        user_id = data.get('user_id', 'default')
        target_date = data.get('date')

        # æ—¥æ¬¡ã‚µãƒãƒªãƒ¼ã‚’è¨ˆç®—ãƒ»ä¿å­˜
        success = calculate_and_save_daily_summary(user_id, target_date)

        if success:
            return jsonify({
                'status': 'success',
                'message': 'æ—¥æ¬¡ã‚µãƒãƒªãƒ¼ã‚’ä¿å­˜ã—ã¾ã—ãŸ',
                'user_id': user_id,
                'date': target_date if target_date else (datetime.now() - timedelta(days=1)).date().isoformat(),
                'timestamp': datetime.now().isoformat()
            })
        else:
            return jsonify({
                'status': 'error',
                'message': 'æ—¥æ¬¡ã‚µãƒãƒªãƒ¼ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚',
                'user_id': user_id
            }), 400

    except Exception as e:
        logger.error(f"æ—¥æ¬¡ã‚µãƒãƒªãƒ¼ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/debug/data-monitor/status', methods=['GET'])
def debug_data_monitor_status():
    """ãƒ‡ãƒ¼ã‚¿ç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰ã®çŠ¶æ…‹ç¢ºèªAPI"""
    try:
        status = {
            'monitor_running': data_monitor_running,
            'monitor_thread_alive': data_monitor_thread.is_alive() if data_monitor_thread else False,
            'last_prediction': last_prediction_result if last_prediction_result else None,
            'current_time': datetime.now().isoformat()
        }

        return jsonify({
            'status': 'success',
            'data': status
        })
    except Exception as e:
        logger.error(f"ãƒ‡ãƒ¼ã‚¿ç›£è¦–çŠ¶æ…‹ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/debug/data-monitor/check', methods=['POST'])
def debug_trigger_data_check():
    """æ‰‹å‹•ã§ãƒ‡ãƒ¼ã‚¿æ›´æ–°ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ãƒ‡ãƒãƒƒã‚°API"""
    try:
        data = request.get_json() or {}
        user_id = data.get('user_id', 'default')

        has_new = sheets_connector.has_new_data(user_id)

        return jsonify({
            'status': 'success',
            'user_id': user_id,
            'has_new_data': has_new,
            'timestamp': datetime.now().isoformat()
        })
    except Exception as e:
        logger.error(f"ãƒ‡ãƒ¼ã‚¿æ›´æ–°ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/tablet/data/<user_id>', methods=['GET'])
def get_tablet_data(user_id):
    """ã‚¿ãƒ–ãƒ¬ãƒƒãƒˆç”¨çµ±åˆãƒ‡ãƒ¼ã‚¿API - ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€åº¦ã«å–å¾—"""
    try:
        if config.ENABLE_DEBUG_LOGS:
            logger.debug(f"ã‚¿ãƒ–ãƒ¬ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿APIå‘¼ã³å‡ºã— - user_id: {user_id}")
        
        # ä»Šæ—¥ã®æ—¥ä»˜ã‚’å–å¾—
        today = datetime.now().strftime('%Y-%m-%d')
        
        # 1. ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        timeline_data = []
        try:
            with app.test_request_context(json={'user_id': user_id, 'date': today}):
                timeline_response = get_frustration_timeline()
                if hasattr(timeline_response, 'get_json'):
                    timeline_result = timeline_response.get_json()
                    if timeline_result and timeline_result.get('status') == 'success':
                        timeline_data = timeline_result.get('timeline', [])
        except Exception as timeline_error:
            logger.warning(f"ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿å–å¾—è­¦å‘Š: {timeline_error}")
        
        # 2. DiCEåˆ†æãƒ‡ãƒ¼ã‚¿ã¨ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’Daily Summaryã‹ã‚‰å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
        # æ¯å›DiCEå®Ÿè¡Œã™ã‚‹ã®ã§ã¯ãªãã€æ—¢ã«ä¿å­˜ã•ã‚ŒãŸDaily Summaryã‚’ä½¿ç”¨
        dice_data = {}
        feedback_data = {}

        try:
            # Daily Summaryã‚·ãƒ¼ãƒˆã‹ã‚‰ä»Šæ—¥ã®ã‚µãƒãƒªãƒ¼ã‚’å–å¾—
            sheet_name = f"{user_id}_Daily_Summary"
            worksheet = sheets_connector._find_worksheet_by_exact_name(sheet_name)

            if worksheet:
                all_values = worksheet.get_all_values()
                for row in all_values[1:]:  # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ã‚¹ã‚­ãƒƒãƒ—
                    if len(row) > 0 and row[0] == today:
                        # ä»Šæ—¥ã®ã‚µãƒãƒªãƒ¼ãŒè¦‹ã¤ã‹ã£ãŸ
                        logger.info(f"Daily Summaryã‹ã‚‰å–å¾—: {today}")
                        feedback_data = {
                            'feedback': row[6] if len(row) > 6 else '',  # ChatGPTãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
                            'action_plan': json.loads(row[7]) if len(row) > 7 and row[7] else []  # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³
                        }
                        dice_data = {
                            'improvement_potential': float(row[4]) if len(row) > 4 and row[4] else 0,  # DiCEæ”¹å–„ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«
                            'suggestion_count': int(row[5]) if len(row) > 5 and row[5] else 0  # DiCEææ¡ˆæ•°
                        }
                        logger.info(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—: DiCE={dice_data.get('suggestion_count')}ä»¶, Feedback={'ã‚ã‚Š' if feedback_data.get('feedback') else 'ãªã—'}")
                        break

            # Daily Summaryã«ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã®ã¿ã€DiCEåˆ†æã¨ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ç”Ÿæˆã‚’å®Ÿè¡Œ
            if not dice_data and not feedback_data:
                logger.info(f"Daily SummaryãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€DiCEåˆ†æã¨ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™: {today}")
                # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ãŒ1æ—¥1å›å®Ÿè¡Œã™ã‚‹ãŸã‚ã€ã“ã“ã§ã¯ç©ºã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™

        except Exception as cache_error:
            logger.warning(f"Daily Summaryã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã‚¨ãƒ©ãƒ¼: {cache_error}")
        
        # 4. ä»Šæ—¥ã®çµ±è¨ˆã‚’è¨ˆç®—
        daily_stats = {
            'date': today,
            'total_activities': len(timeline_data),
            'avg_frustration': 0,
            'min_frustration': 0,
            'max_frustration': 0
        }
        
        if timeline_data:
            frustration_values = [item.get('frustration_value', 0) for item in timeline_data if item.get('frustration_value') is not None]
            if frustration_values:
                daily_stats['avg_frustration'] = round(sum(frustration_values) / len(frustration_values), 1)
                daily_stats['min_frustration'] = min(frustration_values)
                daily_stats['max_frustration'] = max(frustration_values)
        
        # 5. çµ±åˆãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä½œæˆ
        response_data = {
            'status': 'success',
            'user_id': user_id,
            'timestamp': datetime.now().isoformat(),
            'daily_stats': daily_stats,
            'timeline': timeline_data,
            'dice_analysis': dice_data,
            'feedback': feedback_data,
            'system_info': {
                'data_source': 'spreadsheet',
                'last_update': datetime.now().isoformat(),
                'prediction_logging': True,
                'data_monitoring': True
            }
        }
        
        return jsonify(response_data)
        
    except Exception as e:
        logger.error(f"ã‚¿ãƒ–ãƒ¬ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿API ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'status': 'error',
            'user_id': user_id,
            'message': str(e),
            'timestamp': datetime.now().isoformat()
        }), 500

# ===== HELPER FUNCTIONS =====

def get_user_config(user_id: str) -> Dict:
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®šã‚’å–å¾—
    Config.pyã‹ã‚‰è¨­å®šã‚’å–å¾—ã™ã‚‹ã‚ˆã†ã«å¤‰æ›´
    """
    config = Config()
    users = config.get_all_users()

    for user in users:
        if user['user_id'] == user_id:
            return user

    # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯æœ€åˆã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’è¿”ã™
    if users:
        return users[0]

    return {'user_id': user_id, 'name': user_id}

def data_monitor_loop():
    """
    å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç›£è¦–ã—ã€nasa_status='done'ã®æ´»å‹•ã‚’è‡ªå‹•çš„ã«äºˆæ¸¬
    9:00-22:00ã®é–“ã€æ¯æ™‚0åˆ†ã¨30åˆ†ã«å®Ÿè¡Œ
    """
    global data_monitor_running, last_prediction_result

    # å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒªã‚¹ãƒˆã‚’Configã‹ã‚‰å–å¾—
    users_config = config.get_all_users()

    def get_next_run_time():
        """æ¬¡ã®å®Ÿè¡Œæ™‚åˆ»ã‚’è¨ˆç®—ï¼ˆJST: 9:00-22:00ã®é–“ã€æ¯æ™‚0åˆ†ã¨30åˆ†ï¼‰"""
        from datetime import timedelta

        now = datetime.now(JST)
        current_hour = now.hour
        current_minute = now.minute

        # æ¬¡ã®å®Ÿè¡Œæ™‚åˆ»å€™è£œã‚’è¨ˆç®—
        if current_minute < 30:
            # ä»Šã®æ™‚é–“ã®30åˆ†
            next_time = now.replace(minute=30, second=0, microsecond=0)
        else:
            # æ¬¡ã®æ™‚é–“ã®00åˆ†
            next_time = now + timedelta(hours=1)
            next_time = next_time.replace(minute=0, second=0, microsecond=0)

        # 9:00-22:00ã®ç¯„å›²å¤–ã®å ´åˆã¯ç¿Œæ—¥9:00ã«è¨­å®š
        while next_time.hour < 9 or next_time.hour > 22:
            if next_time.hour > 22 or next_time.hour < 9:
                # ç¿Œæ—¥9:00
                next_day = next_time + timedelta(days=1)
                next_time = next_day.replace(hour=9, minute=0, second=0, microsecond=0)
            else:
                break

        return next_time

    logger.warning(f"ğŸ• ãƒ‡ãƒ¼ã‚¿ç›£è¦–ãƒ«ãƒ¼ãƒ—é–‹å§‹: 9:00-22:00ã®é–“ã€æ¯æ™‚0åˆ†ã¨30åˆ†ã«å®Ÿè¡Œ")

    while data_monitor_running:
        try:
            # æ¬¡ã®å®Ÿè¡Œæ™‚åˆ»ã¾ã§å¾…æ©Ÿ
            next_run = get_next_run_time()
            wait_seconds = (next_run - datetime.now(JST)).total_seconds()

            if wait_seconds > 0:
                logger.warning(f"â° æ¬¡ã®å®Ÿè¡Œæ™‚åˆ»: {next_run.strftime('%H:%M')}, å¾…æ©Ÿæ™‚é–“: {int(wait_seconds)}ç§’")
                time.sleep(wait_seconds)

            logger.warning(f"ğŸ” ãƒ‡ãƒ¼ã‚¿ç›£è¦–é–‹å§‹: {datetime.now(JST).strftime('%Y-%m-%d %H:%M')}")

            # å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯
            for user_config in users_config:
                user_id = user_config['user_id']
                user_name = user_config['name']

                # æ¯å›å…¨æ´»å‹•ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆhas_new_dataä¸è¦ï¼‰
                try:
                    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã®predictorã‚’å–å¾—
                    predictor = get_predictor(user_id)

                    # ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—ï¼‰
                    activity_data = sheets_connector.get_activity_data(user_id, use_cache=False)
                    fitbit_data = sheets_connector.get_fitbit_data(user_id, use_cache=False)

                    if activity_data.empty:
                        logger.warning(f"æ´»å‹•ãƒ‡ãƒ¼ã‚¿ãªã—: {user_name}")
                        continue

                    # nasa_status='done'ã®è¡Œã®ã¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    if 'nasa_status' in activity_data.columns:
                        activity_data_done = activity_data[activity_data['nasa_status'] == 'done'].copy()
                        logger.warning(f"ğŸ“Š {user_name}: å…¨æ´»å‹•={len(activity_data)}ä»¶, nasa_status='done'={len(activity_data_done)}ä»¶")
                    else:
                        # nasa_statusåˆ—ãŒãªã„å ´åˆã¯å…¨ã¦ã‚’å‡¦ç†
                        activity_data_done = activity_data.copy()
                        logger.warning(f"âš ï¸ {user_name}: nasa_statusåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…¨æ´»å‹•ã‚’å‡¦ç†ã—ã¾ã™")

                    if activity_data_done.empty:
                        logger.warning(f"nasa_status='done'ã®æ´»å‹•ãªã—: {user_name}")
                        continue

                    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´ç¢ºèª
                    training_result = ensure_model_trained(user_id, force_retrain=False)
                    if training_result['status'] not in ['success', 'already_trained']:
                        logger.warning(f"ãƒ¢ãƒ‡ãƒ«è¨“ç·´å¤±æ•— ({user_name}): {training_result.get('message')}")
                        continue

                    if predictor.model is None:
                        logger.error(f"ãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ ({user_name})")
                        continue

                    # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
                    activity_processed = predictor.preprocess_activity_data(activity_data_done)
                    df_enhanced = predictor.aggregate_fitbit_by_activity(activity_processed, fitbit_data)

                    logger.warning(f"ğŸ” äºˆæ¸¬ãƒã‚§ãƒƒã‚¯é–‹å§‹: {user_name}, å¯¾è±¡æ´»å‹•={len(df_enhanced)}ä»¶")

                    # ã€é‡è¦ã€‘å…¨æœŸé–“ã®Hourly Logã‚’ä¸€åº¦ã«å–å¾—ï¼ˆåŠ¹ç‡åŒ–ã¨é‡è¤‡é˜²æ­¢ï¼‰
                    all_dates = df_enhanced['Timestamp'].dt.strftime('%Y-%m-%d').unique()
                    hourly_log_cache = {}
                    for date in all_dates:
                        hourly_log_cache[date] = sheets_connector.get_hourly_log(user_id, date)

                    logger.warning(f"ğŸ“‹ Hourly Logå–å¾—å®Œäº†: {len(hourly_log_cache)}æ—¥åˆ†")

                    # æ–°è¦æ´»å‹•ã®ã¿ã‚’æŠ½å‡º
                    new_activities = []
                    update_predictions = []

                    for idx, row in df_enhanced.iterrows():
                        try:
                            timestamp = row['Timestamp']
                            date = timestamp.strftime('%Y-%m-%d')
                            time_str = timestamp.strftime('%H:%M')
                            activity = row.get('CatSub', '')

                            # æ´»å‹•åãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
                            if not activity or pd.isna(activity) or activity == 'unknown':
                                continue

                            # å®Ÿæ¸¬å€¤ã‚’å–å¾—
                            actual_frustration = row.get('NASA_F')

                            # ç”Ÿä½“æƒ…å ±ãŒæƒã£ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                            has_biodata = check_fitbit_data_availability(row)

                            # Hourly Logã«æ—¢ã«å­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ï¼‰
                            hourly_log = hourly_log_cache.get(date, pd.DataFrame())
                            is_existing = False
                            existing_predicted = None

                            if not hourly_log.empty:
                                existing = hourly_log[
                                    (hourly_log['æ™‚åˆ»'] == time_str) &
                                    (hourly_log['æ´»å‹•å'] == activity)
                                ]
                                if not existing.empty:
                                    is_existing = True
                                    existing_row = existing.iloc[0]
                                    existing_predicted = existing_row.get('äºˆæ¸¬NASA_F')

                            if is_existing:
                                # äºˆæ¸¬å€¤ãŒç©ºç™½ã§ã€ã‹ã¤ç”Ÿä½“ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã¯äºˆæ¸¬å€¤ã‚’æ›´æ–°å¯¾è±¡ã«è¿½åŠ 
                                if (pd.isna(existing_predicted) or existing_predicted == '') and has_biodata:
                                    update_predictions.append({
                                        'row': row,
                                        'date': date,
                                        'time': time_str,
                                        'activity': activity,
                                        'actual_frustration': actual_frustration
                                    })
                                # ãã‚Œä»¥å¤–ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ—¢ã«ç™»éŒ²æ¸ˆã¿ï¼‰
                                continue

                            # æ–°è¦æ´»å‹•ã¨ã—ã¦è¿½åŠ 
                            new_activities.append({
                                'row': row,
                                'date': date,
                                'time': time_str,
                                'activity': activity,
                                'actual_frustration': actual_frustration,
                                'has_biodata': has_biodata
                            })

                        except Exception as parse_error:
                            logger.error(f"æ´»å‹•ãƒ‡ãƒ¼ã‚¿è§£æã‚¨ãƒ©ãƒ¼: {parse_error}")
                            continue

                    logger.warning(f"ğŸ“Š æ–°è¦æ´»å‹•: {len(new_activities)}ä»¶, äºˆæ¸¬å€¤æ›´æ–°: {len(update_predictions)}ä»¶")

                    # äºˆæ¸¬å€¤æ›´æ–°å‡¦ç†
                    predictions_count = 0
                    for item in update_predictions:
                        try:
                            prediction_result = predictor.predict_from_row(item['row'])
                            if prediction_result and 'predicted_frustration' in prediction_result:
                                predicted_frustration = prediction_result.get('predicted_frustration')
                                if predicted_frustration is not None and not (np.isnan(predicted_frustration) or np.isinf(predicted_frustration)):
                                    predicted_frustration = float(predicted_frustration)
                                    # äºˆæ¸¬å€¤ã‚’æ›´æ–°
                                    sheets_connector.update_hourly_log_prediction(
                                        user_id, item['date'], item['time'], item['activity'], predicted_frustration
                                    )
                                    predictions_count += 1
                                    logger.warning(f"ğŸ”„ äºˆæ¸¬å€¤æ›´æ–°: {item['activity']} @{item['time']}, å®Ÿæ¸¬={item['actual_frustration']}, äºˆæ¸¬={predicted_frustration:.2f}")
                        except Exception as update_error:
                            logger.error(f"äºˆæ¸¬å€¤æ›´æ–°ã‚¨ãƒ©ãƒ¼: {update_error}")
                            continue

                    # æ–°è¦æ´»å‹•ä¿å­˜å‡¦ç†
                    for item in new_activities:
                        try:
                            predicted_frustration = None

                            if item['has_biodata']:
                                # äºˆæ¸¬å®Ÿè¡Œ
                                prediction_result = predictor.predict_from_row(item['row'])
                                if prediction_result and 'predicted_frustration' in prediction_result:
                                    predicted_frustration = prediction_result.get('predicted_frustration')
                                    if predicted_frustration is not None and not (np.isnan(predicted_frustration) or np.isinf(predicted_frustration)):
                                        predicted_frustration = float(predicted_frustration)
                                    else:
                                        predicted_frustration = None

                            # Hourly Logã«ä¿å­˜ï¼ˆäºˆæ¸¬å€¤ãªã—ã§ã‚‚ä¿å­˜ï¼‰
                            hourly_data = {
                                'date': item['date'],
                                'time': item['time'],
                                'activity': item['activity'],
                                'actual_frustration': item['actual_frustration'],
                                'predicted_frustration': predicted_frustration
                            }
                            sheets_connector.save_hourly_log(user_id, hourly_data)
                            predictions_count += 1

                            if predicted_frustration:
                                logger.warning(f"âœ… æ–°è¦ç™»éŒ²: {item['activity']} @{item['time']}, å®Ÿæ¸¬={item['actual_frustration']}, äºˆæ¸¬={predicted_frustration:.2f}")
                            else:
                                logger.warning(f"âœ… æ–°è¦ç™»éŒ²: {item['activity']} @{item['time']}, å®Ÿæ¸¬={item['actual_frustration']}, äºˆæ¸¬=ãªã—ï¼ˆç”Ÿä½“ãƒ‡ãƒ¼ã‚¿ä¸è¶³ï¼‰")

                        except Exception as save_error:
                            logger.error(f"æ–°è¦ç™»éŒ²ã‚¨ãƒ©ãƒ¼: {save_error}")
                            continue

                    logger.warning(f"ğŸ¯ å‡¦ç†å®Œäº†: {user_name}, {predictions_count}ä»¶ã‚’Hourly Logã«ç™»éŒ²")

                    # last_prediction_resultã‚’æ›´æ–°
                    if predictions_count > 0:
                        last_prediction_result[user_id] = {
                            'timestamp': datetime.now(JST).isoformat(),
                            'user_id': user_id,
                            'user_name': user_name,
                            'predictions_count': predictions_count
                        }

                except Exception as user_error:
                    logger.error(f"{user_name} ã®å‡¦ç†ã‚¨ãƒ©ãƒ¼: {user_error}")
                    continue

        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿ç›£è¦–ãƒ«ãƒ¼ãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            logger.error(traceback.format_exc())
            # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚æ¬¡ã®å®Ÿè¡Œæ™‚åˆ»ã¾ã§å¾…æ©Ÿ
            time.sleep(60)

def run_data_monitor_once():
    """
    ãƒ‡ãƒ¼ã‚¿ç›£è¦–ã‚’1å›ã ã‘å®Ÿè¡Œï¼ˆCloud Schedulerç”¨ï¼‰
    æ—¢å­˜ã®data_monitor_loop()ã®ã‚³ã‚¢å‡¦ç†ã‚’1å›å®Ÿè¡Œã™ã‚‹ç‰ˆ
    """
    global last_prediction_result

    logger.warning(f"ğŸ” ãƒ‡ãƒ¼ã‚¿ç›£è¦–é–‹å§‹ï¼ˆ1å›å®Ÿè¡Œï¼‰: {datetime.now(JST).strftime('%Y-%m-%d %H:%M')}")

    # å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒªã‚¹ãƒˆã‚’Configã‹ã‚‰å–å¾—
    users_config = config.get_all_users()

    # å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯
    for user_config in users_config:
        user_id = user_config['user_id']
        user_name = user_config['name']

        try:
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã®predictorã‚’å–å¾—
            predictor = get_predictor(user_id)

            # ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—ï¼‰
            activity_data = sheets_connector.get_activity_data(user_id, use_cache=False)
            fitbit_data = sheets_connector.get_fitbit_data(user_id, use_cache=False)

            if activity_data.empty:
                logger.warning(f"æ´»å‹•ãƒ‡ãƒ¼ã‚¿ãªã—: {user_name}")
                continue

            # nasa_status='done'ã®è¡Œã®ã¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            if 'nasa_status' in activity_data.columns:
                activity_data_done = activity_data[activity_data['nasa_status'] == 'done'].copy()
                logger.warning(f"ğŸ“Š {user_name}: å…¨æ´»å‹•={len(activity_data)}ä»¶, nasa_status='done'={len(activity_data_done)}ä»¶")
            else:
                activity_data_done = activity_data.copy()
                logger.warning(f"âš ï¸ {user_name}: nasa_statusåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…¨æ´»å‹•ã‚’å‡¦ç†ã—ã¾ã™")

            if activity_data_done.empty:
                logger.warning(f"nasa_status='done'ã®æ´»å‹•ãªã—: {user_name}")
                continue

            # ãƒ¢ãƒ‡ãƒ«è¨“ç·´ç¢ºèª
            training_result = ensure_model_trained(user_id, force_retrain=False)
            if training_result['status'] not in ['success', 'already_trained']:
                logger.warning(f"ãƒ¢ãƒ‡ãƒ«è¨“ç·´å¤±æ•— ({user_name}): {training_result.get('message')}")
                continue

            if predictor.model is None:
                logger.error(f"ãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ ({user_name})")
                continue

            # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
            activity_processed = predictor.preprocess_activity_data(activity_data_done)
            df_enhanced = predictor.aggregate_fitbit_by_activity(activity_processed, fitbit_data)

            logger.warning(f"ğŸ” äºˆæ¸¬ãƒã‚§ãƒƒã‚¯é–‹å§‹: {user_name}, å¯¾è±¡æ´»å‹•={len(df_enhanced)}ä»¶")

            # å…¨æœŸé–“ã®Hourly Logã‚’ä¸€åº¦ã«å–å¾—
            all_dates = df_enhanced['Timestamp'].dt.strftime('%Y-%m-%d').unique()
            hourly_log_cache = {}
            for date in all_dates:
                hourly_log_cache[date] = sheets_connector.get_hourly_log(user_id, date)

            logger.warning(f"ğŸ“‹ Hourly Logå–å¾—å®Œäº†: {len(hourly_log_cache)}æ—¥åˆ†")

            # æ–°è¦æ´»å‹•ã®ã¿ã‚’æŠ½å‡º
            new_activities = []
            update_predictions = []

            for idx, row in df_enhanced.iterrows():
                try:
                    timestamp = row['Timestamp']
                    date = timestamp.strftime('%Y-%m-%d')
                    time_str = timestamp.strftime('%H:%M')
                    activity = row.get('CatSub', '')

                    if not activity or pd.isna(activity) or activity == 'unknown':
                        continue

                    actual_frustration = row.get('NASA_F')
                    has_biodata = check_fitbit_data_availability(row)

                    # Hourly Logã«æ—¢ã«å­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                    hourly_log = hourly_log_cache.get(date, pd.DataFrame())
                    is_existing = False
                    existing_predicted = None

                    if not hourly_log.empty:
                        existing = hourly_log[
                            (hourly_log['æ™‚åˆ»'] == time_str) &
                            (hourly_log['æ´»å‹•å'] == activity)
                        ]
                        if not existing.empty:
                            is_existing = True
                            existing_row = existing.iloc[0]
                            existing_predicted = existing_row.get('äºˆæ¸¬NASA_F')

                    if is_existing:
                        if (pd.isna(existing_predicted) or existing_predicted == '') and has_biodata:
                            update_predictions.append({
                                'row': row,
                                'date': date,
                                'time': time_str,
                                'activity': activity,
                                'actual_frustration': actual_frustration
                            })
                        continue

                    # æ–°è¦æ´»å‹•ã¨ã—ã¦è¿½åŠ 
                    new_activities.append({
                        'row': row,
                        'date': date,
                        'time': time_str,
                        'activity': activity,
                        'actual_frustration': actual_frustration,
                        'has_biodata': has_biodata
                    })

                except Exception as parse_error:
                    logger.error(f"æ´»å‹•ãƒ‡ãƒ¼ã‚¿è§£æã‚¨ãƒ©ãƒ¼: {parse_error}")
                    continue

            logger.warning(f"ğŸ“Š æ–°è¦æ´»å‹•: {len(new_activities)}ä»¶, äºˆæ¸¬å€¤æ›´æ–°: {len(update_predictions)}ä»¶")

            # äºˆæ¸¬å€¤æ›´æ–°å‡¦ç†
            predictions_count = 0
            for item in update_predictions:
                try:
                    prediction_result = predictor.predict_from_row(item['row'])
                    if prediction_result and 'predicted_frustration' in prediction_result:
                        predicted_frustration = prediction_result.get('predicted_frustration')
                        if predicted_frustration is not None and not (np.isnan(predicted_frustration) or np.isinf(predicted_frustration)):
                            predicted_frustration = float(predicted_frustration)
                            sheets_connector.update_hourly_log_prediction(
                                user_id, item['date'], item['time'], item['activity'], predicted_frustration
                            )
                            predictions_count += 1
                            logger.warning(f"ğŸ”„ äºˆæ¸¬å€¤æ›´æ–°: {item['activity']} @{item['time']}, äºˆæ¸¬={predicted_frustration:.2f}")
                except Exception as update_error:
                    logger.error(f"äºˆæ¸¬å€¤æ›´æ–°ã‚¨ãƒ©ãƒ¼: {update_error}")
                    continue

            # æ–°è¦æ´»å‹•ä¿å­˜å‡¦ç†
            for item in new_activities:
                try:
                    predicted_frustration = None

                    if item['has_biodata']:
                        prediction_result = predictor.predict_from_row(item['row'])
                        if prediction_result and 'predicted_frustration' in prediction_result:
                            predicted_frustration = prediction_result.get('predicted_frustration')
                            if predicted_frustration is not None and not (np.isnan(predicted_frustration) or np.isinf(predicted_frustration)):
                                predicted_frustration = float(predicted_frustration)
                            else:
                                predicted_frustration = None

                    hourly_data = {
                        'date': item['date'],
                        'time': item['time'],
                        'activity': item['activity'],
                        'actual_frustration': item['actual_frustration'],
                        'predicted_frustration': predicted_frustration
                    }
                    sheets_connector.save_hourly_log(user_id, hourly_data)
                    predictions_count += 1

                    if predicted_frustration:
                        logger.warning(f"âœ… æ–°è¦ç™»éŒ²: {item['activity']} @{item['time']}, äºˆæ¸¬={predicted_frustration:.2f}")
                    else:
                        logger.warning(f"âœ… æ–°è¦ç™»éŒ²: {item['activity']} @{item['time']}, äºˆæ¸¬=ãªã—ï¼ˆç”Ÿä½“ãƒ‡ãƒ¼ã‚¿ä¸è¶³ï¼‰")

                except Exception as save_error:
                    logger.error(f"æ–°è¦ç™»éŒ²ã‚¨ãƒ©ãƒ¼: {save_error}")
                    continue

            logger.warning(f"ğŸ¯ å‡¦ç†å®Œäº†: {user_name}, {predictions_count}ä»¶ã‚’Hourly Logã«ç™»éŒ²")

            # last_prediction_resultã‚’æ›´æ–°
            if predictions_count > 0:
                last_prediction_result[user_id] = {
                    'timestamp': datetime.now(JST).isoformat(),
                    'user_id': user_id,
                    'user_name': user_name,
                    'predictions_count': predictions_count
                }

        except Exception as user_error:
            logger.error(f"{user_name} ã®å‡¦ç†ã‚¨ãƒ©ãƒ¼: {user_error}")
            continue

    logger.warning(f"âœ… ãƒ‡ãƒ¼ã‚¿ç›£è¦–å®Œäº†ï¼ˆ1å›å®Ÿè¡Œï¼‰")
    return {'status': 'success', 'processed_users': len(users_config)}

def initialize_application():
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åˆæœŸåŒ–ï¼ˆä¸€åº¦ã ã‘å®Ÿè¡Œï¼‰"""
    global data_monitor_thread, data_monitor_running, user_predictors, _app_initialized

    # æ—¢ã«åˆæœŸåŒ–æ¸ˆã¿ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
    if _app_initialized:
        return

    try:
        # åˆæœŸåŒ–é–‹å§‹ãƒ­ã‚°ï¼ˆå¸¸ã«å‡ºåŠ›ï¼‰
        logger.warning("ğŸš€ ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’åˆæœŸåŒ–ã—ã¦ã„ã¾ã™...")

        # æ—¢å­˜ã®ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¯ãƒªã‚¢ï¼ˆKNOWN_ACTIVITIESã®æ›´æ–°ãªã©ã«å¯¾å¿œï¼‰
        if user_predictors:
            logger.warning("ğŸ”„ æ—¢å­˜ã®ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚æ¬¡å›ã‚¢ã‚¯ã‚»ã‚¹æ™‚ã«æ–°ã—ã„KNOWN_ACTIVITIESã§å†è¨“ç·´ã•ã‚Œã¾ã™ã€‚")
            user_predictors.clear()

        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼é–‹å§‹
        scheduler.start_scheduler()
        logger.warning("âœ… å®šæœŸãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚’é–‹å§‹ã—ã¾ã—ãŸï¼ˆæ¯æ—¥14:55 UTC = 23:55 JSTã«DiCEå®Ÿè¡Œ + ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ç”Ÿæˆï¼‰")

        # ãƒ‡ãƒ¼ã‚¿æ›´æ–°ç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹
        data_monitor_running = True
        data_monitor_thread = threading.Thread(target=data_monitor_loop, daemon=True)
        data_monitor_thread.start()
        logger.warning("âœ… ãƒ‡ãƒ¼ã‚¿æ›´æ–°ç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é–‹å§‹ã—ã¾ã—ãŸ (15åˆ†ã”ã¨ã«ãƒã‚§ãƒƒã‚¯)")

        _app_initialized = True
        logger.warning("ğŸ‰ ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åˆæœŸåŒ–å®Œäº†")
    except Exception as e:
        logger.error(f"ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")

@app.route('/api/sheets/recreate-prediction', methods=['POST'])
def recreate_prediction_sheet_endpoint():
    """
    PREDICTION_DATAã‚·ãƒ¼ãƒˆã‚’å†ä½œæˆã™ã‚‹APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    """
    try:
        data = request.get_json()
        user_id = data.get('user_id', 'default')

        logger.info(f"PREDICTION_DATAã‚·ãƒ¼ãƒˆå†ä½œæˆãƒªã‚¯ã‚¨ã‚¹ãƒˆ: user_id={user_id}")

        # ã‚·ãƒ¼ãƒˆã‚’å†ä½œæˆ
        result = sheets_connector.recreate_prediction_sheet(user_id)

        if result:
            return jsonify({
                'status': 'success',
                'message': f'PREDICTION_DATA_{user_id}ã‚·ãƒ¼ãƒˆã‚’å†ä½œæˆã—ã¾ã—ãŸ',
                'user_id': user_id
            }), 200
        else:
            return jsonify({
                'status': 'error',
                'message': 'ã‚·ãƒ¼ãƒˆå†ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ',
                'user_id': user_id
            }), 500

    except Exception as e:
        logger.error(f"PREDICTION_DATAã‚·ãƒ¼ãƒˆå†ä½œæˆã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/sheets/recreate-daily-summary', methods=['POST'])
def recreate_daily_summary_sheet_endpoint():
    """
    DAILY_SUMMARYã‚·ãƒ¼ãƒˆã‚’å†ä½œæˆã™ã‚‹APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    """
    try:
        data = request.get_json()
        user_id = data.get('user_id', 'default')

        logger.info(f"DAILY_SUMMARYã‚·ãƒ¼ãƒˆå†ä½œæˆãƒªã‚¯ã‚¨ã‚¹ãƒˆ: user_id={user_id}")

        # ã‚·ãƒ¼ãƒˆã‚’å†ä½œæˆ
        result = sheets_connector.recreate_daily_summary_sheet(user_id)

        if result:
            return jsonify({
                'status': 'success',
                'message': f'DAILY_SUMMARY_{user_id}ã‚·ãƒ¼ãƒˆã‚’å†ä½œæˆã—ã¾ã—ãŸ',
                'user_id': user_id
            }), 200
        else:
            return jsonify({
                'status': 'error',
                'message': 'ã‚·ãƒ¼ãƒˆå†ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ',
                'user_id': user_id
            }), 500

    except Exception as e:
        logger.error(f"DAILY_SUMMARYã‚·ãƒ¼ãƒˆå†ä½œæˆã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

def cleanup_application():
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³çµ‚äº†å‡¦ç†"""
    global data_monitor_running

    try:
        if config.ENABLE_INFO_LOGS:
            logger.info("ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’çµ‚äº†ã—ã¦ã„ã¾ã™...")

        # ãƒ‡ãƒ¼ã‚¿ç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰åœæ­¢
        data_monitor_running = False

        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼åœæ­¢
        scheduler.stop_scheduler()
        if config.ENABLE_INFO_LOGS:
            logger.info("ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³çµ‚äº†å®Œäº†")
    except Exception as e:
        logger.error(f"ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³çµ‚äº†ã‚¨ãƒ©ãƒ¼: {e}")

# Gunicorn/Cloud Runç”¨: ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ™‚ã«åˆæœŸåŒ–
# __name__ == '__main__' ã®å¤–ã§å®Ÿè¡Œã•ã‚Œã‚‹ãŸã‚ã€æœ¬ç•ªç’°å¢ƒã§ã‚‚å‹•ä½œã™ã‚‹
initialize_application()

if __name__ == '__main__':
    try:
        # é–‹ç™ºã‚µãƒ¼ãƒãƒ¼èµ·å‹•ï¼ˆæœ¬ç•ªã§ã¯GunicornãŒä½¿ã‚ã‚Œã‚‹ã®ã§ã“ã®ãƒ–ãƒ­ãƒƒã‚¯ã¯å®Ÿè¡Œã•ã‚Œãªã„ï¼‰
        port = int(os.environ.get('PORT', 8080))
        app.run(host='0.0.0.0', port=port, debug=False, threaded=True)

    except KeyboardInterrupt:
        if config.ENABLE_INFO_LOGS:
            logger.info("ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰å‰²ã‚Šè¾¼ã¿ã«ã‚ˆã‚‹çµ‚äº†")
    except Exception as e:
        logger.error(f"ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
    finally:
        cleanup_application()