"""
ãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å€¤äºˆæ¸¬ãƒ»åå®Ÿä»®æƒ³èª¬æ˜ã‚·ã‚¹ãƒ†ãƒ 
è¡Œå‹•å¤‰æ›´ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§ã®äºˆæ¸¬ã¨DiCEã«ã‚ˆã‚‹æ”¹å–„ææ¡ˆã‚’æä¾›ã™ã‚‹Flaskã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
"""

from flask import Flask, render_template, jsonify, request
import os
import logging
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict
import threading
import time
import json

from ml_model import FrustrationPredictor
from sheets_connector import SheetsConnector
from counterfactual_explainer import ActivityCounterfactualExplainer
from llm_feedback_generator import LLMFeedbackGenerator
from scheduler import FeedbackScheduler
from config import Config

app = Flask(__name__)

# Cloud Runç”¨ãƒ­ã‚°è¨­å®š
config = Config()

# æ§‹é€ åŒ–ãƒ­ã‚°ç”¨ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼
class StructuredFormatter(logging.Formatter):
    def format(self, record):
        log_obj = {
            'severity': record.levelname,
            'message': record.getMessage(),
            'timestamp': datetime.utcnow().isoformat() + 'Z'
        }
        if hasattr(record, 'user_id'):
            log_obj['user_id'] = record.user_id
        if hasattr(record, 'endpoint'):
            log_obj['endpoint'] = record.endpoint
        if record.exc_info:
            log_obj['exception'] = self.formatException(record.exc_info)
        return json.dumps(log_obj)

# ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«è¨­å®š
# ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šä¸€æ™‚çš„ã«INFOãƒ¬ãƒ™ãƒ«ã«è¨­å®šï¼ˆDiCEã¨Appäºˆæ¸¬ã®èª¿æŸ»ã®ãŸã‚ï¼‰
log_level_str = os.environ.get('LOG_LEVEL', config.LOG_LEVEL)
if log_level_str == 'WARNING' and not config.IS_CLOUD_RUN:
    # ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ã¯è‡ªå‹•çš„ã«INFOãƒ¬ãƒ™ãƒ«ã«ã—ã¦ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ã‚’è¡¨ç¤º
    log_level = logging.INFO
    print("âš ï¸ ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰: ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã‚’INFOã«è¨­å®šã—ã¾ã—ãŸ")
else:
    log_level = getattr(logging, log_level_str.upper(), logging.WARNING)

# Cloud Runç’°å¢ƒã§ã¯æ§‹é€ åŒ–ãƒ­ã‚°ã‚’ä½¿ç”¨
if config.IS_CLOUD_RUN:
    handler = logging.StreamHandler()
    handler.setFormatter(StructuredFormatter())
    logging.basicConfig(level=log_level, handlers=[handler])
else:
    # ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ã¯æ¨™æº–ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

logger = logging.getLogger(__name__)

# Flaskæ¨™æº–ãƒ­ã‚°ã®æŠ‘åˆ¶ï¼ˆHTTPã‚¢ã‚¯ã‚»ã‚¹ãƒ­ã‚°ã‚’ç„¡åŠ¹åŒ–ï¼‰
log = logging.getLogger('werkzeug')
log.setLevel(logging.ERROR)
log.disabled = True  # å®Œå…¨ã«ç„¡åŠ¹åŒ–

# GUnicornã‚¢ã‚¯ã‚»ã‚¹ãƒ­ã‚°ã‚‚æŠ‘åˆ¶
gunicorn_logger = logging.getLogger('gunicorn.access')
gunicorn_logger.disabled = True
gunicorn_error_logger = logging.getLogger('gunicorn.error')
gunicorn_error_logger.setLevel(logging.ERROR)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
sheets_connector = SheetsConnector()
explainer = ActivityCounterfactualExplainer()
feedback_generator = LLMFeedbackGenerator()
scheduler = FeedbackScheduler()

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã®ãƒ¢ãƒ‡ãƒ«ç®¡ç†
user_predictors = {}  # {user_id: FrustrationPredictor}

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åˆæœŸåŒ–ãƒ•ãƒ©ã‚°
_app_initialized = False

def get_predictor(user_id: str) -> FrustrationPredictor:
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã®predictorã‚’å–å¾—ï¼ˆå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆï¼‰
    """
    if user_id not in user_predictors:
        logger.info(f"æ–°ã—ã„predictorã‚’ä½œæˆ: user_id={user_id}")
        user_predictors[user_id] = FrustrationPredictor()
    return user_predictors[user_id]

def ensure_model_trained(user_id: str, force_retrain: bool = False) -> dict:
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
    å¿…è¦ã«å¿œã˜ã¦è‡ªå‹•è¨“ç·´ã‚’å®Ÿè¡Œ

    Args:
        user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
        force_retrain: å¼·åˆ¶å†è¨“ç·´

    Returns:
        è¨“ç·´çµæœã¾ãŸã¯çŠ¶æ…‹æƒ…å ±
    """
    predictor = get_predictor(user_id)

    # ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´æ¸ˆã¿ã§å¼·åˆ¶å†è¨“ç·´ã§ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
    if predictor.model is not None and not force_retrain:
        return {
            'status': 'already_trained',
            'message': 'ãƒ¢ãƒ‡ãƒ«ã¯æ—¢ã«è¨“ç·´æ¸ˆã¿ã§ã™'
        }

    # ãƒ‡ãƒ¼ã‚¿å–å¾—
    activity_data = sheets_connector.get_activity_data(user_id)
    fitbit_data = sheets_connector.get_fitbit_data(user_id)

    if activity_data.empty:
        return {
            'status': 'no_data',
            'message': 'ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“',
            'user_id': user_id
        }

    # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
    activity_processed = predictor.preprocess_activity_data(activity_data)
    df_enhanced = predictor.aggregate_fitbit_by_activity(activity_processed, fitbit_data)

    # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
    data_quality = predictor.check_data_quality(df_enhanced)

    if len(df_enhanced) < 10:
        return {
            'status': 'insufficient_data',
            'message': f'ãƒ‡ãƒ¼ã‚¿ä¸è¶³: {len(df_enhanced)}ä»¶ < 10ä»¶',
            'user_id': user_id,
            'data_count': len(df_enhanced),
            'data_quality': data_quality
        }

    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
    try:
        training_results = predictor.walk_forward_validation_train(df_enhanced)

        # è¨“ç·´çµæœã«ã‚¨ãƒ©ãƒ¼ãŒå«ã¾ã‚Œã¦ã„ãªã„ã‹ãƒã‚§ãƒƒã‚¯
        if 'error' in training_results:
            logger.error(f"ãƒ¢ãƒ‡ãƒ«è¨“ç·´å¤±æ•—: user_id={user_id}, error={training_results['error']}")
            return {
                'status': 'error',
                'message': f"è¨“ç·´å¤±æ•—: {training_results['error']}",
                'user_id': user_id,
                'data_quality': data_quality
            }

        # ãƒ¢ãƒ‡ãƒ«ãŒæ­£å¸¸ã«åˆæœŸåŒ–ã•ã‚ŒãŸã‹æœ€çµ‚ç¢ºèª
        if predictor.model is None:
            logger.error(f"ãƒ¢ãƒ‡ãƒ«è¨“ç·´å¾Œã‚‚modelãŒNoneã§ã™: user_id={user_id}")
            return {
                'status': 'error',
                'message': 'ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ',
                'user_id': user_id,
                'data_quality': data_quality
            }

        logger.info(f"ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†: user_id={user_id}, "
                   f"RMSE={training_results.get('walk_forward_rmse', 0):.4f}, "
                   f"RÂ²={training_results.get('walk_forward_r2', 0):.3f}")
        return {
            'status': 'success',
            'message': 'ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†',
            'user_id': user_id,
            'data_count': len(df_enhanced),
            'training_results': training_results,
            'data_quality': data_quality
        }
    except Exception as e:
        logger.error(f"ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã‚¨ãƒ©ãƒ¼: user_id={user_id}, error={e}", exc_info=True)
        return {
            'status': 'error',
            'message': f'è¨“ç·´ã‚¨ãƒ©ãƒ¼: {str(e)}',
            'user_id': user_id
        }

# DiCE daily scheduler
dice_scheduler_thread = None
dice_scheduler_running = False
last_dice_result = {}

# ãƒ‡ãƒ¼ã‚¿æ›´æ–°ç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰
data_monitor_thread = None
data_monitor_running = False
last_prediction_result = {}  # å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®äºˆæ¸¬çµæœã‚’ä¿å­˜: {user_id: prediction_data}

def check_fitbit_data_availability(row):
    """
    Fitbitãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯ã™ã‚‹
    ä¸»è¦ãªç”Ÿä½“æƒ…å ±ï¼ˆå¿ƒæ‹æ•°ã€æ­©æ•°ã€ã‚«ãƒ­ãƒªãƒ¼ç­‰ï¼‰ãŒå­˜åœ¨ã™ã‚‹ã‹ã‚’ç¢ºèª
    """
    try:
        # é‡è¦ãªFitbitçµ±è¨ˆé‡ã‚«ãƒ©ãƒ ã‚’ãƒã‚§ãƒƒã‚¯
        essential_fitbit_columns = [
            'avg_Steps', 'avg_Calories', 'std_Steps', 'std_Calories',
            'max_Steps', 'min_Steps', 'max_Calories', 'min_Calories'
        ]
        
        # å°‘ãªãã¨ã‚‚åŠåˆ†ä»¥ä¸Šã®é‡è¦ãƒ‡ãƒ¼ã‚¿ãŒæœ‰åŠ¹å€¤ã‚’æŒã£ã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚‹
        valid_count = 0
        total_count = len(essential_fitbit_columns)
        
        for col in essential_fitbit_columns:
            value = row.get(col)
            # Noneã€NaNã€ç©ºæ–‡å­—ã€0ä»¥å¤–ã®æœ‰åŠ¹ãªå€¤ã‚’ãƒã‚§ãƒƒã‚¯
            if value is not None and str(value).strip() != '' and pd.notna(value):
                try:
                    float_val = float(value)
                    if float_val > 0:  # 0ã‚ˆã‚Šå¤§ãã„å€¤ã®ã¿æœ‰åŠ¹ã¨ã™ã‚‹
                        valid_count += 1
                except (ValueError, TypeError):
                    continue
        
        # å°‘ãªãã¨ã‚‚60%ä»¥ä¸Šã®é‡è¦ãƒ‡ãƒ¼ã‚¿ãŒæœ‰åŠ¹ãªå ´åˆã€Fitbitãƒ‡ãƒ¼ã‚¿ã‚ã‚Šã¨ã™ã‚‹
        availability_ratio = valid_count / total_count
        is_available = availability_ratio >= 0.6
        
        if not is_available:
            logger.debug(f"Fitbitãƒ‡ãƒ¼ã‚¿ä¸è¶³: {valid_count}/{total_count} ã‚«ãƒ©ãƒ ãŒæœ‰åŠ¹ ({availability_ratio:.1%})")
        
        return is_available
        
    except Exception as e:
        logger.warning(f"Fitbitãƒ‡ãƒ¼ã‚¿å¯ç”¨æ€§ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
        return False

@app.route('/')
def index():
    """ãƒ¡ã‚¤ãƒ³ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ - éå»24æ™‚é–“ã®DiCEçµæœå¯è¦–åŒ–"""
    import time
    timestamp = str(int(time.time()))
    return render_template('frustration_dashboard.html', timestamp=timestamp)

@app.route('/mirror')
def smart_mirror():
    """ã‚¹ãƒãƒ¼ãƒˆãƒŸãƒ©ãƒ¼å°‚ç”¨UI - å®Œå…¨è‡ªå‹•é‹è»¢ãƒ»ã‚¿ãƒƒãƒãƒ¬ã‚¹æ“ä½œ"""
    return render_template('smart_mirror.html')

@app.route('/tablet')
def tablet_mirror():
    """ã‚¿ãƒ–ãƒ¬ãƒƒãƒˆå°‚ç”¨UI - æ‰‹å‹•ãƒ¦ãƒ¼ã‚¶ãƒ¼é¸æŠãƒ»æ—¥æ¬¡å¹³å‡è¡¨ç¤º"""
    return render_template('tablet_mirror.html')

@app.route('/mobile')
def mobile_mirror():
    """ã‚¹ãƒãƒ›å°‚ç”¨UI - æ‰‹å‹•ãƒ¦ãƒ¼ã‚¶ãƒ¼é¸æŠãƒ»ç¸¦ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«è¡¨ç¤º"""
    return render_template('mobile_mirror.html')

@app.route('/trends')
def frustration_trends():
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ¥ãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å€¤æ¨ç§»ç¢ºèªã‚·ãƒ¼ãƒˆ"""
    return render_template('frustration_trends.html')

@app.route('/activity')
def activity_page():
    """æ´»å‹•ãƒ‡ãƒ¼ã‚¿å…¥åŠ›ãƒšãƒ¼ã‚¸ (my-liff-appäº’æ›) - ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆ"""
    from flask import redirect
    return redirect('https://kotoomii.github.io/my-liff-app/activity.html')

@app.route('/nasa')
def nasa_page():
    """NASA-TLXè©•ä¾¡ãƒšãƒ¼ã‚¸ (my-liff-appäº’æ›) - ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆ"""
    from flask import redirect
    return redirect('https://kotoomii.github.io/my-liff-app/nasa.html')

@app.route('/api/frustration/predict', methods=['POST'])
def predict_frustration():
    """
    è¡Œå‹•å¤‰æ›´ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§ã®ãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å€¤äºˆæ¸¬API
    """
    try:
        data = request.get_json()
        user_id = data.get('user_id', 'default')
        target_timestamp = data.get('timestamp')
        
        if target_timestamp:
            target_timestamp = datetime.fromisoformat(target_timestamp)
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã®predictorã‚’å–å¾—
        predictor = get_predictor(user_id)

        # ãƒ‡ãƒ¼ã‚¿å–å¾—
        activity_data = sheets_connector.get_activity_data(user_id)
        fitbit_data = sheets_connector.get_fitbit_data(user_id)

        if activity_data.empty:
            return jsonify({
                'status': 'error',
                'message': 'æ´»å‹•ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'
            }), 400

        # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
        activity_processed = predictor.preprocess_activity_data(activity_data)
        if activity_processed.empty:
            return jsonify({
                'status': 'error',
                'message': 'ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ'
            }), 400

        # Fitbitãƒ‡ãƒ¼ã‚¿ã¨ã®çµ±åˆ
        df_enhanced = predictor.aggregate_fitbit_by_activity(activity_processed, fitbit_data)

        # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
        data_quality = predictor.check_data_quality(df_enhanced)

        # ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´ã•ã‚Œã¦ã„ãªã„å ´åˆã¯è‡ªå‹•è¨“ç·´
        training_info = {'auto_trained': False}
        if predictor.model is None:
            logger.info(f"ãƒ¢ãƒ‡ãƒ«æœªè¨“ç·´: user_id={user_id}, è‡ªå‹•è¨“ç·´ã‚’é–‹å§‹ã—ã¾ã™")
            training_result = ensure_model_trained(user_id)
            training_info = {
                'auto_trained': True,
                'status': training_result.get('status'),
                'message': training_result.get('message')
            }

            # è¨“ç·´ãŒå¤±æ•—ã—ãŸå ´åˆã¯ã‚¨ãƒ©ãƒ¼ã‚’è¿”ã™
            if training_result.get('status') != 'success':
                return jsonify({
                    'status': 'error',
                    'message': f"ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã«å¤±æ•—ã—ã¾ã—ãŸ: {training_result.get('message')}",
                    'user_id': user_id,
                    'data_quality': data_quality,
                    'training_result': training_result
                }), 400

        # ãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å€¤äºˆæ¸¬
        prediction_result = predictor.predict_frustration_at_activity_change(
            df_enhanced, target_timestamp
        )

        # ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±
        training_results = training_info if training_info['auto_trained'] else {'status': 'already_trained'}

        # è¡Œå‹•å¤‰æ›´ã‚¿ã‚¤ãƒŸãƒ³ã‚°ä¸€è¦§
        change_timestamps = predictor.get_activity_change_timestamps(df_enhanced)

        response = {
            'status': 'success',
            'user_id': user_id,
            'prediction': prediction_result,
            'activity_change_timestamps': [ts.isoformat() for ts in change_timestamps],
            'model_performance': training_results,
            'data_quality': data_quality,
            'timestamp': datetime.now().isoformat()
        }

        # ãƒ‡ãƒ¼ã‚¿ä¸è¶³æ™‚ã®è­¦å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
        if not data_quality['is_sufficient']:
            response['warning'] = {
                'message': 'ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚ã€äºˆæ¸¬ç²¾åº¦ãŒä½ã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚',
                'details': data_quality['warnings'],
                'recommendations': data_quality['recommendations']
            }

        return jsonify(response)
        
    except Exception as e:
        logger.error(f"ãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å€¤äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/frustration/predict-activity', methods=['POST'])
def predict_activity_frustration():
    """
    æ–°ã—ã„æ´»å‹•å…¥åŠ›æ™‚ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³äºˆæ¸¬API
    """
    try:
        from datetime import datetime, timedelta
        
        data = request.get_json()
        user_id = data.get('user_id', 'default')
        activity_category = data.get('CatSub', 'ãã®ä»–')  # æ´»å‹•ã‚«ãƒ†ã‚´ãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ãã®ä»–ï¼‰
        activity_subcategory = data.get('CatMid', activity_category)  # æ´»å‹•ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒª

        # æ´»å‹•ã‚«ãƒ†ã‚´ãƒªã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        if not activity_category or activity_category.strip() == '':
            activity_category = 'ãã®ä»–'
            logger.warning("æ´»å‹•ã‚«ãƒ†ã‚´ãƒªãŒç©ºã¾ãŸã¯Noneã§ã™ã€‚'ãã®ä»–'ã«è¨­å®šã—ã¾ã—ãŸã€‚")
        
        # æ™‚é–“ã®è¨ˆç®— - start_timeã¨end_timeãŒã‚ã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨
        start_time = data.get('start_time')
        end_time = data.get('end_time')
        duration = data.get('Duration')
        
        if start_time and end_time and not duration:
            # start_timeã¨end_timeã‹ã‚‰æ™‚é–“ã‚’è¨ˆç®—
            try:
                # æ™‚åˆ»å½¢å¼ã‚’è§£æ (HH:MMå½¢å¼ã‚’æƒ³å®š)
                start_hour, start_min = map(int, start_time.split(':'))
                end_hour, end_min = map(int, end_time.split(':'))
                
                start_total_min = start_hour * 60 + start_min
                end_total_min = end_hour * 60 + end_min
                
                # æ—¥ä»˜ã‚’ã¾ãŸãå ´åˆã‚’è€ƒæ…®
                if end_total_min < start_total_min:
                    end_total_min += 24 * 60  # ç¿Œæ—¥ã¨ã¿ãªã™
                
                duration = end_total_min - start_total_min
                if config.ENABLE_DEBUG_LOGS:
                    logger.debug(f"æ™‚é–“è¨ˆç®—: {start_time} â†’ {end_time} = {duration}åˆ†")
                
            except (ValueError, AttributeError) as e:
                logger.warning(f"æ™‚åˆ»è§£æã‚¨ãƒ©ãƒ¼: {e}, ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ60åˆ†ã‚’ä½¿ç”¨")
                duration = 60
        elif not duration:
            duration = 60  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
        timestamp = data.get('timestamp', datetime.now().isoformat())
        
        if isinstance(timestamp, str):
            timestamp = datetime.fromisoformat(timestamp)
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã®predictorã‚’å–å¾—
        predictor = get_predictor(user_id)

        # ãƒ‡ãƒãƒƒã‚°: predictorã®çŠ¶æ…‹ç¢ºèª
        logger.warning(f"ğŸ” predict-activityå‘¼ã³å‡ºã—: user_id={user_id}, predictor.model={'ã‚ã‚Š' if predictor.model else 'ãªã—'}")

        # éå»ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ»å‰å‡¦ç†
        activity_data = sheets_connector.get_activity_data(user_id)
        fitbit_data = sheets_connector.get_fitbit_data(user_id)

        if activity_data.empty:
            # ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®ãŸã‚äºˆæ¸¬ä¸å¯
            return jsonify({
                'status': 'error',
                'message': 'ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚æ´»å‹•ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²ã—ã¦ãã ã•ã„ã€‚',
                'user_id': user_id,
                'activity': activity_category,
                'duration': duration,
                'timestamp': timestamp.isoformat()
            }), 400

        # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã¨ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
        activity_processed = predictor.preprocess_activity_data(activity_data)
        df_enhanced = predictor.aggregate_fitbit_by_activity(activity_processed, fitbit_data)

        # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
        data_quality = predictor.check_data_quality(df_enhanced)

        # ãƒ‡ãƒãƒƒã‚°: ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºç¢ºèª
        logger.warning(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ç¢ºèª: df_enhanced={len(df_enhanced)}ä»¶, predictor.model={'ã‚ã‚Š' if predictor.model else 'ãªã—'}")

        # ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´ã•ã‚Œã¦ã„ãªã„å ´åˆã¯è‡ªå‹•è¨“ç·´
        if predictor.model is None:
            logger.warning(f"ğŸ” predictor.model is None - è¨“ç·´ãƒã‚§ãƒƒã‚¯ã«å…¥ã‚Šã¾ã™")
            if len(df_enhanced) >= 10:
                logger.warning(f"âš ï¸ ãƒ¢ãƒ‡ãƒ«æœªè¨“ç·´: user_id={user_id}, è‡ªå‹•è¨“ç·´ã‚’é–‹å§‹ã—ã¾ã™")
                training_result = ensure_model_trained(user_id)

                logger.warning(f"ğŸ“Š è¨“ç·´çµæœ: {training_result.get('status')} - {training_result.get('message', '')}")

                # è¨“ç·´ãŒå¤±æ•—ã—ãŸå ´åˆã¯ã‚¨ãƒ©ãƒ¼ã‚’è¿”ã™
                if training_result.get('status') != 'success':
                    logger.error(f"âŒ ãƒ¢ãƒ‡ãƒ«è¨“ç·´å¤±æ•—: {training_result.get('message')}")
                    return jsonify({
                        'status': 'error',
                        'message': f"ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã«å¤±æ•—ã—ã¾ã—ãŸ: {training_result.get('message')}",
                        'user_id': user_id,
                        'data_quality': data_quality,
                        'training_result': training_result
                    }), 400
                else:
                    logger.warning(f"âœ… ãƒ¢ãƒ‡ãƒ«è¨“ç·´æˆåŠŸ: user_id={user_id}")
            else:
                # ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®å ´åˆ
                logger.warning(f"âŒ ãƒ‡ãƒ¼ã‚¿ä¸è¶³: {len(df_enhanced)}ä»¶ < 10ä»¶")
                return jsonify({
                    'status': 'error',
                    'message': f'ãƒ‡ãƒ¼ã‚¿ä¸è¶³: {len(df_enhanced)}ä»¶ < 10ä»¶ã€‚ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã§ãã¾ã›ã‚“ã€‚',
                    'user_id': user_id,
                    'data_count': len(df_enhanced),
                    'data_quality': data_quality,
                    'warning': {
                        'message': 'ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã§ãã¾ã›ã‚“ã€‚',
                        'recommendations': data_quality.get('recommendations', [])
                    }
                }), 400
        else:
            # ãƒ¢ãƒ‡ãƒ«ãŒæ—¢ã«è¨“ç·´æ¸ˆã¿ã®å ´åˆ
            logger.warning(f"âœ… ãƒ¢ãƒ‡ãƒ«æ—¢è¨“ç·´æ¸ˆã¿: user_id={user_id}, predictor.modelãŒå­˜åœ¨ã—ã¾ã™")

        # æ–°ã—ã„æ´»å‹•ã®ãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å€¤äºˆæ¸¬ï¼ˆå±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰
        prediction_result = predictor.predict_with_history(
            activity_category,
            duration,
            timestamp,
            df_enhanced  # å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’æ¸¡ã™
        )

        if 'error' in prediction_result:
            return jsonify({
                'status': 'error',
                'message': prediction_result['error'],
                'data_quality': data_quality
            }), 400

        predicted_frustration = prediction_result['predicted_frustration']
        confidence = prediction_result['confidence']

        # NaN/Infãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        import numpy as np
        if np.isnan(predicted_frustration) or np.isinf(predicted_frustration):
            return jsonify({
                'status': 'error',
                'message': 'äºˆæ¸¬å€¤ã®è¨ˆç®—ã«å¤±æ•—ã—ã¾ã—ãŸ (NaN/Inf)',
                'user_id': user_id,
                'activity': activity_category,
                'data_quality': data_quality
            }), 400

        # ãƒ‡ãƒ¼ã‚¿å“è³ªã«åŸºã¥ã„ã¦ä¿¡é ¼åº¦ã‚’èª¿æ•´
        if not data_quality['is_sufficient']:
            confidence = min(confidence, 0.3)  # ãƒ‡ãƒ¼ã‚¿ä¸è¶³æ™‚ã¯ä¿¡é ¼åº¦ã‚’ä¸‹ã’ã‚‹
        elif data_quality['quality_level'] == 'minimal':
            confidence = min(confidence, 0.5)

        # äºˆæ¸¬çµæœã‚’ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«è¨˜éŒ²
        prediction_data = {
            'timestamp': timestamp.isoformat(),
            'user_id': user_id,
            'activity': activity_category,
            'duration': duration,
            'predicted_frustration': float(predicted_frustration),  # æ˜ç¤ºçš„ã«floatå¤‰æ›
            'confidence': float(confidence),  # æ˜ç¤ºçš„ã«floatå¤‰æ›
            'source': 'manual_api',  # æ‰‹å‹•APIäºˆæ¸¬ã§ã‚ã‚‹ã“ã¨ã‚’æ˜è¨˜
            'notes': f'Subcategory: {activity_subcategory}'
        }

        # æ‰‹å‹•äºˆæ¸¬ã®å ´åˆã®ã¿ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ä¿å­˜ï¼ˆè‡ªå‹•ç›£è¦–ã¨ã®é‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚ï¼‰
        # ãƒ‡ãƒ¼ã‚¿ç›£è¦–ãƒ«ãƒ¼ãƒ—ã«ã‚ˆã‚‹è‡ªå‹•äºˆæ¸¬çµæœã¯ data_monitor_loop ã§ä¿å­˜ã•ã‚Œã‚‹
        try:
            sheets_connector.save_prediction_data(prediction_data)
            if config.LOG_PREDICTIONS:
                logger.info(f"æ‰‹å‹•äºˆæ¸¬çµæœã‚’ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«è¨˜éŒ²: {user_id}, {activity_category}, äºˆæ¸¬å€¤: {predicted_frustration:.2f}")
        except Exception as save_error:
            logger.error(f"äºˆæ¸¬çµæœä¿å­˜ã‚¨ãƒ©ãƒ¼: {save_error}")
            # ä¿å­˜ã‚¨ãƒ©ãƒ¼ãŒã‚ã£ã¦ã‚‚APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã«ã¯å½±éŸ¿ã—ãªã„

        response = {
            'status': 'success',
            'user_id': user_id,
            'predicted_frustration': round(float(predicted_frustration), 2),
            'activity': activity_category,
            'subcategory': activity_subcategory,
            'duration': duration,
            'confidence': round(float(confidence), 3),
            'timestamp': timestamp.isoformat(),
            'logged_to_sheets': True,
            'data_quality': data_quality
        }

        # ãƒ‡ãƒ¼ã‚¿ä¸è¶³æ™‚ã®è­¦å‘Šã‚’è¿½åŠ 
        if not data_quality['is_sufficient']:
            response['warning'] = {
                'message': 'ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚ã€äºˆæ¸¬ç²¾åº¦ãŒä½ããªã£ã¦ã„ã¾ã™ã€‚',
                'details': data_quality['warnings'],
                'recommendations': data_quality['recommendations']
            }

        return jsonify(response)
        
    except Exception as e:
        logger.error(f"æ´»å‹•ãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/frustration/daily-dice-schedule', methods=['POST'])
def generate_daily_dice_schedule():
    """
    1æ—¥ã®çµ‚ã‚ã‚Šã«æ™‚é–“ã”ã¨ã®DiCEæ”¹å–„ææ¡ˆã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ç”Ÿæˆ
    """
    try:
        data = request.get_json()
        user_id = data.get('user_id', 'default')
        target_date = data.get('date', datetime.now().date().isoformat())

        if isinstance(target_date, str):
            target_date = datetime.fromisoformat(target_date).date()

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã®predictorã‚’å–å¾—
        predictor = get_predictor(user_id)

        # ãã®æ—¥ã®æ´»å‹•ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        activity_data = sheets_connector.get_activity_data(user_id)
        fitbit_data = sheets_connector.get_fitbit_data(user_id)

        if activity_data.empty:
            return jsonify({
                'status': 'error',
                'message': 'æ´»å‹•ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'
            }), 400

        # æŒ‡å®šæ—¥ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        activity_data['Date'] = pd.to_datetime(activity_data['Timestamp']).dt.date
        daily_activities = activity_data[activity_data['Date'] == target_date].copy()

        if daily_activities.empty:
            return jsonify({
                'status': 'success',
                'user_id': user_id,
                'date': target_date.isoformat(),
                'message': 'ãã®æ—¥ã®æ´»å‹•ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“',
                'schedule': [],
                'recommendations': []
            })

        # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã¨ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
        activity_processed = predictor.preprocess_activity_data(activity_data)
        df_enhanced = predictor.aggregate_fitbit_by_activity(activity_processed, fitbit_data)

        # ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªï¼ˆDiCEå®Ÿè¡Œå‰ã«å¿…é ˆï¼‰
        training_result = ensure_model_trained(user_id)
        if training_result.get('status') not in ['success', 'already_trained']:
            return jsonify({
                'status': 'error',
                'message': f"ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã«å¤±æ•—ã—ã¾ã—ãŸ: {training_result.get('message')}",
                'user_id': user_id,
                'training_result': training_result
            }), 400

        # 1æ™‚é–“ã”ã¨ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ææ¡ˆã‚’ç”Ÿæˆï¼ˆDiCEæ–¹å¼ï¼‰
        dice_result = explainer.generate_hourly_alternatives(
            df_enhanced,  # å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’æ¸¡ã™
            predictor,
            target_date
        )
        
        if dice_result and dice_result.get('hourly_schedule'):
            hourly_schedule = dice_result['hourly_schedule']
            message = dice_result.get('message', 'æ™‚é–“åˆ¥æ”¹å–„ææ¡ˆã‚’ç”Ÿæˆã—ã¾ã—ãŸ')
            total_improvement = dice_result.get('total_improvement', 0)
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šåŸºæœ¬çš„ãªæ™‚é–“åˆ¥ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
            hourly_schedule = []
            daily_activities = daily_activities.sort_values('Timestamp')
            
            for hour in range(24):  # 0-23æ™‚
                hour_start = datetime.combine(target_date, datetime.min.time()) + timedelta(hours=hour)
                hour_end = hour_start + timedelta(hours=1)
                
                # ãã®æ™‚é–“å¸¯ã®æ´»å‹•ã‚’å–å¾—
                hour_activities = daily_activities[
                    (daily_activities['Timestamp'] >= hour_start) & 
                    (daily_activities['Timestamp'] < hour_end)
                ]
                
                if not hour_activities.empty:
                    # å®Ÿéš›ã®æ´»å‹•ã¨ãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å€¤
                    actual_activity = hour_activities.iloc[0]
                    actual_frustration = actual_activity.get('NASA_F', 10.0)
                    
                
                # ãã®æ™‚é–“ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«æƒ…å ±
                hour_info = {
                    'hour': hour,
                    'time_range': f"{hour:02d}:00-{(hour+1):02d}:00",
                    'actual_activity': actual_activity['CatSub'],
                    'actual_frustration': round(float(actual_frustration), 2),
                    'actual_duration': int(actual_activity.get('Duration', 60)),
                    'suggested_activity': None,
                    'suggested_frustration': None,
                    'improvement': 0.0,
                    'has_suggestion': False
                }
                
                # DiCEææ¡ˆãŒã‚ã‚‹å ´åˆ
                if dice_suggestions and dice_suggestions.get('alternatives'):
                    best_alternative = dice_suggestions['alternatives'][0]
                    suggested_frustration = best_alternative.get('predicted_frustration', actual_frustration)
                    improvement = actual_frustration - suggested_frustration
                    
                    if improvement > 0.5:  # 0.5ä»¥ä¸Šã®æ”¹å–„ãŒè¦‹è¾¼ã‚ã‚‹å ´åˆã®ã¿ææ¡ˆ
                        hour_info.update({
                            'suggested_activity': best_alternative.get('activity', actual_activity['CatSub']),
                            'suggested_frustration': round(suggested_frustration, 2),
                            'improvement': round(improvement, 2),
                            'has_suggestion': True
                        })
                        
                        # æ—¥æ¬¡æ¨å¥¨äº‹é …ã«è¿½åŠ 
                        daily_recommendations.append({
                            'time': f"{hour:02d}:00",
                            'original': f"{actual_activity['CatSub']} (ãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: {actual_frustration:.1f})",
                            'suggested': f"{best_alternative.get('activity')} (äºˆæ¸¬ãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: {suggested_frustration:.1f})",
                            'improvement': round(improvement, 2),
                            'reason': f"ã“ã®æ™‚é–“ã«{best_alternative.get('activity')}ã‚’è¡Œã†ã“ã¨ã§ã€ãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’{improvement:.1f}ãƒã‚¤ãƒ³ãƒˆå‰Šæ¸›ã§ãã¾ã™"
                        })
                
                hourly_schedule.append(hour_info)
            
            else:
                # ãã®æ™‚é–“ã«æ´»å‹•ãŒãªã„å ´åˆ
                hourly_schedule.append({
                    'hour': hour,
                    'time_range': f"{hour:02d}:00-{(hour+1):02d}:00",
                    'actual_activity': None,
                    'actual_frustration': None,
                    'actual_duration': 0,
                    'suggested_activity': None,
                    'suggested_frustration': None,
                    'improvement': 0.0,
                    'has_suggestion': False
                })
        
        # å…¨ä½“çµ±è¨ˆ
        total_actual_frustration = sum([h['actual_frustration'] for h in hourly_schedule if h['actual_frustration'] is not None])
        total_suggested_frustration = sum([h['suggested_frustration'] for h in hourly_schedule if h['suggested_frustration'] is not None])
        total_improvement = sum([h['improvement'] for h in hourly_schedule])
        
        # DiCEã«ã‚ˆã‚‹æ™‚é–“åˆ¥æ”¹å–„ææ¡ˆãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼ã«çµ±ä¸€
        if dice_result and dice_result.get('hourly_schedule'):
            return jsonify({
                'status': 'success',
                'user_id': user_id,
                'date': target_date.isoformat(),
                'schedule': dice_result['hourly_schedule'],
                'message': dice_result.get('message', 'ä»Šæ—¥ã“ã®ã‚ˆã†ãªæ´»å‹•ã‚’ã—ã¦ã„ãŸã‚‰ã‚¹ãƒˆãƒ¬ã‚¹ãƒ¬ãƒ™ãƒ«ãŒä¸‹ãŒã£ã¦ã„ã¾ã—ãŸ'),
                'total_improvement': dice_result.get('total_improvement', 0),
                'summary': dice_result.get('summary', ''),
                'confidence': dice_result.get('confidence', 0.7),
                'generated_at': datetime.now().isoformat()
            })
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¿œç­”
            return jsonify({
                'status': 'success',
                'user_id': user_id,
                'date': target_date.isoformat(),
                'schedule': [],
                'message': 'ä»Šæ—¥ã“ã®ã‚ˆã†ãªæ´»å‹•ã‚’ã—ã¦ã„ãŸã‚‰ã‚¹ãƒˆãƒ¬ã‚¹ãƒ¬ãƒ™ãƒ«ãŒä¸‹ãŒã£ã¦ã„ã¾ã—ãŸ',
                'total_improvement': 0,
                'summary': 'ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚ã€è©³ç´°ãªææ¡ˆã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸ',
                'confidence': 0.3,
                'generated_at': datetime.now().isoformat()
            })
        
    except Exception as e:
        logger.error(f"æ—¥æ¬¡DiCEã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/frustration/dice-analysis', methods=['POST'])
def generate_dice_analysis():
    """
    éå»24æ™‚é–“ã®è¡Œå‹•ã«å¯¾ã™ã‚‹DiCEåˆ†æAPI
    """
    try:
        data = request.get_json()
        user_id = data.get('user_id', 'default')
        target_timestamp = data.get('timestamp')
        lookback_hours = data.get('lookback_hours', 24)

        if target_timestamp:
            target_timestamp = datetime.fromisoformat(target_timestamp)

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã®predictorã‚’å–å¾—
        predictor = get_predictor(user_id)

        # ãƒ‡ãƒ¼ã‚¿å–å¾—
        activity_data = sheets_connector.get_activity_data(user_id)
        fitbit_data = sheets_connector.get_fitbit_data(user_id)

        if activity_data.empty:
            return jsonify({
                'status': 'error',
                'message': 'æ´»å‹•ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'
            }), 400

        # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
        activity_processed = predictor.preprocess_activity_data(activity_data)
        if activity_processed.empty:
            return jsonify({
                'status': 'error',
                'message': 'ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ'
            }), 400

        # Fitbitãƒ‡ãƒ¼ã‚¿ã¨ã®çµ±åˆ
        df_enhanced = predictor.aggregate_fitbit_by_activity(activity_processed, fitbit_data)

        # ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªï¼ˆDiCEå®Ÿè¡Œå‰ã«å¿…é ˆï¼‰
        training_result = ensure_model_trained(user_id)
        if training_result.get('status') not in ['success', 'already_trained']:
            return jsonify({
                'status': 'error',
                'message': f"ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã«å¤±æ•—ã—ã¾ã—ãŸ: {training_result.get('message')}",
                'user_id': user_id,
                'training_result': training_result
            }), 400

        # DiCEåˆ†æå®Ÿè¡Œ
        dice_result = explainer.generate_activity_based_explanation(
            df_enhanced, predictor, target_timestamp, lookback_hours
        )
        
        return jsonify({
            'status': 'success',
            'user_id': user_id,
            'dice_analysis': dice_result,
            'lookback_hours': lookback_hours,
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"DiCEåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/frustration/timeline', methods=['POST'])
def get_frustration_timeline():
    """
    éå»24æ™‚é–“ã®ãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å€¤ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³å–å¾—API
    """
    try:
        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ç¢ºèªã¨ä¿®æ­£
        if request.is_json:
            data = request.get_json()
            if data is None:
                data = {}
        else:
            data = {}
        
        user_id = data.get('user_id', 'default')
        date = data.get('date', datetime.now().strftime('%Y-%m-%d'))
        
        if config.ENABLE_DEBUG_LOGS:
            logger.debug(f"Timeline APIå‘¼ã³å‡ºã— - user_id: {user_id}, date: {date}")
        
        # ãƒ‡ãƒ¼ã‚¿å–å¾—
        activity_data = sheets_connector.get_activity_data(user_id)
        fitbit_data = sheets_connector.get_fitbit_data(user_id)
        
        # Google Sheetsæ¥ç¶šã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯é©åˆ‡ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿”ã™
        if activity_data.empty:
            if sheets_connector.gc is None:
                return jsonify({
                    'status': 'success',
                    'date': date,
                    'timeline': [],
                    'is_new_user': True,
                    'message': 'Google Sheetsã«æ¥ç¶šã§ãã¾ã›ã‚“ã€‚èªè¨¼è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚'
                })
            else:
                return jsonify({
                    'status': 'success',
                    'date': date,
                    'timeline': [],
                    'is_new_user': True,
                    'message': 'ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“',
                    'warning': {
                        'message': 'æ–°è¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¾ãŸã¯ãƒ‡ãƒ¼ã‚¿æœªè¨˜éŒ²ã§ã™ã€‚',
                        'recommendations': [
                            'æ´»å‹•ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²ã—ã¦ãã ã•ã„ã€‚',
                            'ãƒ‡ãƒ¼ã‚¿ãŒè“„ç©ã•ã‚Œã‚‹ã¨ã€ãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³äºˆæ¸¬ã¨DiCEææ¡ˆãŒåˆ©ç”¨å¯èƒ½ã«ãªã‚Šã¾ã™ã€‚'
                        ]
                    }
                })
        
        # æŒ‡å®šæ—¥ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        target_date = datetime.strptime(date, '%Y-%m-%d').date()
        if 'Timestamp' in activity_data.columns:
            activity_data['date'] = pd.to_datetime(activity_data['Timestamp']).dt.date
            daily_data = activity_data[activity_data['date'] == target_date]
        else:
            daily_data = pd.DataFrame()
        
        if daily_data.empty:
            return jsonify({
                'status': 'success',
                'date': date,
                'timeline': [],
                'message': 'æŒ‡å®šæ—¥ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'
            })
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã®predictorã‚’å–å¾—
        predictor = get_predictor(user_id)

        # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
        activity_processed = predictor.preprocess_activity_data(daily_data)
        df_enhanced = predictor.aggregate_fitbit_by_activity(activity_processed, fitbit_data)

        # ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´ã•ã‚Œã¦ã„ãªã„å ´åˆã¯è‡ªå‹•è¨“ç·´
        training_info = None
        if predictor.model is None:
            logger.info(f"ãƒ¢ãƒ‡ãƒ«æœªè¨“ç·´: user_id={user_id}, è‡ªå‹•è¨“ç·´ã‚’é–‹å§‹ã—ã¾ã™")
            training_result = ensure_model_trained(user_id)
            training_info = {
                'auto_trained': True,
                'status': training_result.get('status'),
                'message': training_result.get('message')
            }

            if training_result.get('status') != 'success':
                # è¨“ç·´å¤±æ•—æ™‚ã¯è­¦å‘Šã‚’å«ã‚ã¦ç¶™ç¶š
                logger.warning(f"è‡ªå‹•è¨“ç·´å¤±æ•—: {training_result.get('message')}")
        else:
            training_info = {
                'auto_trained': False,
                'status': 'already_trained'
            }

        # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ä½œæˆï¼ˆãƒ¢ãƒ‡ãƒ«äºˆæ¸¬å€¤ã‚’ä½¿ç”¨ï¼‰
        timeline = []
        for idx, row in df_enhanced.iterrows():
            predicted_frustration = None

            # Fitbitãƒ‡ãƒ¼ã‚¿ã®æœ‰ç„¡ã‚’ãƒã‚§ãƒƒã‚¯
            has_fitbit_data = check_fitbit_data_availability(row)

            # Fitbitãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã®ã¿äºˆæ¸¬ã‚’å®Ÿè¡Œ
            if has_fitbit_data:
                # å„æ´»å‹•ã«å¯¾ã—ã¦ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ã‚’å®Ÿè¡Œï¼ˆå®Ÿæ¸¬å€¤ã®ç”Ÿä½“æƒ…å ±ã‚’ä½¿ç”¨ï¼‰
                try:
                    # è¡Œãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç›´æ¥äºˆæ¸¬ï¼ˆDiCEã¨åŒã˜æ–¹æ³•ï¼‰
                    prediction_result = predictor.predict_from_row(row)

                    if prediction_result and 'predicted_frustration' in prediction_result:
                        predicted_frustration = prediction_result['predicted_frustration']
                        if config.ENABLE_DEBUG_LOGS:
                            logger.debug(f"Timelineäºˆæ¸¬: {row.get('CatSub')} at {row['Timestamp']}, Få€¤={predicted_frustration:.2f} (å®Ÿæ¸¬å€¤ãƒ™ãƒ¼ã‚¹)")

                except Exception as e:
                    logger.warning(f"äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
                    predicted_frustration = None
            else:
                if config.ENABLE_DEBUG_LOGS:
                    logger.debug(f"Fitbitãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®ãŸã‚äºˆæ¸¬ã‚’ã‚¹ã‚­ãƒƒãƒ—: {row.get('Timestamp', 'unknown')}")
                predicted_frustration = None
            
            # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã«è¿½åŠ ï¼ˆäºˆæ¸¬å€¤ãŒãªã„å ´åˆã¯å®Ÿæ¸¬å€¤ã‚’ä½¿ç”¨ï¼‰
            frustration_to_use = predicted_frustration
            if frustration_to_use is None:
                # äºˆæ¸¬å€¤ãŒãªã„å ´åˆã¯å®Ÿæ¸¬å€¤ï¼ˆNASA_Fï¼‰ã‚’ä½¿ç”¨
                frustration_to_use = row.get('NASA_F')
                if config.ENABLE_DEBUG_LOGS:
                    logger.debug(f"äºˆæ¸¬å€¤ãªã—ã€å®Ÿæ¸¬å€¤ã‚’ä½¿ç”¨: {row.get('CatSub', 'unknown')} at {row['Timestamp']}")

            # äºˆæ¸¬å€¤ã‚‚å®Ÿæ¸¬å€¤ã‚‚ãªã„å ´åˆã®ã¿ã‚¹ã‚­ãƒƒãƒ—
            if frustration_to_use is not None:
                timeline.append({
                    'timestamp': row['Timestamp'].isoformat(),
                    'hour': row.get('hour', 0),
                    'activity': row.get('CatSub', 'unknown'),
                    'duration': row.get('Duration', 0),
                    'frustration_value': float(frustration_to_use),  # äºˆæ¸¬å€¤ã¾ãŸã¯å®Ÿæ¸¬å€¤
                    'actual_frustration': row.get('NASA_F'),      # å®Ÿãƒ‡ãƒ¼ã‚¿ã‚‚ä¿æŒï¼ˆæ¯”è¼ƒç”¨ï¼‰
                    'is_predicted': predicted_frustration is not None,  # äºˆæ¸¬å€¤ã‹ã©ã†ã‹ã®ãƒ•ãƒ©ã‚°
                    'activity_change': row.get('activity_change', 0) == 1,
                    'lorenz_stats': {
                        'mean': row.get('lorenz_mean', 0),
                        'std': row.get('lorenz_std', 0)
                    }
                })
            else:
                if config.ENABLE_DEBUG_LOGS:
                    logger.debug(f"æ´»å‹•ã‚’ã‚¹ã‚­ãƒƒãƒ—: äºˆæ¸¬å€¤ã‚‚å®Ÿæ¸¬å€¤ã‚‚ã‚ã‚Šã¾ã›ã‚“ - {row.get('CatSub', 'unknown')} at {row['Timestamp']}")
        
        # æ™‚é–“é †ã«ã‚½ãƒ¼ãƒˆ
        timeline.sort(key=lambda x: x['timestamp'])
        
        return jsonify({
            'status': 'success',
            'user_id': user_id,
            'date': date,
            'timeline': timeline,
            'total_entries': len(timeline),
            'training_info': training_info
        })
        
    except Exception as e:
        logger.error(f"ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/feedback/generate', methods=['POST'])
def generate_feedback():
    """
    LLMã‚’ä½¿ç”¨ã—ãŸè‡ªç„¶è¨€èªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ç”ŸæˆAPI (æ—¥æ¬¡ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®ã¿)
    """
    try:
        data = request.get_json()
        user_id = data.get('user_id', 'default')
        # 'type'ã¨'feedback_type'ã®ä¸¡æ–¹ã«å¯¾å¿œ
        feedback_type = data.get('feedback_type', data.get('type', 'daily'))

        # ä»Šæ—¥ã®æ—¥ä»˜ã‚’å–å¾—
        today = datetime.now().strftime('%Y-%m-%d')

        logger.info(f"æ—¥æ¬¡ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ç”Ÿæˆé–‹å§‹: user_id={user_id}, date={today}")

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã®predictorã‚’å–å¾—
        predictor = get_predictor(user_id)

        # ãƒ‡ãƒ¼ã‚¿å–å¾—
        activity_data = sheets_connector.get_activity_data(user_id)
        fitbit_data = sheets_connector.get_fitbit_data(user_id)

        if activity_data.empty:
            return jsonify({
                'status': 'error',
                'message': 'æ´»å‹•ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'
            }), 400

        # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
        activity_processed = predictor.preprocess_activity_data(activity_data)
        df_enhanced = predictor.aggregate_fitbit_by_activity(activity_processed, fitbit_data)

        # ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
        training_result = ensure_model_trained(user_id)
        if training_result.get('status') not in ['success', 'already_trained']:
            return jsonify({
                'status': 'error',
                'message': f"ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã«å¤±æ•—ã—ã¾ã—ãŸ: {training_result.get('message')}",
                'user_id': user_id
            }), 400

        # ä»Šæ—¥ã®DiCEåˆ†æã‚’å®Ÿè¡Œ
        dice_result = explainer.generate_activity_based_explanation(
            df_enhanced, predictor, None, 24
        )

        # ä»Šæ—¥ã®ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆå®Ÿæ¸¬å€¤ã¨äºˆæ¸¬å€¤ã‚’å«ã‚€ï¼‰
        timeline_data = []
        target_date = datetime.strptime(today, '%Y-%m-%d').date()

        if 'Timestamp' in activity_data.columns:
            activity_data['date'] = pd.to_datetime(activity_data['Timestamp']).dt.date
            daily_data = activity_data[activity_data['date'] == target_date]

            if not daily_data.empty:
                activity_processed_daily = predictor.preprocess_activity_data(daily_data)
                df_enhanced_daily = predictor.aggregate_fitbit_by_activity(activity_processed_daily, fitbit_data)

                # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰
                for idx, row in df_enhanced_daily.iterrows():
                    predicted_frustration = None

                    # Fitbitãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã®ã¿äºˆæ¸¬
                    if check_fitbit_data_availability(row):
                        try:
                            prediction_result = predictor.predict_from_row(row)
                            if prediction_result and 'predicted_frustration' in prediction_result:
                                predicted_frustration = prediction_result['predicted_frustration']
                        except Exception as e:
                            logger.warning(f"äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")

                    # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã«è¿½åŠ 
                    frustration_value = predicted_frustration if predicted_frustration is not None else row.get('NASA_F')
                    if frustration_value is not None:
                        timeline_data.append({
                            'timestamp': row['Timestamp'].isoformat(),
                            'hour': row.get('hour', 0),
                            'activity': row.get('CatSub', 'unknown'),
                            'duration': row.get('Duration', 0),
                            'frustration_value': float(frustration_value),
                            'actual_frustration': row.get('NASA_F'),
                            'predicted_frustration': predicted_frustration,
                            'is_predicted': predicted_frustration is not None
                        })

        # LLMã§æ—¥æ¬¡ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ç”Ÿæˆ
        feedback_result = feedback_generator.generate_daily_dice_feedback(
            dice_result,
            timeline_data
        )

        logger.info(f"æ—¥æ¬¡ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ç”Ÿæˆå®Œäº†: user_id={user_id}, suggestions={len(dice_result.get('hourly_schedule', []))}")

        # æ—¥æ¬¡å¹³å‡ã‚’è¨ˆç®—
        avg_actual = None
        avg_predicted = None

        if timeline_data:
            actual_values = [item['actual_frustration'] for item in timeline_data if item.get('actual_frustration') is not None]
            predicted_values = [item['predicted_frustration'] for item in timeline_data if item.get('predicted_frustration') is not None]

            if actual_values:
                avg_actual = sum(actual_values) / len(actual_values)
            if predicted_values:
                avg_predicted = sum(predicted_values) / len(predicted_values)

        # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«æ—¥æ¬¡ã‚µãƒãƒªãƒ¼ã‚’ä¿å­˜
        summary_data = {
            'date': today,
            'avg_actual': avg_actual,
            'avg_predicted': avg_predicted,
            'dice_improvement': feedback_result.get('total_improvement_potential', 0),
            'dice_count': feedback_result.get('num_suggestions', 0),
            'chatgpt_feedback': feedback_result.get('main_feedback', ''),
            'action_plan': feedback_result.get('action_plan', []),
            'generated_at': feedback_result.get('generated_at', datetime.now().isoformat())
        }

        save_success = sheets_connector.save_daily_feedback_summary(user_id, summary_data)
        if save_success:
            logger.info(f"æ—¥æ¬¡ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚µãƒãƒªãƒ¼ã‚’ä¿å­˜ã—ã¾ã—ãŸ: user_id={user_id}, date={today}")
        else:
            logger.warning(f"æ—¥æ¬¡ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚µãƒãƒªãƒ¼ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: user_id={user_id}")

        return jsonify({
            'status': 'success',
            'user_id': user_id,
            'feedback_type': 'daily',
            'feedback': feedback_result,
            'daily_stats': {
                'avg_actual': round(avg_actual, 2) if avg_actual is not None else None,
                'avg_predicted': round(avg_predicted, 2) if avg_predicted is not None else None,
                'total_activities': len(timeline_data)
            },
            'saved_to_spreadsheet': save_success,
            'timestamp': datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/scheduler/status', methods=['GET'])
def scheduler_status():
    """å®šæœŸãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã®çŠ¶æ…‹å–å¾—"""
    try:
        status = scheduler.get_status()
        return jsonify({
            'status': 'success',
            'scheduler': status
        })
    except Exception as e:
        logger.error(f"ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼çŠ¶æ…‹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/scheduler/config', methods=['POST'])
def update_scheduler_config():
    """å®šæœŸãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã®è¨­å®šæ›´æ–°"""
    try:
        data = request.get_json()
        morning_time = data.get('morning_time')
        evening_time = data.get('evening_time')
        enabled = data.get('enabled')
        
        scheduler.update_schedule_config(morning_time, evening_time, enabled)
        
        return jsonify({
            'status': 'success',
            'message': 'ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®šã‚’æ›´æ–°ã—ã¾ã—ãŸ'
        })
    except Exception as e:
        logger.error(f"ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®šæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/scheduler/trigger', methods=['POST'])
def trigger_manual_feedback():
    """æ‰‹å‹•ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å®Ÿè¡Œ"""
    try:
        data = request.get_json()
        user_id = data.get('user_id', 'default')
        feedback_type = data.get('type', 'evening')
        
        feedback = scheduler.trigger_manual_feedback(user_id, feedback_type)
        
        return jsonify({
            'status': 'success',
            'feedback': feedback,
            'message': f'{feedback_type}ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’æ‰‹å‹•å®Ÿè¡Œã—ã¾ã—ãŸ'
        })
    except Exception as e:
        logger.error(f"æ‰‹å‹•ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/feedback/history', methods=['GET'])
def get_feedback_history():
    """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å±¥æ­´å–å¾—"""
    try:
        user_id = request.args.get('user_id')
        days = int(request.args.get('days', 7))
        
        history = scheduler.get_recent_feedback(user_id, days)
        
        return jsonify({
            'status': 'success',
            'history': history,
            'count': len(history)
        })
    except Exception as e:
        logger.error(f"ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å±¥æ­´å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/health', methods=['GET'])
def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    try:
        user_id = request.args.get('user_id', 'default')

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã®predictorã‚’å–å¾—
        predictor = get_predictor(user_id)

        # å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®çŠ¶æ…‹ç¢ºèª
        sheets_status = sheets_connector.test_connection()
        scheduler_status = scheduler.get_status()

        return jsonify({
            'status': 'success',
            'timestamp': datetime.now().isoformat(),
            'components': {
                'sheets_connector': sheets_status,
                'scheduler': scheduler_status,
                'predictor_ready': predictor.model is not None,
                'explainer_ready': explainer is not None,
                'feedback_generator_ready': feedback_generator is not None
            }
        })
    except Exception as e:
        logger.error(f"ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/debug/model', methods=['GET'])
def debug_model():
    """ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒãƒƒã‚°æƒ…å ±å–å¾—API"""
    try:
        user_id = request.args.get('user_id', 'default')

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã®predictorã‚’å–å¾—
        predictor = get_predictor(user_id)

        # ãƒ‡ãƒ¼ã‚¿å–å¾—
        activity_data = sheets_connector.get_activity_data(user_id)
        fitbit_data = sheets_connector.get_fitbit_data(user_id)

        # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
        activity_processed = predictor.preprocess_activity_data(activity_data)
        df_enhanced = predictor.aggregate_fitbit_by_activity(activity_processed, fitbit_data)

        # ãƒ‡ãƒ¼ã‚¿å“è³ª
        data_quality = predictor.check_data_quality(df_enhanced)

        # NASA_Fçµ±è¨ˆ
        nasa_f_stats = None
        if not df_enhanced.empty and 'NASA_F' in df_enhanced.columns:
            nasa_f = df_enhanced['NASA_F'].dropna()
            if not nasa_f.empty:
                nasa_f_stats = {
                    'count': int(len(nasa_f)),
                    'mean': float(nasa_f.mean()),
                    'std': float(nasa_f.std()),
                    'min': float(nasa_f.min()),
                    'max': float(nasa_f.max()),
                    'unique_values': int(nasa_f.nunique()),
                    'value_distribution': nasa_f.value_counts().head(10).to_dict()
                }

        # æ´»å‹•çµ±è¨ˆ
        activity_stats = None
        if not df_enhanced.empty and 'CatSub' in df_enhanced.columns:
            activities = df_enhanced['CatSub'].dropna()
            if not activities.empty:
                activity_stats = {
                    'total': int(len(activities)),
                    'unique': int(activities.nunique()),
                    'top_5': activities.value_counts().head(5).to_dict()
                }

        # ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹
        model_info = {
            'is_trained': predictor.model is not None,
            'feature_count': len(predictor.feature_columns) if predictor.feature_columns else 0,
            'feature_names': predictor.feature_columns if predictor.feature_columns else []
        }

        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼æƒ…å ±
        encoders_info = {}
        if hasattr(predictor, 'encoders'):
            for key, encoder in predictor.encoders.items():
                if hasattr(encoder, 'classes_'):
                    encoders_info[key] = {
                        'n_classes': len(encoder.classes_),
                        'classes': list(encoder.classes_)
                    }

        # Walk Forward Validationçµæœ
        wfv_results = None
        if predictor.model is not None and len(df_enhanced) >= 10:
            try:
                training_results = predictor.walk_forward_validation_train(df_enhanced)
                wfv_results = {
                    'rmse': float(training_results.get('walk_forward_rmse', 0)),
                    'mae': float(training_results.get('walk_forward_mae', 0)),
                    'r2': float(training_results.get('walk_forward_r2', 0)),
                    'prediction_diversity': training_results.get('prediction_diversity', {}),
                    'feature_importance': {
                        k: float(v) for k, v in sorted(
                            training_results.get('feature_importance', {}).items(),
                            key=lambda x: x[1],
                            reverse=True
                        )[:15]  # ä¸Šä½15ç‰¹å¾´é‡
                    }
                }
            except Exception as e:
                logger.error(f"WFVå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
                wfv_results = {'error': str(e)}

        # ãƒ†ã‚¹ãƒˆäºˆæ¸¬
        test_predictions = []
        if not df_enhanced.empty and predictor.model is not None:
            test_cases = df_enhanced.head(3)
            for idx, row in test_cases.iterrows():
                activity = row.get('CatSub')
                if activity is None:
                    activity = 'unknown'

                duration = row.get('Duration', 60)
                timestamp = pd.to_datetime(row.get('Timestamp'))
                actual = row.get('NASA_F')

                # å›ºå®šå€¤äºˆæ¸¬
                pred1 = predictor.predict_single_activity(activity, duration, timestamp)

                # å±¥æ­´ä½¿ç”¨äºˆæ¸¬
                pred2 = predictor.predict_with_history(activity, duration, timestamp, df_enhanced)

                test_predictions.append({
                    'activity': activity,
                    'timestamp': timestamp.isoformat(),
                    'actual': float(actual) if pd.notna(actual) else None,
                    'predicted_fixed': float(pred1.get('predicted_frustration', 0)),
                    'predicted_history': float(pred2.get('predicted_frustration', 0)),
                    'historical_records': int(pred2.get('historical_records', 0))
                })

        # è¨ºæ–­
        issues = []
        if data_quality['total_samples'] < 10:
            issues.append(f"ãƒ‡ãƒ¼ã‚¿æ•°ä¸è¶³: {data_quality['total_samples']}ä»¶")
        if nasa_f_stats and nasa_f_stats['std'] < 1.0:
            issues.append(f"NASA_Fã®åˆ†æ•£ãŒå°ã•ã„: {nasa_f_stats['std']:.2f}")
        if nasa_f_stats and nasa_f_stats['unique_values'] < 3:
            issues.append(f"NASA_Fã®ç¨®é¡ãŒå°‘ãªã„: {nasa_f_stats['unique_values']}ç¨®é¡")
        if not predictor.model:
            issues.append("ãƒ¢ãƒ‡ãƒ«æœªè¨“ç·´")
        if activity_stats and activity_stats['unique'] < 3:
            issues.append(f"æ´»å‹•ç¨®é¡ãŒå°‘ãªã„: {activity_stats['unique']}ç¨®é¡")

        return jsonify({
            'status': 'success',
            'user_id': user_id,
            'timestamp': datetime.now().isoformat(),
            'data_quality': data_quality,
            'nasa_f_stats': nasa_f_stats,
            'activity_stats': activity_stats,
            'model_info': model_info,
            'encoders_info': encoders_info,
            'wfv_results': wfv_results,
            'test_predictions': test_predictions,
            'issues': issues,
            'diagnosis': 'OK' if not issues else 'Issues detected'
        })

    except Exception as e:
        logger.error(f"ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒãƒƒã‚°ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/data/stats', methods=['GET'])
def get_data_stats():
    """ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆæƒ…å ±å–å¾—API"""
    try:
        user_id = request.args.get('user_id', 'default')

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã®predictorã‚’å–å¾—
        predictor = get_predictor(user_id)

        # ãƒ‡ãƒ¼ã‚¿å–å¾—
        activity_data = sheets_connector.get_activity_data(user_id)
        fitbit_data = sheets_connector.get_fitbit_data(user_id)

        # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
        activity_processed = predictor.preprocess_activity_data(activity_data)
        df_enhanced = predictor.aggregate_fitbit_by_activity(activity_processed, fitbit_data)

        # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
        data_quality = predictor.check_data_quality(df_enhanced)

        # NASA_Fçµ±è¨ˆ
        nasa_f_stats = {}
        if not activity_data.empty and 'NASA_F' in activity_data.columns:
            nasa_f_values = pd.to_numeric(activity_data['NASA_F'], errors='coerce').dropna()
            if not nasa_f_values.empty:
                nasa_f_stats = {
                    'count': int(len(nasa_f_values)),
                    'mean': float(nasa_f_values.mean()),
                    'std': float(nasa_f_values.std()),
                    'min': float(nasa_f_values.min()),
                    'max': float(nasa_f_values.max()),
                    'median': float(nasa_f_values.median()),
                    'unique_values': int(nasa_f_values.nunique())
                }

        # æ´»å‹•ã‚«ãƒ†ã‚´ãƒªçµ±è¨ˆ
        activity_stats = {}
        if not activity_data.empty and 'CatSub' in activity_data.columns:
            activity_counts = activity_data['CatSub'].value_counts()
            activity_stats = {
                'total_activities': int(len(activity_data)),
                'unique_activities': int(activity_data['CatSub'].nunique()),
                'top_activities': activity_counts.head(10).to_dict()
            }

        # Fitbitãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ
        fitbit_stats = {
            'total_records': int(len(fitbit_data)) if not fitbit_data.empty else 0,
            'has_data': not fitbit_data.empty
        }

        # ãƒ¢ãƒ‡ãƒ«çµ±è¨ˆ
        model_stats = {
            'is_trained': predictor.model is not None,
            'feature_count': len(predictor.feature_columns) if hasattr(predictor, 'feature_columns') and predictor.feature_columns else 0,
            'training_history_count': len(predictor.walk_forward_history) if hasattr(predictor, 'walk_forward_history') and predictor.walk_forward_history else 0
        }

        # æ—¥ä»˜ç¯„å›²
        date_range = {}
        if not activity_data.empty and 'Timestamp' in activity_data.columns:
            timestamps = pd.to_datetime(activity_data['Timestamp'], errors='coerce').dropna()
            if not timestamps.empty:
                date_range = {
                    'earliest': timestamps.min().isoformat(),
                    'latest': timestamps.max().isoformat(),
                    'days_span': (timestamps.max() - timestamps.min()).days
                }

        return jsonify({
            'status': 'success',
            'user_id': user_id,
            'timestamp': datetime.now().isoformat(),
            'data_quality': data_quality,
            'nasa_f_stats': nasa_f_stats,
            'activity_stats': activity_stats,
            'fitbit_stats': fitbit_stats,
            'model_stats': model_stats,
            'date_range': date_range,
            'summary': {
                'total_activity_records': int(len(activity_data)) if not activity_data.empty else 0,
                'processed_records': int(len(df_enhanced)) if not df_enhanced.empty else 0,
                'data_sufficient': data_quality.get('is_sufficient', False),
                'quality_level': data_quality.get('quality_level', 'unknown')
            }
        })

    except Exception as e:
        logger.error(f"ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/users', methods=['GET'])
def get_users():
    """åˆ©ç”¨å¯èƒ½ãƒ¦ãƒ¼ã‚¶ãƒ¼ä¸€è¦§å–å¾—"""
    try:
        # Config.pyã‹ã‚‰è¨­å®šæ¸ˆã¿ãƒ¦ãƒ¼ã‚¶ãƒ¼ä¸€è¦§ã‚’å–å¾—
        config = Config()
        users = config.get_all_users()
        
        return jsonify({
            'status': 'success',
            'users': users
        })
    except Exception as e:
        logger.error(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/frustration/trends', methods=['POST'])
def get_frustration_trends():
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ¥ãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å€¤æ¨ç§»ãƒ‡ãƒ¼ã‚¿å–å¾—"""
    try:
        data = request.get_json()
        user_id = data.get('user_id', 'default')
        days = data.get('days', 30)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ30æ—¥é–“
        
        # ãƒ‡ãƒ¼ã‚¿å–å¾—
        activity_data = sheets_connector.get_activity_data(user_id)
        
        if activity_data.empty:
            return jsonify({
                'status': 'error',
                'message': 'æ´»å‹•ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'
            }), 400
        
        # æ—¥åˆ¥å¹³å‡ãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å€¤ã‚’è¨ˆç®—
        activity_data['Date'] = pd.to_datetime(activity_data['Timestamp']).dt.date
        
        # éå»æŒ‡å®šæ—¥æ•°ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        end_date = datetime.now().date()
        start_date = end_date - timedelta(days=days)
        activity_data = activity_data[activity_data['Date'] >= start_date]
        
        # æ—¥åˆ¥çµ±è¨ˆã®è¨ˆç®—
        daily_stats = []
        date_range = [start_date + timedelta(days=x) for x in range(days)]
        
        for date in date_range:
            day_data = activity_data[activity_data['Date'] == date]
            
            if not day_data.empty:
                avg_frustration = day_data['NASA_F'].mean()
                min_frustration = day_data['NASA_F'].min()
                max_frustration = day_data['NASA_F'].max()
                activity_count = len(day_data)
                total_duration = day_data['Duration'].sum()
                
                # æ´»å‹•åˆ¥é›†è¨ˆ
                activity_summary = day_data.groupby('CatSub').agg({
                    'NASA_F': ['mean', 'count'],
                    'Duration': 'sum'
                }).round(1)
                
                activity_breakdown = []
                for activity in activity_summary.index:
                    activity_breakdown.append({
                        'activity': activity,
                        'avg_frustration': activity_summary.loc[activity, ('NASA_F', 'mean')],
                        'count': int(activity_summary.loc[activity, ('NASA_F', 'count')]),
                        'total_duration': int(activity_summary.loc[activity, ('Duration', 'sum')])
                    })
            else:
                avg_frustration = None
                min_frustration = None
                max_frustration = None
                activity_count = 0
                total_duration = 0
                activity_breakdown = []
            
            daily_stats.append({
                'date': date.isoformat(),
                'avg_frustration': avg_frustration,
                'min_frustration': min_frustration,
                'max_frustration': max_frustration,
                'activity_count': activity_count,
                'total_duration': total_duration,
                'activity_breakdown': activity_breakdown
            })
        
        # æœŸé–“å…¨ä½“ã®çµ±è¨ˆ
        if not activity_data.empty:
            period_stats = {
                'avg_frustration': activity_data['NASA_F'].mean(),
                'min_frustration': activity_data['NASA_F'].min(),
                'max_frustration': activity_data['NASA_F'].max(),
                'total_activities': len(activity_data),
                'total_duration': activity_data['Duration'].sum(),
                'improvement_trend': calculate_trend(activity_data)
            }
        else:
            period_stats = {
                'avg_frustration': 0,
                'min_frustration': 0,
                'max_frustration': 0,
                'total_activities': 0,
                'total_duration': 0,
                'improvement_trend': 0
            }
        
        return jsonify({
            'status': 'success',
            'user_id': user_id,
            'period': f'{days}æ—¥é–“',
            'daily_trends': daily_stats,
            'period_summary': period_stats,
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"æ¨ç§»ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

def calculate_trend(activity_data):
    """ãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å€¤ã®æ”¹å–„ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’è¨ˆç®—"""
    try:
        if len(activity_data) < 2:
            return 0

        # æ—¥ä»˜é †ã«ã‚½ãƒ¼ãƒˆ
        activity_data = activity_data.sort_values('Timestamp')

        # å‰åŠã¨å¾ŒåŠã«åˆ†ã‘ã¦å¹³å‡å€¤ã‚’æ¯”è¼ƒ
        mid_point = len(activity_data) // 2
        first_half_avg = activity_data.iloc[:mid_point]['NASA_F'].mean()
        second_half_avg = activity_data.iloc[mid_point:]['NASA_F'].mean()

        # æ”¹å–„åº¦ã‚’è¨ˆç®—ï¼ˆè² ã®å€¤ãŒæ”¹å–„ã‚’æ„å‘³ã™ã‚‹ï¼‰
        trend = second_half_avg - first_half_avg
        return round(trend, 2)

    except Exception:
        return 0

def calculate_and_save_daily_summary(user_id: str, target_date=None):
    """
    æŒ‡å®šãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ—¥æ¬¡ã‚µãƒãƒªãƒ¼ã‚’è¨ˆç®—ã—ã¦ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ä¿å­˜

    Args:
        user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
        target_date: å¯¾è±¡æ—¥ä»˜ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯æ˜¨æ—¥ï¼‰

    Returns:
        ä¿å­˜æˆåŠŸ: True, å¤±æ•—: False
    """
    try:
        if target_date is None:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯æ˜¨æ—¥
            target_date = (datetime.now() - timedelta(days=1)).date()
        elif isinstance(target_date, str):
            target_date = datetime.fromisoformat(target_date).date()

        # ãƒ‡ãƒ¼ã‚¿å–å¾—
        activity_data = sheets_connector.get_activity_data(user_id, use_cache=False)

        if activity_data.empty:
            logger.warning(f"æ—¥æ¬¡ã‚µãƒãƒªãƒ¼è¨ˆç®—: ãƒ¦ãƒ¼ã‚¶ãƒ¼ {user_id} ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return False

        # å¯¾è±¡æ—¥ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        activity_data['Date'] = pd.to_datetime(activity_data['Timestamp']).dt.date
        daily_data = activity_data[activity_data['Date'] == target_date]

        if daily_data.empty:
            logger.info(f"æ—¥æ¬¡ã‚µãƒãƒªãƒ¼è¨ˆç®—: {target_date} ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ (ãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_id})")
            return False

        # ãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å€¤ã®çµ±è¨ˆã‚’è¨ˆç®—
        frustration_values = daily_data['NASA_F'].dropna()

        if frustration_values.empty:
            logger.warning(f"æ—¥æ¬¡ã‚µãƒãƒªãƒ¼è¨ˆç®—: NASA_Få€¤ãŒã‚ã‚Šã¾ã›ã‚“ (ãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_id}, æ—¥ä»˜: {target_date})")
            return False

        # ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        summary_data = {
            'avg_frustration': float(frustration_values.mean()),
            'min_frustration': float(frustration_values.min()),
            'max_frustration': float(frustration_values.max()),
            'activity_count': int(len(daily_data)),
            'total_duration': int(daily_data['Duration'].sum()),
            'unique_activities': int(daily_data['CatSub'].nunique()),
            'notes': f'Auto-generated from {len(daily_data)} activities'
        }

        # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ä¿å­˜
        success = sheets_connector.save_daily_summary(
            user_id=user_id,
            date=target_date.isoformat(),
            summary_data=summary_data
        )

        if success and config.ENABLE_INFO_LOGS:
            logger.info(f"æ—¥æ¬¡ã‚µãƒãƒªãƒ¼ä¿å­˜å®Œäº†: {user_id}, {target_date}, "
                       f"å¹³å‡: {summary_data['avg_frustration']:.2f}, "
                       f"æ´»å‹•æ•°: {summary_data['activity_count']}")

        return success

    except Exception as e:
        logger.error(f"æ—¥æ¬¡ã‚µãƒãƒªãƒ¼è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
        return False

# ===== DEBUG API ENDPOINTS =====

@app.route('/api/frustration/daily-summary', methods=['POST'])
def generate_daily_summary():
    """
    æ—¥æ¬¡ã‚µãƒãƒªãƒ¼è¨ˆç®—ãƒ»ä¿å­˜API
    æŒ‡å®šæ—¥ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯æ˜¨æ—¥ï¼‰ã®ãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµ±è¨ˆã‚’è¨ˆç®—ã—ã¦ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ä¿å­˜
    """
    try:
        data = request.get_json() or {}
        user_id = data.get('user_id', 'default')
        target_date = data.get('date')

        # æ—¥æ¬¡ã‚µãƒãƒªãƒ¼ã‚’è¨ˆç®—ãƒ»ä¿å­˜
        success = calculate_and_save_daily_summary(user_id, target_date)

        if success:
            return jsonify({
                'status': 'success',
                'message': 'æ—¥æ¬¡ã‚µãƒãƒªãƒ¼ã‚’ä¿å­˜ã—ã¾ã—ãŸ',
                'user_id': user_id,
                'date': target_date if target_date else (datetime.now() - timedelta(days=1)).date().isoformat(),
                'timestamp': datetime.now().isoformat()
            })
        else:
            return jsonify({
                'status': 'error',
                'message': 'æ—¥æ¬¡ã‚µãƒãƒªãƒ¼ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚',
                'user_id': user_id
            }), 400

    except Exception as e:
        logger.error(f"æ—¥æ¬¡ã‚µãƒãƒªãƒ¼ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/debug/dice-scheduler/status', methods=['GET'])
def debug_dice_scheduler_status():
    """DiCEã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã®çŠ¶æ…‹ã¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ç¢ºèªAPI"""
    try:
        now = datetime.now()
        next_run = now.replace(hour=21, minute=0, second=0, microsecond=0)
        if now > next_run:
            next_run += timedelta(days=1)
        
        status = {
            'scheduler_running': dice_scheduler_running,
            'current_time': now.isoformat(),
            'next_scheduled_run': next_run.isoformat(),
            'seconds_until_next_run': (next_run - now).total_seconds(),
            'last_dice_result': last_dice_result,
            'scheduler_thread_alive': dice_scheduler_thread.is_alive() if dice_scheduler_thread else False
        }
        
        return jsonify({
            'status': 'success',
            'data': status
        })
    except Exception as e:
        logger.error(f"DiCEã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼çŠ¶æ…‹ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/debug/dice-scheduler/trigger', methods=['POST'])
def debug_trigger_dice():
    """æ‰‹å‹•ã§DiCEæ”¹å–„ææ¡ˆã‚’å®Ÿè¡Œã™ã‚‹ãƒ‡ãƒãƒƒã‚°API"""
    global last_dice_result
    
    try:
        data = request.get_json() or {}
        user_id = data.get('user_id', 'default')
        target_date = data.get('date')
        
        if target_date:
            if isinstance(target_date, str):
                target_date = datetime.fromisoformat(target_date.replace('Z', '+00:00')).date()
        else:
            target_date = datetime.now().date()
        
        if config.ENABLE_DEBUG_LOGS:
            logger.debug(f"æ‰‹å‹•DiCEå®Ÿè¡Œé–‹å§‹: ãƒ¦ãƒ¼ã‚¶ãƒ¼={user_id}, æ—¥ä»˜={target_date}")
        
        dice_result = run_daily_dice_for_user(user_id)
        
        if dice_result:
            last_dice_result = {
                'timestamp': datetime.now().isoformat(),
                'user_id': user_id,
                'result': dice_result,
                'execution_type': 'manual_debug'
            }
            
            return jsonify({
                'status': 'success',
                'message': 'æ‰‹å‹•DiCEå®Ÿè¡Œå®Œäº†',
                'data': {
                    'user_id': user_id,
                    'execution_time': last_dice_result['timestamp'],
                    'total_improvement': dice_result.get('total_improvement', 0),
                    'schedule_items': len(dice_result.get('hourly_schedule', [])),
                    'dice_message': dice_result.get('message', ''),
                    'confidence': dice_result.get('confidence', 0),
                    'summary': dice_result.get('summary', ''),
                    'full_result': dice_result
                }
            })
        else:
            return jsonify({
                'status': 'error',
                'message': 'DiCEå®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚'
            }), 400
            
    except Exception as e:
        logger.error(f"æ‰‹å‹•DiCEå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'status': 'error',
            'message': f'ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}'
        }), 500

@app.route('/api/debug/dice-scheduler/results', methods=['GET'])
def debug_get_dice_results():
    """æœ€æ–°ã®DiCEçµæœã‚’å–å¾—ã™ã‚‹ãƒ‡ãƒãƒƒã‚°API"""
    try:
        if not last_dice_result:
            return jsonify({
                'status': 'success',
                'message': 'DiCEçµæœãŒã¾ã ã‚ã‚Šã¾ã›ã‚“',
                'data': None
            })

        return jsonify({
            'status': 'success',
            'data': last_dice_result
        })
    except Exception as e:
        logger.error(f"DiCEçµæœå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/debug/data-monitor/status', methods=['GET'])
def debug_data_monitor_status():
    """ãƒ‡ãƒ¼ã‚¿ç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰ã®çŠ¶æ…‹ç¢ºèªAPI"""
    try:
        status = {
            'monitor_running': data_monitor_running,
            'monitor_thread_alive': data_monitor_thread.is_alive() if data_monitor_thread else False,
            'last_prediction': last_prediction_result if last_prediction_result else None,
            'current_time': datetime.now().isoformat()
        }

        return jsonify({
            'status': 'success',
            'data': status
        })
    except Exception as e:
        logger.error(f"ãƒ‡ãƒ¼ã‚¿ç›£è¦–çŠ¶æ…‹ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/debug/data-monitor/check', methods=['POST'])
def debug_trigger_data_check():
    """æ‰‹å‹•ã§ãƒ‡ãƒ¼ã‚¿æ›´æ–°ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ãƒ‡ãƒãƒƒã‚°API"""
    try:
        data = request.get_json() or {}
        user_id = data.get('user_id', 'default')

        has_new = sheets_connector.has_new_data(user_id)

        return jsonify({
            'status': 'success',
            'user_id': user_id,
            'has_new_data': has_new,
            'timestamp': datetime.now().isoformat()
        })
    except Exception as e:
        logger.error(f"ãƒ‡ãƒ¼ã‚¿æ›´æ–°ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/tablet/data/<user_id>', methods=['GET'])
def get_tablet_data(user_id):
    """ã‚¿ãƒ–ãƒ¬ãƒƒãƒˆç”¨çµ±åˆãƒ‡ãƒ¼ã‚¿API - ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€åº¦ã«å–å¾—"""
    try:
        if config.ENABLE_DEBUG_LOGS:
            logger.debug(f"ã‚¿ãƒ–ãƒ¬ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿APIå‘¼ã³å‡ºã— - user_id: {user_id}")
        
        # ä»Šæ—¥ã®æ—¥ä»˜ã‚’å–å¾—
        today = datetime.now().strftime('%Y-%m-%d')
        
        # 1. ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        timeline_data = []
        try:
            with app.test_request_context(json={'user_id': user_id, 'date': today}):
                timeline_response = get_frustration_timeline()
                if hasattr(timeline_response, 'get_json'):
                    timeline_result = timeline_response.get_json()
                    if timeline_result and timeline_result.get('status') == 'success':
                        timeline_data = timeline_result.get('timeline', [])
        except Exception as timeline_error:
            logger.warning(f"ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿å–å¾—è­¦å‘Š: {timeline_error}")
        
        # 2. DiCEåˆ†æãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        dice_data = {}
        try:
            with app.test_request_context(json={'user_id': user_id}):
                dice_response = generate_dice_analysis()
                if hasattr(dice_response, 'get_json'):
                    dice_result = dice_response.get_json()
                    if dice_result and dice_result.get('status') == 'success':
                        dice_data = dice_result.get('dice_analysis', {})
        except Exception as dice_error:
            logger.warning(f"DiCEåˆ†æãƒ‡ãƒ¼ã‚¿å–å¾—è­¦å‘Š: {dice_error}")
        
        # 3. ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        feedback_data = {}
        try:
            # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ç”Ÿæˆãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            with app.test_request_context(json={'user_id': user_id, 'feedback_type': 'daily'}):
                feedback_response = generate_feedback()
                if hasattr(feedback_response, 'get_json'):
                    feedback_result = feedback_response.get_json()
                    if feedback_result and feedback_result.get('status') == 'success':
                        feedback_data = feedback_result.get('feedback', {})
        except Exception as feedback_error:
            logger.warning(f"ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿å–å¾—è­¦å‘Š: {feedback_error}")
        
        # 4. ä»Šæ—¥ã®çµ±è¨ˆã‚’è¨ˆç®—
        daily_stats = {
            'date': today,
            'total_activities': len(timeline_data),
            'avg_frustration': 0,
            'min_frustration': 0,
            'max_frustration': 0
        }
        
        if timeline_data:
            frustration_values = [item.get('frustration_value', 0) for item in timeline_data if item.get('frustration_value') is not None]
            if frustration_values:
                daily_stats['avg_frustration'] = round(sum(frustration_values) / len(frustration_values), 1)
                daily_stats['min_frustration'] = min(frustration_values)
                daily_stats['max_frustration'] = max(frustration_values)
        
        # 5. çµ±åˆãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä½œæˆ
        response_data = {
            'status': 'success',
            'user_id': user_id,
            'timestamp': datetime.now().isoformat(),
            'daily_stats': daily_stats,
            'timeline': timeline_data,
            'dice_analysis': dice_data,
            'feedback': feedback_data,
            'system_info': {
                'data_source': 'spreadsheet',
                'last_update': datetime.now().isoformat(),
                'prediction_logging': True,
                'data_monitoring': True
            }
        }
        
        return jsonify(response_data)
        
    except Exception as e:
        logger.error(f"ã‚¿ãƒ–ãƒ¬ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿API ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'status': 'error',
            'user_id': user_id,
            'message': str(e),
            'timestamp': datetime.now().isoformat()
        }), 500

# ===== HELPER FUNCTIONS =====

def get_user_config(user_id: str) -> Dict:
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®šã‚’å–å¾—
    Config.pyã‹ã‚‰è¨­å®šã‚’å–å¾—ã™ã‚‹ã‚ˆã†ã«å¤‰æ›´
    """
    config = Config()
    users = config.get_all_users()

    for user in users:
        if user['user_id'] == user_id:
            return user

    # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯æœ€åˆã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’è¿”ã™
    if users:
        return users[0]

    return {'user_id': user_id, 'name': user_id}

def daily_dice_scheduler():
    """æ¯æ—¥21:00ã«DiCEæ”¹å–„ææ¡ˆã‚’ç”Ÿæˆã™ã‚‹ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼"""
    global dice_scheduler_running, last_dice_result

    while dice_scheduler_running:
        try:
            now = datetime.now()
            # æ¯æ—¥21:00ã«å®Ÿè¡Œ
            target_time = now.replace(hour=21, minute=0, second=0, microsecond=0)

            # ä»Šæ—¥ã®21:00ãŒã¾ã æ¥ã¦ã„ãªã„å ´åˆã¯ãã®ã¾ã¾ã€éãã¦ã„ã‚‹å ´åˆã¯æ˜æ—¥ã®21:00
            if now > target_time:
                target_time += timedelta(days=1)

            sleep_seconds = (target_time - now).total_seconds()
            if config.ENABLE_INFO_LOGS:
                logger.info(f"æ¬¡å›DiCEå®Ÿè¡Œäºˆå®š: {target_time.strftime('%Y-%m-%d %H:%M:%S')} ({sleep_seconds:.0f}ç§’å¾Œ)")

            # æŒ‡å®šæ™‚åˆ»ã¾ã§å¾…æ©Ÿ
            time.sleep(sleep_seconds)

            if dice_scheduler_running:  # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ãŒåœæ­¢ã•ã‚Œã¦ã„ãªã„ã‹ç¢ºèª
                if config.ENABLE_INFO_LOGS:
                    logger.info("å®šæ™‚DiCEæ”¹å–„ææ¡ˆã¨æ—¥æ¬¡ã‚µãƒãƒªãƒ¼ã‚’å®Ÿè¡Œä¸­...")

                # å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å¯¾ã—ã¦å®Ÿè¡Œ
                users_config = config.get_all_users()

                for user_config in users_config:
                    user_id = user_config['user_id']
                    user_name = user_config['name']

                    # DiCEå®Ÿè¡Œ
                    dice_result = run_daily_dice_for_user(user_id)

                    if dice_result:
                        last_dice_result = {
                            'timestamp': datetime.now().isoformat(),
                            'user_id': user_id,
                            'user_name': user_name,
                            'result': dice_result,
                            'execution_type': 'scheduled'
                        }
                        if config.ENABLE_INFO_LOGS:
                            logger.info(f"å®šæ™‚DiCEå®Ÿè¡Œå®Œäº† ({user_name}): æ”¹å–„ãƒã‚¤ãƒ³ãƒˆ {dice_result.get('total_improvement', 0):.1f}ç‚¹")
                    else:
                        logger.error(f"å®šæ™‚DiCEå®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ: {user_name}")

                    # æ—¥æ¬¡ã‚µãƒãƒªãƒ¼è¨ˆç®—ã¨ä¿å­˜ï¼ˆæ˜¨æ—¥ã®ãƒ‡ãƒ¼ã‚¿ï¼‰
                    yesterday = (datetime.now() - timedelta(days=1)).date()
                    summary_success = calculate_and_save_daily_summary(user_id, yesterday)

                    if summary_success and config.ENABLE_INFO_LOGS:
                        logger.info(f"æ—¥æ¬¡ã‚µãƒãƒªãƒ¼ä¿å­˜å®Œäº† ({user_name}): {yesterday}")
                    elif not summary_success:
                        logger.warning(f"æ—¥æ¬¡ã‚µãƒãƒªãƒ¼ä¿å­˜å¤±æ•— ({user_name}): {yesterday}")

        except Exception as e:
            logger.error(f"DiCEã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}")
            time.sleep(3600)  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯1æ™‚é–“å¾…æ©Ÿ

def run_daily_dice_for_user(user_id: str):
    """æŒ‡å®šãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ—¥æ¬¡DiCEæ”¹å–„ææ¡ˆã‚’å®Ÿè¡Œ"""
    try:
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã®predictorã‚’å–å¾—
        predictor = get_predictor(user_id)

        # ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªï¼ˆensure_model_trainedã‚’ä½¿ç”¨ï¼‰
        training_result = ensure_model_trained(user_id, force_retrain=True)

        if training_result['status'] not in ['success', 'already_trained']:
            logger.warning(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ {user_id} ã®ãƒ¢ãƒ‡ãƒ«è¨“ç·´å¤±æ•—: {training_result.get('message')}")
            return None

        # ãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã‚‹ã‹æœ€çµ‚ç¢ºèª
        if predictor.model is None:
            logger.error(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ {user_id} ã®ãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return None

        # ãƒ‡ãƒ¼ã‚¿ã‚’å†å–å¾—ï¼ˆè¨“ç·´å¾Œã®æœ€æ–°ãƒ‡ãƒ¼ã‚¿ï¼‰
        activity_data = sheets_connector.get_activity_data(user_id)
        fitbit_data = sheets_connector.get_fitbit_data(user_id)

        if activity_data.empty:
            logger.warning(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ {user_id} ã®æ´»å‹•ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None

        # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
        enhanced_data = predictor.preprocess_activity_data(activity_data)
        if not enhanced_data.empty:
            enhanced_data = predictor.aggregate_fitbit_by_activity(enhanced_data, fitbit_data)

        # æ˜¨æ—¥ã®ãƒ‡ãƒ¼ã‚¿ã§DiCEå®Ÿè¡Œ
        yesterday = datetime.now() - timedelta(days=1)
        dice_result = explainer.generate_hourly_alternatives(enhanced_data, predictor, yesterday)

        return dice_result

    except Exception as e:
        logger.error(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ {user_id} ã®DiCEå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        return None

def data_monitor_loop():
    """
    å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ‡ãƒ¼ã‚¿æ›´æ–°ã‚’ç›£è¦–ã—ã€æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãŒè¿½åŠ ã•ã‚ŒãŸã‚‰è‡ªå‹•çš„ã«ãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³äºˆæ¸¬ã‚’å®Ÿè¡Œ
    """
    global data_monitor_running, last_prediction_result

    check_interval = 600  # 600ç§’ï¼ˆ10åˆ†ï¼‰ã”ã¨ã«ãƒã‚§ãƒƒã‚¯
    
    # å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒªã‚¹ãƒˆã‚’Configã‹ã‚‰å–å¾—
    users_config = config.get_all_users()

    while data_monitor_running:
        try:
            # å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯
            for user_config in users_config:
                user_id = user_config['user_id']
                user_name = user_config['name']
                
                if sheets_connector.has_new_data(user_id):
                    if config.ENABLE_INFO_LOGS:
                        logger.info(f"æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œçŸ¥ã—ã¾ã—ãŸã€‚ãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³äºˆæ¸¬ã‚’å®Ÿè¡Œã—ã¾ã™: {user_name} ({user_id})")

                    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã®predictorã‚’å–å¾—
                    predictor = get_predictor(user_id)

                    # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¦æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼‰
                    activity_data = sheets_connector.get_activity_data(user_id, use_cache=False)
                    fitbit_data = sheets_connector.get_fitbit_data(user_id, use_cache=False)

                    if not activity_data.empty:
                        # ãƒ¢ãƒ‡ãƒ«å†è¨“ç·´ï¼ˆensure_model_trainedã‚’ä½¿ç”¨ï¼‰
                        training_result = ensure_model_trained(user_id, force_retrain=True)

                        if training_result['status'] not in ['success', 'already_trained']:
                            logger.warning(f"ãƒ¢ãƒ‡ãƒ«å†è¨“ç·´å¤±æ•— ({user_name}): {training_result.get('message')}")
                            continue

                        # ãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
                        if predictor.model is None:
                            logger.error(f"ãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ ({user_name})")
                            continue

                        # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
                        activity_processed = predictor.preprocess_activity_data(activity_data)
                        df_enhanced = predictor.aggregate_fitbit_by_activity(activity_processed, fitbit_data)

                        # æœ€æ–°ã®æ´»å‹•ã«å¯¾ã™ã‚‹ãƒ•ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³äºˆæ¸¬ï¼ˆå±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰
                        latest_activity = activity_processed.iloc[-1]
                        prediction_result = predictor.predict_with_history(
                            latest_activity.get('CatSub', 'unknown'),
                            latest_activity.get('Duration', 60),
                            latest_activity.get('Timestamp', datetime.now()),
                            df_enhanced  # å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’æ¸¡ã™
                        )

                        # ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ¥ã«äºˆæ¸¬çµæœã‚’ä¿å­˜
                        last_prediction_result[user_id] = {
                            'timestamp': datetime.now().isoformat(),
                            'user_id': user_id,
                            'user_name': user_name,
                            'latest_activity': latest_activity.get('CatSub', 'unknown'),
                            'prediction': prediction_result,
                            'data_count': len(df_enhanced)
                        }

                        if config.LOG_PREDICTIONS:
                            logger.info(f"è‡ªå‹•äºˆæ¸¬å®Œäº† ({user_name}): {prediction_result}")

                        # äºˆæ¸¬çµæœã‚’ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ä¿å­˜ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰
                        activity_timestamp = latest_activity.get('Timestamp')
                        predicted_frust = prediction_result.get('predicted_frustration', 0)
                        predicted_conf = prediction_result.get('confidence', 0)

                        # NaN/Infãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
                        import numpy as np
                        if np.isnan(predicted_frust) or np.isinf(predicted_frust):
                            logger.warning(f"äºˆæ¸¬å€¤ãŒä¸æ­£ã§ã™ (NaN/Inf) - ãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_name}, ä¿å­˜ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                            continue

                        prediction_data = {
                            'timestamp': datetime.now().isoformat(),
                            'user_id': user_id,
                            'activity': latest_activity.get('CatSub', 'unknown'),
                            'duration': latest_activity.get('Duration', 0),
                            'predicted_frustration': float(predicted_frust),  # æ˜ç¤ºçš„ã«floatå¤‰æ›
                            'confidence': float(predicted_conf),  # æ˜ç¤ºçš„ã«floatå¤‰æ›
                            'actual_frustration': latest_activity.get('NASA_F', None),
                            'source': 'auto_monitoring',  # è‡ªå‹•ç›£è¦–ã«ã‚ˆã‚‹äºˆæ¸¬ã§ã‚ã‚‹ã“ã¨ã‚’æ˜è¨˜
                            'activity_timestamp': activity_timestamp  # é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ã®æ´»å‹•ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
                        }

                        try:
                            # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼šåŒã˜activity_timestampã®äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ãŒæ—¢ã«å­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                            if not sheets_connector.is_prediction_duplicate(user_id, activity_timestamp):
                                sheets_connector.save_prediction_data(prediction_data)
                                if config.LOG_PREDICTIONS:
                                    logger.info(f"è‡ªå‹•äºˆæ¸¬çµæœã‚’ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«è¨˜éŒ²: {user_name}, {latest_activity.get('CatSub', 'unknown')}, äºˆæ¸¬å€¤: {predicted_frust:.2f}")
                            else:
                                if config.ENABLE_DEBUG_LOGS:
                                    logger.debug(f"é‡è¤‡ã™ã‚‹äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚­ãƒƒãƒ—: {user_name}, {latest_activity.get('CatSub', 'unknown')}")
                        except Exception as save_error:
                            logger.error(f"äºˆæ¸¬çµæœä¿å­˜ã‚¨ãƒ©ãƒ¼ ({user_name}): {save_error}")

            # æ¬¡ã®ãƒã‚§ãƒƒã‚¯ã¾ã§å¾…æ©Ÿ
            time.sleep(check_interval)

        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿ç›£è¦–ãƒ«ãƒ¼ãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            time.sleep(check_interval)

def initialize_application():
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åˆæœŸåŒ–ï¼ˆä¸€åº¦ã ã‘å®Ÿè¡Œï¼‰"""
    global dice_scheduler_thread, dice_scheduler_running, data_monitor_thread, data_monitor_running, user_predictors, _app_initialized

    # æ—¢ã«åˆæœŸåŒ–æ¸ˆã¿ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
    if _app_initialized:
        return

    try:
        # åˆæœŸåŒ–é–‹å§‹ãƒ­ã‚°ï¼ˆå¸¸ã«å‡ºåŠ›ï¼‰
        logger.warning("ğŸš€ ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’åˆæœŸåŒ–ã—ã¦ã„ã¾ã™...")

        # æ—¢å­˜ã®ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¯ãƒªã‚¢ï¼ˆKNOWN_ACTIVITIESã®æ›´æ–°ãªã©ã«å¯¾å¿œï¼‰
        if user_predictors:
            logger.warning("ğŸ”„ æ—¢å­˜ã®ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚æ¬¡å›ã‚¢ã‚¯ã‚»ã‚¹æ™‚ã«æ–°ã—ã„KNOWN_ACTIVITIESã§å†è¨“ç·´ã•ã‚Œã¾ã™ã€‚")
            user_predictors.clear()

        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼é–‹å§‹
        scheduler.start_scheduler()
        logger.warning("âœ… å®šæœŸãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚’é–‹å§‹ã—ã¾ã—ãŸ")

        # DiCE daily scheduleré–‹å§‹
        dice_scheduler_running = True
        dice_scheduler_thread = threading.Thread(target=daily_dice_scheduler, daemon=True)
        dice_scheduler_thread.start()
        logger.warning("âœ… DiCEæ—¥æ¬¡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚’é–‹å§‹ã—ã¾ã—ãŸ (æ¯æ—¥21:00å®Ÿè¡Œ)")

        # ãƒ‡ãƒ¼ã‚¿æ›´æ–°ç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹
        data_monitor_running = True
        data_monitor_thread = threading.Thread(target=data_monitor_loop, daemon=True)
        data_monitor_thread.start()
        logger.warning("âœ… ãƒ‡ãƒ¼ã‚¿æ›´æ–°ç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é–‹å§‹ã—ã¾ã—ãŸ (10åˆ†ã”ã¨ã«ãƒã‚§ãƒƒã‚¯)")

        _app_initialized = True
        logger.warning("ğŸ‰ ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åˆæœŸåŒ–å®Œäº†")
    except Exception as e:
        logger.error(f"ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")

@app.route('/api/sheets/recreate-prediction', methods=['POST'])
def recreate_prediction_sheet_endpoint():
    """
    PREDICTION_DATAã‚·ãƒ¼ãƒˆã‚’å†ä½œæˆã™ã‚‹APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    """
    try:
        data = request.get_json()
        user_id = data.get('user_id', 'default')

        logger.info(f"PREDICTION_DATAã‚·ãƒ¼ãƒˆå†ä½œæˆãƒªã‚¯ã‚¨ã‚¹ãƒˆ: user_id={user_id}")

        # ã‚·ãƒ¼ãƒˆã‚’å†ä½œæˆ
        result = sheets_connector.recreate_prediction_sheet(user_id)

        if result:
            return jsonify({
                'status': 'success',
                'message': f'PREDICTION_DATA_{user_id}ã‚·ãƒ¼ãƒˆã‚’å†ä½œæˆã—ã¾ã—ãŸ',
                'user_id': user_id
            }), 200
        else:
            return jsonify({
                'status': 'error',
                'message': 'ã‚·ãƒ¼ãƒˆå†ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ',
                'user_id': user_id
            }), 500

    except Exception as e:
        logger.error(f"PREDICTION_DATAã‚·ãƒ¼ãƒˆå†ä½œæˆã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@app.route('/api/sheets/recreate-daily-summary', methods=['POST'])
def recreate_daily_summary_sheet_endpoint():
    """
    DAILY_SUMMARYã‚·ãƒ¼ãƒˆã‚’å†ä½œæˆã™ã‚‹APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    """
    try:
        data = request.get_json()
        user_id = data.get('user_id', 'default')

        logger.info(f"DAILY_SUMMARYã‚·ãƒ¼ãƒˆå†ä½œæˆãƒªã‚¯ã‚¨ã‚¹ãƒˆ: user_id={user_id}")

        # ã‚·ãƒ¼ãƒˆã‚’å†ä½œæˆ
        result = sheets_connector.recreate_daily_summary_sheet(user_id)

        if result:
            return jsonify({
                'status': 'success',
                'message': f'DAILY_SUMMARY_{user_id}ã‚·ãƒ¼ãƒˆã‚’å†ä½œæˆã—ã¾ã—ãŸ',
                'user_id': user_id
            }), 200
        else:
            return jsonify({
                'status': 'error',
                'message': 'ã‚·ãƒ¼ãƒˆå†ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ',
                'user_id': user_id
            }), 500

    except Exception as e:
        logger.error(f"DAILY_SUMMARYã‚·ãƒ¼ãƒˆå†ä½œæˆã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

def cleanup_application():
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³çµ‚äº†å‡¦ç†"""
    global dice_scheduler_running, data_monitor_running

    try:
        if config.ENABLE_INFO_LOGS:
            logger.info("ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’çµ‚äº†ã—ã¦ã„ã¾ã™...")

        # DiCE ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼åœæ­¢
        dice_scheduler_running = False

        # ãƒ‡ãƒ¼ã‚¿ç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰åœæ­¢
        data_monitor_running = False

        scheduler.stop_scheduler()
        if config.ENABLE_INFO_LOGS:
            logger.info("ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³çµ‚äº†å®Œäº†")
    except Exception as e:
        logger.error(f"ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³çµ‚äº†ã‚¨ãƒ©ãƒ¼: {e}")

# Gunicorn/Cloud Runç”¨: ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ™‚ã«åˆæœŸåŒ–
# __name__ == '__main__' ã®å¤–ã§å®Ÿè¡Œã•ã‚Œã‚‹ãŸã‚ã€æœ¬ç•ªç’°å¢ƒã§ã‚‚å‹•ä½œã™ã‚‹
initialize_application()

if __name__ == '__main__':
    try:
        # é–‹ç™ºã‚µãƒ¼ãƒãƒ¼èµ·å‹•ï¼ˆæœ¬ç•ªã§ã¯GunicornãŒä½¿ã‚ã‚Œã‚‹ã®ã§ã“ã®ãƒ–ãƒ­ãƒƒã‚¯ã¯å®Ÿè¡Œã•ã‚Œãªã„ï¼‰
        port = int(os.environ.get('PORT', 8080))
        app.run(host='0.0.0.0', port=port, debug=False, threaded=True)

    except KeyboardInterrupt:
        if config.ENABLE_INFO_LOGS:
            logger.info("ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰å‰²ã‚Šè¾¼ã¿ã«ã‚ˆã‚‹çµ‚äº†")
    except Exception as e:
        logger.error(f"ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
    finally:
        cleanup_application()